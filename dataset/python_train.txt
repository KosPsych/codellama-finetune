def commit(self, *innerpats, **inneropts):
            extras = opts.get('extra')
            for raw in extras:
                if b'=' not in raw:
                    msg = _(
                        b"unable to parse '%s', should follow "
                        b"KEY=VALUE format"
                    )
                    raise error.Abort(msg % raw)
                k, v = raw.split(b'=', 1)
                if not k:
                    msg = _(b"unable to parse '%s', keys can't be empty")
                    raise error.Abort(msg % raw)
                if re.search(br'[^\w-]', k):
                    msg = _(
                        b"keys can only contain ascii letters, digits,"
                        b" '_' and '-'"
                    )
                    raise error.Abort(msg)
                if k in usedinternally:
                    msg = _(
                        b"key '%s' is used internally, can't be set "
                        b"manually"
                    )
                    raise error.Abort(msg % k)
                inneropts['extra'][k] = v
            return super(repoextra, self).commit(*innerpats, **inneropts)
###END###
def __init__(self, node, name):
        self.node, self.name = node, name
###END###
def __bytes__(self):
        return hex(self.node) + b':' + self.name
###END###
def __init__(self, pf, plainmode=False):
        def eatdiff(lines):
            while lines:
                l = lines[-1]
                if (
                    l.startswith(b"diff -")
                    or l.startswith(b"Index:")
                    or l.startswith(b"===========")
                ):
                    del lines[-1]
                else:
                    break

        def eatempty(lines):
            while lines:
                if not lines[-1].strip():
                    del lines[-1]
                else:
                    break

        message = []
        comments = []
        user = None
        date = None
        parent = None
        format = None
        subject = None
        branch = None
        nodeid = None
        diffstart = 0

        for line in open(pf, b'rb'):
            line = line.rstrip()
            if line.startswith(b'diff --git') or (
                diffstart and line.startswith(b'+++ ')
            ):
                diffstart = 2
                break
            diffstart = 0  # reset
            if line.startswith(b"--- "):
                diffstart = 1
                continue
            elif format == b"hgpatch":
                # parse values when importing the result of an hg export
                if line.startswith(b"# User "):
                    user = line[7:]
                elif line.startswith(b"# Date "):
                    date = line[7:]
                elif line.startswith(b"# Parent "):
                    parent = line[9:].lstrip()  # handle double trailing space
                elif line.startswith(b"# Branch "):
                    branch = line[9:]
                elif line.startswith(b"# Node ID "):
                    nodeid = line[10:]
                elif not line.startswith(b"# ") and line:
                    message.append(line)
                    format = None
            elif line == b'# HG changeset patch':
                message = []
                format = b"hgpatch"
            elif format != b"tagdone" and (
                line.startswith(b"Subject: ") or line.startswith(b"subject: ")
            ):
                subject = line[9:]
                format = b"tag"
            elif format != b"tagdone" and (
                line.startswith(b"From: ") or line.startswith(b"from: ")
            ):
                user = line[6:]
                format = b"tag"
            elif format != b"tagdone" and (
                line.startswith(b"Date: ") or line.startswith(b"date: ")
            ):
                date = line[6:]
                format = b"tag"
            elif format == b"tag" and line == b"":
                # when looking for tags (subject: from: etc) they
                # end once you find a blank line in the source
                format = b"tagdone"
            elif message or line:
                message.append(line)
            comments.append(line)

        eatdiff(message)
        eatdiff(comments)
        # Remember the exact starting line of the patch diffs before consuming
        # empty lines, for external use by TortoiseHg and others
        self.diffstartline = len(comments)
        eatempty(message)
        eatempty(comments)

        # make sure message isn't empty
        if format and format.startswith(b"tag") and subject:
            message.insert(0, subject)

        self.message = message
        self.comments = comments
        self.user = user
        self.date = date
        self.parent = parent
        # nodeid and branch are for external use by TortoiseHg and others
        self.nodeid = nodeid
        self.branch = branch
        self.haspatch = diffstart > 1
        self.plainmode = (
            plainmode
            or b'# HG changeset patch' not in self.comments
            and any(
                c.startswith(b'Date: ') or c.startswith(b'From: ')
                for c in self.comments
            )
        )
###END###
def setuser(self, user):
        try:
            inserthgheader(self.comments, b'# User ', user)
        except ValueError:
            if self.plainmode:
                insertplainheader(self.comments, b'From', user)
            else:
                tmp = [b'# HG changeset patch', b'# User ' + user]
                self.comments = tmp + self.comments
        self.user = user
###END###
def setdate(self, date):
        try:
            inserthgheader(self.comments, b'# Date ', date)
        except ValueError:
            if self.plainmode:
                insertplainheader(self.comments, b'Date', date)
            else:
                tmp = [b'# HG changeset patch', b'# Date ' + date]
                self.comments = tmp + self.comments
        self.date = date
###END###
def setparent(self, parent):
        try:
            inserthgheader(self.comments, b'# Parent  ', parent)
        except ValueError:
            if not self.plainmode:
                tmp = [b'# HG changeset patch', b'# Parent  ' + parent]
                self.comments = tmp + self.comments
        self.parent = parent
###END###
def setmessage(self, message):
        if self.comments:
            self._delmsg()
        self.message = [message]
        if message:
            if self.plainmode and self.comments and self.comments[-1]:
                self.comments.append(b'')
            self.comments.append(message)
###END###
def __bytes__(self):
        s = b'\n'.join(self.comments).rstrip()
        if not s:
            return b''
        return s + b'\n\n'
###END###
def _delmsg(self):
        """Remove existing message, keeping the rest of the comments fields.
        If comments contains 'subject: ', message will prepend
        the field and a blank line."""
        if self.message:
            subj = b'subject: ' + self.message[0].lower()
            for i in pycompat.xrange(len(self.comments)):
                if subj == self.comments[i].lower():
                    del self.comments[i]
                    self.message = self.message[2:]
                    break
        ci = 0
        for mi in self.message:
            while mi != self.comments[ci]:
                ci += 1
            del self.comments[ci]
###END###
def __init__(self, ui, baseui, path, patchdir=None):
        self.basepath = path
        try:
            with open(os.path.join(path, b'patches.queue'), 'rb') as fh:
                cur = fh.read().rstrip()

            if not cur:
                curpath = os.path.join(path, b'patches')
            else:
                curpath = os.path.join(path, b'patches-' + cur)
        except IOError:
            curpath = os.path.join(path, b'patches')
        self.path = patchdir or curpath
        self.opener = vfsmod.vfs(self.path)
        self.ui = ui
        self.baseui = baseui
        self.applieddirty = False
        self.seriesdirty = False
        self.added = []
        self.seriespath = b"series"
        self.statuspath = b"status"
        self.guardspath = b"guards"
        self.activeguards = None
        self.guardsdirty = False
        # Handle mq.git as a bool with extended values
        gitmode = ui.config(b'mq', b'git').lower()
        boolmode = stringutil.parsebool(gitmode)
        if boolmode is not None:
            if boolmode:
                gitmode = b'yes'
            else:
                gitmode = b'no'
        self.gitmode = gitmode
        # deprecated config: mq.plain
        self.plainmode = ui.configbool(b'mq', b'plain')
        self.checkapplied = True
###END###
def applied(self):
        def parselines(lines):
            for l in lines:
                entry = l.split(b':', 1)
                if len(entry) > 1:
                    n, name = entry
                    yield statusentry(bin(n), name)
                elif l.strip():
                    self.ui.warn(
                        _(b'malformated mq status line: %s\n')
                        % stringutil.pprint(entry)
                    )
                # else we ignore empty lines

        try:
            lines = self.opener.read(self.statuspath).splitlines()
            return list(parselines(lines))
        except IOError as e:
            if e.errno == errno.ENOENT:
                return []
            raise
###END###
def fullseries(self):
        try:
            return self.opener.read(self.seriespath).splitlines()
        except IOError as e:
            if e.errno == errno.ENOENT:
                return []
            raise
###END###
def series(self):
        self.parseseries()
        return self.series
###END###
def seriesguards(self):
        self.parseseries()
        return self.seriesguards
###END###
def invalidate(self):
        for a in 'applied fullseries series seriesguards'.split():
            if a in self.__dict__:
                delattr(self, a)
        self.applieddirty = False
        self.seriesdirty = False
        self.guardsdirty = False
        self.activeguards = None
###END###
def diffopts(self, opts=None, patchfn=None, plain=False):
        """Return diff options tweaked for this mq use, possibly upgrading to
        git format, and possibly plain and without lossy options."""
        diffopts = patchmod.difffeatureopts(
            self.ui,
            opts,
            git=True,
            whitespace=not plain,
            formatchanging=not plain,
        )
        if self.gitmode == b'auto':
            diffopts.upgrade = True
        elif self.gitmode == b'keep':
            pass
        elif self.gitmode in (b'yes', b'no'):
            diffopts.git = self.gitmode == b'yes'
        else:
            raise error.Abort(
                _(b'mq.git option can be auto/keep/yes/no got %s')
                % self.gitmode
            )
        if patchfn:
            diffopts = self.patchopts(diffopts, patchfn)
        return diffopts
###END###
def patchopts(self, diffopts, *patches):
        """Return a copy of input diff options with git set to true if
        referenced patch is a git patch and should be preserved as such.
        """
        diffopts = diffopts.copy()
        if not diffopts.git and self.gitmode == b'keep':
            for patchfn in patches:
                patchf = self.opener(patchfn, b'r')
                # if the patch was a git patch, refresh it as a git patch
                diffopts.git = any(
                    line.startswith(b'diff --git') for line in patchf
                )
                patchf.close()
        return diffopts
###END###
def join(self, *p):
        return os.path.join(self.path, *p)
###END###
def findseries(self, patch):
        def matchpatch(l):
            l = l.split(b'#', 1)[0]
            return l.strip() == patch

        for index, l in enumerate(self.fullseries):
            if matchpatch(l):
                return index
        return None
###END###
def parseseries(self):
        self.series = []
        self.seriesguards = []
        for l in self.fullseries:
            h = l.find(b'#')
            if h == -1:
                patch = l
                comment = b''
            elif h == 0:
                continue
            else:
                patch = l[:h]
                comment = l[h:]
            patch = patch.strip()
            if patch:
                if patch in self.series:
                    raise error.Abort(
                        _(b'%s appears more than once in %s')
                        % (patch, self.join(self.seriespath))
                    )
                self.series.append(patch)
                self.seriesguards.append(self.guard_re.findall(comment))
###END###
def checkguard(self, guard):
        if not guard:
            return _(b'guard cannot be an empty string')
        bad_chars = b'# \t\r\n\f'
        first = guard[0]
        if first in b'-+':
            return _(b'guard %r starts with invalid character: %r') % (
                guard,
                first,
            )
        for c in bad_chars:
            if c in guard:
                return _(b'invalid character in guard %r: %r') % (guard, c)
###END###
def setactive(self, guards):
        for guard in guards:
            bad = self.checkguard(guard)
            if bad:
                raise error.Abort(bad)
        guards = sorted(set(guards))
        self.ui.debug(b'active guards: %s\n' % b' '.join(guards))
        self.activeguards = guards
        self.guardsdirty = True
###END###
def active(self):
        if self.activeguards is None:
            self.activeguards = []
            try:
                guards = self.opener.read(self.guardspath).split()
            except IOError as err:
                if err.errno != errno.ENOENT:
                    raise
                guards = []
            for i, guard in enumerate(guards):
                bad = self.checkguard(guard)
                if bad:
                    self.ui.warn(
                        b'%s:%d: %s\n'
                        % (self.join(self.guardspath), i + 1, bad)
                    )
                else:
                    self.activeguards.append(guard)
        return self.activeguards
###END###
def setguards(self, idx, guards):
        for g in guards:
            if len(g) < 2:
                raise error.Abort(_(b'guard %r too short') % g)
            if g[0] not in b'-+':
                raise error.Abort(_(b'guard %r starts with invalid char') % g)
            bad = self.checkguard(g[1:])
            if bad:
                raise error.Abort(bad)
        drop = self.guard_re.sub(b'', self.fullseries[idx])
        self.fullseries[idx] = drop + b''.join([b' #' + g for g in guards])
        self.parseseries()
        self.seriesdirty = True
###END###
def pushable(self, idx):
        if isinstance(idx, bytes):
            idx = self.series.index(idx)
        patchguards = self.seriesguards[idx]
        if not patchguards:
            return True, None
        guards = self.active()
        exactneg = [
            g for g in patchguards if g.startswith(b'-') and g[1:] in guards
        ]
        if exactneg:
            return False, stringutil.pprint(exactneg[0])
        pos = [g for g in patchguards if g.startswith(b'+')]
        exactpos = [g for g in pos if g[1:] in guards]
        if pos:
            if exactpos:
                return True, stringutil.pprint(exactpos[0])
            return False, b' '.join([stringutil.pprint(p) for p in pos])
        return True, b''
###END###
def explainpushable(self, idx, all_patches=False):
        if all_patches:
            write = self.ui.write
        else:
            write = self.ui.warn

        if all_patches or self.ui.verbose:
            if isinstance(idx, bytes):
                idx = self.series.index(idx)
            pushable, why = self.pushable(idx)
            if all_patches and pushable:
                if why is None:
                    write(
                        _(b'allowing %s - no guards in effect\n')
                        % self.series[idx]
                    )
                else:
                    if not why:
                        write(
                            _(b'allowing %s - no matching negative guards\n')
                            % self.series[idx]
                        )
                    else:
                        write(
                            _(b'allowing %s - guarded by %s\n')
                            % (self.series[idx], why)
                        )
            if not pushable:
                if why:
                    write(
                        _(b'skipping %s - guarded by %s\n')
                        % (self.series[idx], why)
                    )
                else:
                    write(
                        _(b'skipping %s - no matching guards\n')
                        % self.series[idx]
                    )
###END###
def savedirty(self):
        def writelist(items, path):
            fp = self.opener(path, b'wb')
            for i in items:
                fp.write(b"%s\n" % i)
            fp.close()

        if self.applieddirty:
            writelist(map(bytes, self.applied), self.statuspath)
            self.applieddirty = False
        if self.seriesdirty:
            writelist(self.fullseries, self.seriespath)
            self.seriesdirty = False
        if self.guardsdirty:
            writelist(self.activeguards, self.guardspath)
            self.guardsdirty = False
        if self.added:
            qrepo = self.qrepo()
            if qrepo:
                qrepo[None].add(f for f in self.added if f not in qrepo[None])
            self.added = []
###END###
def removeundo(self, repo):
        undo = repo.sjoin(b'undo')
        if not os.path.exists(undo):
            return
        try:
            os.unlink(undo)
        except OSError as inst:
            self.ui.warn(
                _(b'error removing undo: %s\n') % stringutil.forcebytestr(inst)
            )
###END###
def backup(self, repo, files, copy=False):
        # backup local changes in --force case
        for f in sorted(files):
            absf = repo.wjoin(f)
            if os.path.lexists(absf):
                absorig = scmutil.backuppath(self.ui, repo, f)
                self.ui.note(
                    _(b'saving current version of %s as %s\n')
                    % (f, os.path.relpath(absorig))
                )

                if copy:
                    util.copyfile(absf, absorig)
                else:
                    util.rename(absf, absorig)
###END###
def printdiff(
        self,
        repo,
        diffopts,
        node1,
        node2=None,
        files=None,
        fp=None,
        changes=None,
        opts=None,
    ):
        if opts is None:
            opts = {}
        stat = opts.get(b'stat')
        m = scmutil.match(repo[node1], files, opts)
        logcmdutil.diffordiffstat(
            self.ui,
            repo,
            diffopts,
            repo[node1],
            repo[node2],
            m,
            changes,
            stat,
            fp,
        )
###END###
def mergeone(self, repo, mergeq, head, patch, rev, diffopts):
        # first try just applying the patch
        (err, n) = self.apply(
            repo, [patch], update_status=False, strict=True, merge=rev
        )

        if err == 0:
            return (err, n)

        if n is None:
            raise error.Abort(_(b"apply failed for patch %s") % patch)

        self.ui.warn(_(b"patch didn't work out, merging %s\n") % patch)

        # apply failed, strip away that rev and merge.
        hg.clean(repo, head)
        strip(self.ui, repo, [n], update=False, backup=False)

        ctx = repo[rev]
        ret = hg.merge(ctx, remind=False)
        if ret:
            raise error.Abort(_(b"update returned %d") % ret)
        n = newcommit(repo, None, ctx.description(), ctx.user(), force=True)
        if n is None:
            raise error.Abort(_(b"repo commit failed"))
        try:
            ph = patchheader(mergeq.join(patch), self.plainmode)
        except Exception:
            raise error.Abort(_(b"unable to read %s") % patch)

        diffopts = self.patchopts(diffopts, patch)
        patchf = self.opener(patch, b"w")
        comments = bytes(ph)
        if comments:
            patchf.write(comments)
        self.printdiff(repo, diffopts, head, n, fp=patchf)
        patchf.close()
        self.removeundo(repo)
        return (0, n)
###END###
def qparents(self, repo, rev=None):
        """return the mq handled parent or p1

        In some case where mq get himself in being the parent of a merge the
        appropriate parent may be p2.
        (eg: an in progress merge started with mq disabled)

        If no parent are managed by mq, p1 is returned.
        """
        if rev is None:
            (p1, p2) = repo.dirstate.parents()
            if p2 == repo.nullid:
                return p1
            if not self.applied:
                return None
            return self.applied[-1].node
        p1, p2 = repo.changelog.parents(rev)
        if p2 != repo.nullid and p2 in [x.node for x in self.applied]:
            return p2
        return p1
###END###
def mergepatch(self, repo, mergeq, series, diffopts):
        if not self.applied:
            # each of the patches merged in will have two parents.  This
            # can confuse the qrefresh, qdiff, and strip code because it
            # needs to know which parent is actually in the patch queue.
            # so, we insert a merge marker with only one parent.  This way
            # the first patch in the queue is never a merge patch
            #
            pname = b".hg.patches.merge.marker"
            n = newcommit(repo, None, b'[mq]: merge marker', force=True)
            self.removeundo(repo)
            self.applied.append(statusentry(n, pname))
            self.applieddirty = True

        head = self.qparents(repo)

        for patch in series:
            patch = mergeq.lookup(patch, strict=True)
            if not patch:
                self.ui.warn(_(b"patch %s does not exist\n") % patch)
                return (1, None)
            pushable, reason = self.pushable(patch)
            if not pushable:
                self.explainpushable(patch, all_patches=True)
                continue
            info = mergeq.isapplied(patch)
            if not info:
                self.ui.warn(_(b"patch %s is not applied\n") % patch)
                return (1, None)
            rev = info[1]
            err, head = self.mergeone(repo, mergeq, head, patch, rev, diffopts)
            if head:
                self.applied.append(statusentry(head, patch))
                self.applieddirty = True
            if err:
                return (err, head)
        self.savedirty()
        return (0, head)
###END###
def patch(self, repo, patchfile):
        """Apply patchfile  to the working directory.
        patchfile: name of patch file"""
        files = set()
        try:
            fuzz = patchmod.patch(
                self.ui, repo, patchfile, strip=1, files=files, eolmode=None
            )
            return (True, list(files), fuzz)
        except Exception as inst:
            self.ui.note(stringutil.forcebytestr(inst) + b'\n')
            if not self.ui.verbose:
                self.ui.warn(_(b"patch failed, unable to continue (try -v)\n"))
            self.ui.traceback()
            return (False, list(files), False)
###END###
def apply(
        self,
        repo,
        series,
        list=False,
        update_status=True,
        strict=False,
        patchdir=None,
        merge=None,
        all_files=None,
        tobackup=None,
        keepchanges=False,
    ):
        wlock = lock = tr = None
        try:
            wlock = repo.wlock()
            lock = repo.lock()
            tr = repo.transaction(b"qpush")
            try:
                ret = self._apply(
                    repo,
                    series,
                    list,
                    update_status,
                    strict,
                    patchdir,
                    merge,
                    all_files=all_files,
                    tobackup=tobackup,
                    keepchanges=keepchanges,
                )
                tr.close()
                self.savedirty()
                return ret
            except AbortNoCleanup:
                tr.close()
                self.savedirty()
                raise
            except:  # re-raises
                try:
                    tr.abort()
                finally:
                    self.invalidate()
                raise
        finally:
            release(tr, lock, wlock)
            self.removeundo(repo)
###END###
def _apply(
        self,
        repo,
        series,
        list=False,
        update_status=True,
        strict=False,
        patchdir=None,
        merge=None,
        all_files=None,
        tobackup=None,
        keepchanges=False,
    ):
        """returns (error, hash)

        error = 1 for unable to read, 2 for patch failed, 3 for patch
        fuzz. tobackup is None or a set of files to backup before they
        are modified by a patch.
        """
        # TODO unify with commands.py
        if not patchdir:
            patchdir = self.path
        err = 0
        n = None
        for patchname in series:
            pushable, reason = self.pushable(patchname)
            if not pushable:
                self.explainpushable(patchname, all_patches=True)
                continue
            self.ui.status(_(b"applying %s\n") % patchname)
            pf = os.path.join(patchdir, patchname)

            try:
                ph = patchheader(self.join(patchname), self.plainmode)
            except IOError:
                self.ui.warn(_(b"unable to read %s\n") % patchname)
                err = 1
                break

            message = ph.message
            if not message:
                # The commit message should not be translated
                message = b"imported patch %s\n" % patchname
            else:
                if list:
                    # The commit message should not be translated
                    message.append(b"\nimported patch %s" % patchname)
                message = b'\n'.join(message)

            if ph.haspatch:
                if tobackup:
                    touched = patchmod.changedfiles(self.ui, repo, pf)
                    touched = set(touched) & tobackup
                    if touched and keepchanges:
                        raise AbortNoCleanup(
                            _(b"conflicting local changes found"),
                            hint=_(b"did you forget to qrefresh?"),
                        )
                    self.backup(repo, touched, copy=True)
                    tobackup = tobackup - touched
                (patcherr, files, fuzz) = self.patch(repo, pf)
                if all_files is not None:
                    all_files.update(files)
                patcherr = not patcherr
            else:
                self.ui.warn(_(b"patch %s is empty\n") % patchname)
                patcherr, files, fuzz = 0, [], 0

            if merge and files:
                # Mark as removed/merged and update dirstate parent info
                with repo.dirstate.parentchange():
                    for f in files:
                        repo.dirstate.update_file_p1(f, p1_tracked=True)
                    p1 = repo.dirstate.p1()
                    repo.setparents(p1, merge)

            if all_files and b'.hgsubstate' in all_files:
                wctx = repo[None]
                pctx = repo[b'.']
                overwrite = False
                mergedsubstate = subrepoutil.submerge(
                    repo, pctx, wctx, wctx, overwrite
                )
                files += mergedsubstate.keys()

            match = scmutil.matchfiles(repo, files or [])
            oldtip = repo.changelog.tip()
            n = newcommit(
                repo, None, message, ph.user, ph.date, match=match, force=True
            )
            if repo.changelog.tip() == oldtip:
                raise error.Abort(
                    _(b"qpush exactly duplicates child changeset")
                )
            if n is None:
                raise error.Abort(_(b"repository commit failed"))

            if update_status:
                self.applied.append(statusentry(n, patchname))

            if patcherr:
                self.ui.warn(
                    _(b"patch failed, rejects left in working directory\n")
                )
                err = 2
                break

            if fuzz and strict:
                self.ui.warn(_(b"fuzz found when applying patch, stopping\n"))
                err = 3
                break
        return (err, n)
###END###
def _cleanup(self, patches, numrevs, keep=False):
        if not keep:
            r = self.qrepo()
            if r:
                r[None].forget(patches)
            for p in patches:
                try:
                    os.unlink(self.join(p))
                except OSError as inst:
                    if inst.errno != errno.ENOENT:
                        raise

        qfinished = []
        if numrevs:
            qfinished = self.applied[:numrevs]
            del self.applied[:numrevs]
            self.applieddirty = True

        unknown = []

        sortedseries = []
        for p in patches:
            idx = self.findseries(p)
            if idx is None:
                sortedseries.append((-1, p))
            else:
                sortedseries.append((idx, p))

        sortedseries.sort(reverse=True)
        for (i, p) in sortedseries:
            if i != -1:
                del self.fullseries[i]
            else:
                unknown.append(p)

        if unknown:
            if numrevs:
                rev = {entry.name: entry.node for entry in qfinished}
                for p in unknown:
                    msg = _(b'revision %s refers to unknown patches: %s\n')
                    self.ui.warn(msg % (short(rev[p]), p))
            else:
                msg = _(b'unknown patches: %s\n')
                raise error.Abort(b''.join(msg % p for p in unknown))

        self.parseseries()
        self.seriesdirty = True
        return [entry.node for entry in qfinished]
###END###
def _revpatches(self, repo, revs):
        firstrev = repo[self.applied[0].node].rev()
        patches = []
        for i, rev in enumerate(revs):

            if rev < firstrev:
                raise error.Abort(_(b'revision %d is not managed') % rev)

            ctx = repo[rev]
            base = self.applied[i].node
            if ctx.node() != base:
                msg = _(b'cannot delete revision %d above applied patches')
                raise error.Abort(msg % rev)

            patch = self.applied[i].name
            for fmt in (b'[mq]: %s', b'imported patch %s'):
                if ctx.description() == fmt % patch:
                    msg = _(b'patch %s finalized without changeset message\n')
                    repo.ui.status(msg % patch)
                    break

            patches.append(patch)
        return patches
###END###
def finish(self, repo, revs):
        # Manually trigger phase computation to ensure phasedefaults is
        # executed before we remove the patches.
        repo._phasecache
        patches = self._revpatches(repo, sorted(revs))
        qfinished = self._cleanup(patches, len(patches))
        if qfinished and repo.ui.configbool(b'mq', b'secret'):
            # only use this logic when the secret option is added
            oldqbase = repo[qfinished[0]]
            tphase = phases.newcommitphase(repo.ui)
            if oldqbase.phase() > tphase and oldqbase.p1().phase() <= tphase:
                with repo.transaction(b'qfinish') as tr:
                    phases.advanceboundary(repo, tr, tphase, qfinished)
###END###
def delete(self, repo, patches, opts):
        if not patches and not opts.get(b'rev'):
            raise error.Abort(
                _(b'qdelete requires at least one revision or patch name')
            )

        realpatches = []
        for patch in patches:
            patch = self.lookup(patch, strict=True)
            info = self.isapplied(patch)
            if info:
                raise error.Abort(_(b"cannot delete applied patch %s") % patch)
            if patch not in self.series:
                raise error.Abort(_(b"patch %s not in series file") % patch)
            if patch not in realpatches:
                realpatches.append(patch)

        numrevs = 0
        if opts.get(b'rev'):
            if not self.applied:
                raise error.Abort(_(b'no patches applied'))
            revs = scmutil.revrange(repo, opts.get(b'rev'))
            revs.sort()
            revpatches = self._revpatches(repo, revs)
            realpatches += revpatches
            numrevs = len(revpatches)

        self._cleanup(realpatches, numrevs, opts.get(b'keep'))
###END###
def checktoppatch(self, repo):
        '''check that working directory is at qtip'''
        if self.applied:
            top = self.applied[-1].node
            patch = self.applied[-1].name
            if repo.dirstate.p1() != top:
                raise error.Abort(_(b"working directory revision is not qtip"))
            return top, patch
        return None, None
###END###
def putsubstate2changes(self, substatestate, changes):
        if isinstance(changes, list):
            mar = changes[:3]
        else:
            mar = (changes.modified, changes.added, changes.removed)
        if any((b'.hgsubstate' in files for files in mar)):
            return  # already listed up
        # not yet listed up
        if substatestate in b'a?':
            mar[1].append(b'.hgsubstate')
        elif substatestate in b'r':
            mar[2].append(b'.hgsubstate')
        else:  # modified
            mar[0].append(b'.hgsubstate')
###END###
def checklocalchanges(self, repo, force=False, refresh=True):
        excsuffix = b''
        if refresh:
            excsuffix = b', qrefresh first'
            # plain versions for i18n tool to detect them
            _(b"local changes found, qrefresh first")
            _(b"local changed subrepos found, qrefresh first")

        s = repo.status()
        if not force:
            cmdutil.checkunfinished(repo)
            if s.modified or s.added or s.removed or s.deleted:
                _(b"local changes found")  # i18n tool detection
                raise error.Abort(_(b"local changes found" + excsuffix))
            if checksubstate(repo):
                _(b"local changed subrepos found")  # i18n tool detection
                raise error.Abort(
                    _(b"local changed subrepos found" + excsuffix)
                )
        else:
            cmdutil.checkunfinished(repo, skipmerge=True)
        return s
###END###
def checkreservedname(self, name):
        if name in self._reserved:
            raise error.Abort(
                _(b'"%s" cannot be used as the name of a patch') % name
            )
        if name != name.strip():
            # whitespace is stripped by parseseries()
            raise error.Abort(
                _(b'patch name cannot begin or end with whitespace')
            )
        for prefix in (b'.hg', b'.mq'):
            if name.startswith(prefix):
                raise error.Abort(
                    _(b'patch name cannot begin with "%s"') % prefix
                )
        for c in (b'#', b':', b'\r', b'\n'):
            if c in name:
                raise error.Abort(
                    _(b'%r cannot be used in the name of a patch')
                    % pycompat.bytestr(c)
                )
###END###
def checkpatchname(self, name, force=False):
        self.checkreservedname(name)
        if not force and os.path.exists(self.join(name)):
            if os.path.isdir(self.join(name)):
                raise error.Abort(
                    _(b'"%s" already exists as a directory') % name
                )
            else:
                raise error.Abort(_(b'patch "%s" already exists') % name)
###END###
def makepatchname(self, title, fallbackname):
        """Return a suitable filename for title, adding a suffix to make
        it unique in the existing list"""
        namebase = re.sub(br'[\s\W_]+', b'_', title.lower()).strip(b'_')
        namebase = namebase[:75]  # avoid too long name (issue5117)
        if namebase:
            try:
                self.checkreservedname(namebase)
            except error.Abort:
                namebase = fallbackname
        else:
            namebase = fallbackname
        name = namebase
        i = 0
        while True:
            if name not in self.fullseries:
                try:
                    self.checkpatchname(name)
                    break
                except error.Abort:
                    pass
            i += 1
            name = b'%s__%d' % (namebase, i)
        return name
###END###
def checkkeepchanges(self, keepchanges, force):
        if force and keepchanges:
            raise error.Abort(_(b'cannot use both --force and --keep-changes'))
###END###
def new(self, repo, patchfn, *pats, **opts):
        """options:
        msg: a string or a no-argument function returning a string
        """
        opts = pycompat.byteskwargs(opts)
        msg = opts.get(b'msg')
        edit = opts.get(b'edit')
        editform = opts.get(b'editform', b'mq.qnew')
        user = opts.get(b'user')
        date = opts.get(b'date')
        if date:
            date = dateutil.parsedate(date)
        diffopts = self.diffopts({b'git': opts.get(b'git')}, plain=True)
        if opts.get(b'checkname', True):
            self.checkpatchname(patchfn)
        inclsubs = checksubstate(repo)
        if inclsubs:
            substatestate = repo.dirstate[b'.hgsubstate']
        if opts.get(b'include') or opts.get(b'exclude') or pats:
            # detect missing files in pats
            def badfn(f, msg):
                if f != b'.hgsubstate':  # .hgsubstate is auto-created
                    raise error.Abort(b'%s: %s' % (f, msg))

            match = scmutil.match(repo[None], pats, opts, badfn=badfn)
            changes = repo.status(match=match)
        else:
            changes = self.checklocalchanges(repo, force=True)
        commitfiles = list(inclsubs)
        commitfiles.extend(changes.modified)
        commitfiles.extend(changes.added)
        commitfiles.extend(changes.removed)
        match = scmutil.matchfiles(repo, commitfiles)
        if len(repo[None].parents()) > 1:
            raise error.Abort(_(b'cannot manage merge changesets'))
        self.checktoppatch(repo)
        insert = self.fullseriesend()
        with repo.wlock():
            try:
                # if patch file write fails, abort early
                p = self.opener(patchfn, b"w")
            except IOError as e:
                raise error.Abort(
                    _(b'cannot write patch "%s": %s')
                    % (patchfn, encoding.strtolocal(e.strerror))
                )
            try:
                defaultmsg = b"[mq]: %s" % patchfn
                editor = cmdutil.getcommiteditor(editform=editform)
                if edit:

                    def finishdesc(desc):
                        if desc.rstrip():
                            return desc
                        else:
                            return defaultmsg

                    # i18n: this message is shown in editor with "HG: " prefix
                    extramsg = _(b'Leave message empty to use default message.')
                    editor = cmdutil.getcommiteditor(
                        finishdesc=finishdesc,
                        extramsg=extramsg,
                        editform=editform,
                    )
                    commitmsg = msg
                else:
                    commitmsg = msg or defaultmsg

                n = newcommit(
                    repo,
                    None,
                    commitmsg,
                    user,
                    date,
                    match=match,
                    force=True,
                    editor=editor,
                )
                if n is None:
                    raise error.Abort(_(b"repo commit failed"))
                try:
                    self.fullseries[insert:insert] = [patchfn]
                    self.applied.append(statusentry(n, patchfn))
                    self.parseseries()
                    self.seriesdirty = True
                    self.applieddirty = True
                    nctx = repo[n]
                    ph = patchheader(self.join(patchfn), self.plainmode)
                    if user:
                        ph.setuser(user)
                    if date:
                        ph.setdate(b'%d %d' % date)
                    ph.setparent(hex(nctx.p1().node()))
                    msg = nctx.description().strip()
                    if msg == defaultmsg.strip():
                        msg = b''
                    ph.setmessage(msg)
                    p.write(bytes(ph))
                    if commitfiles:
                        parent = self.qparents(repo, n)
                        if inclsubs:
                            self.putsubstate2changes(substatestate, changes)
                        chunks = patchmod.diff(
                            repo,
                            node1=parent,
                            node2=n,
                            changes=changes,
                            opts=diffopts,
                        )
                        for chunk in chunks:
                            p.write(chunk)
                    p.close()
                    r = self.qrepo()
                    if r:
                        r[None].add([patchfn])
                except:  # re-raises
                    repo.rollback()
                    raise
            except Exception:
                patchpath = self.join(patchfn)
                try:
                    os.unlink(patchpath)
                except OSError:
                    self.ui.warn(_(b'error unlinking %s\n') % patchpath)
                raise
            self.removeundo(repo)
###END###
def isapplied(self, patch):
        """returns (index, rev, patch)"""
        for i, a in enumerate(self.applied):
            if a.name == patch:
                return (i, a.node, a.name)
        return None
###END###
def lookup(self, patch, strict=False):
        def partialname(s):
            if s in self.series:
                return s
            matches = [x for x in self.series if s in x]
            if len(matches) > 1:
                self.ui.warn(_(b'patch name "%s" is ambiguous:\n') % s)
                for m in matches:
                    self.ui.warn(b'  %s\n' % m)
                return None
            if matches:
                return matches[0]
            if self.series and self.applied:
                if s == b'qtip':
                    return self.series[self.seriesend(True) - 1]
                if s == b'qbase':
                    return self.series[0]
            return None

        if patch in self.series:
            return patch

        if not os.path.isfile(self.join(patch)):
            try:
                sno = int(patch)
            except (ValueError, OverflowError):
                pass
            else:
                if -len(self.series) <= sno < len(self.series):
                    return self.series[sno]

            if not strict:
                res = partialname(patch)
                if res:
                    return res
                minus = patch.rfind(b'-')
                if minus >= 0:
                    res = partialname(patch[:minus])
                    if res:
                        i = self.series.index(res)
                        try:
                            off = int(patch[minus + 1 :] or 1)
                        except (ValueError, OverflowError):
                            pass
                        else:
                            if i - off >= 0:
                                return self.series[i - off]
                plus = patch.rfind(b'+')
                if plus >= 0:
                    res = partialname(patch[:plus])
                    if res:
                        i = self.series.index(res)
                        try:
                            off = int(patch[plus + 1 :] or 1)
                        except (ValueError, OverflowError):
                            pass
                        else:
                            if i + off < len(self.series):
                                return self.series[i + off]
        raise error.Abort(_(b"patch %s not in series") % patch)
###END###
def push(
        self,
        repo,
        patch=None,
        force=False,
        list=False,
        mergeq=None,
        all=False,
        move=False,
        exact=False,
        nobackup=False,
        keepchanges=False,
    ):
        self.checkkeepchanges(keepchanges, force)
        diffopts = self.diffopts()
        with repo.wlock():
            heads = []
            for hs in repo.branchmap().iterheads():
                heads.extend(hs)
            if not heads:
                heads = [repo.nullid]
            if repo.dirstate.p1() not in heads and not exact:
                self.ui.status(_(b"(working directory not at a head)\n"))

            if not self.series:
                self.ui.warn(_(b'no patches in series\n'))
                return 0

            # Suppose our series file is: A B C and the current 'top'
            # patch is B. qpush C should be performed (moving forward)
            # qpush B is a NOP (no change) qpush A is an error (can't
            # go backwards with qpush)
            if patch:
                patch = self.lookup(patch)
                info = self.isapplied(patch)
                if info and info[0] >= len(self.applied) - 1:
                    self.ui.warn(
                        _(b'qpush: %s is already at the top\n') % patch
                    )
                    return 0

                pushable, reason = self.pushable(patch)
                if pushable:
                    if self.series.index(patch) < self.seriesend():
                        raise error.Abort(
                            _(b"cannot push to a previous patch: %s") % patch
                        )
                else:
                    if reason:
                        reason = _(b'guarded by %s') % reason
                    else:
                        reason = _(b'no matching guards')
                    self.ui.warn(
                        _(b"cannot push '%s' - %s\n") % (patch, reason)
                    )
                    return 1
            elif all:
                patch = self.series[-1]
                if self.isapplied(patch):
                    self.ui.warn(_(b'all patches are currently applied\n'))
                    return 0

            # Following the above example, starting at 'top' of B:
            # qpush should be performed (pushes C), but a subsequent
            # qpush without an argument is an error (nothing to
            # apply). This allows a loop of "...while hg qpush..." to
            # work as it detects an error when done
            start = self.seriesend()
            if start == len(self.series):
                self.ui.warn(_(b'patch series already fully applied\n'))
                return 1
            if not force and not keepchanges:
                self.checklocalchanges(repo, refresh=self.applied)

            if exact:
                if keepchanges:
                    raise error.Abort(
                        _(b"cannot use --exact and --keep-changes together")
                    )
                if move:
                    raise error.Abort(
                        _(b'cannot use --exact and --move together')
                    )
                if self.applied:
                    raise error.Abort(
                        _(b'cannot push --exact with applied patches')
                    )
                root = self.series[start]
                target = patchheader(self.join(root), self.plainmode).parent
                if not target:
                    raise error.Abort(
                        _(b"%s does not have a parent recorded") % root
                    )
                if not repo[target] == repo[b'.']:
                    hg.update(repo, target)

            if move:
                if not patch:
                    raise error.Abort(_(b"please specify the patch to move"))
                for fullstart, rpn in enumerate(self.fullseries):
                    # strip markers for patch guards
                    if self.guard_re.split(rpn, 1)[0] == self.series[start]:
                        break
                for i, rpn in enumerate(self.fullseries[fullstart:]):
                    # strip markers for patch guards
                    if self.guard_re.split(rpn, 1)[0] == patch:
                        break
                index = fullstart + i
                assert index < len(self.fullseries)
                fullpatch = self.fullseries[index]
                del self.fullseries[index]
                self.fullseries.insert(fullstart, fullpatch)
                self.parseseries()
                self.seriesdirty = True

            self.applieddirty = True
            if start > 0:
                self.checktoppatch(repo)
            if not patch:
                patch = self.series[start]
                end = start + 1
            else:
                end = self.series.index(patch, start) + 1

            tobackup = set()
            if (not nobackup and force) or keepchanges:
                status = self.checklocalchanges(repo, force=True)
                if keepchanges:
                    tobackup.update(
                        status.modified
                        + status.added
                        + status.removed
                        + status.deleted
                    )
                else:
                    tobackup.update(status.modified + status.added)

            s = self.series[start:end]
            all_files = set()
            try:
                if mergeq:
                    ret = self.mergepatch(repo, mergeq, s, diffopts)
                else:
                    ret = self.apply(
                        repo,
                        s,
                        list,
                        all_files=all_files,
                        tobackup=tobackup,
                        keepchanges=keepchanges,
                    )
            except AbortNoCleanup:
                raise
            except:  # re-raises
                self.ui.warn(_(b'cleaning up working directory...\n'))
                cmdutil.revert(
                    self.ui,
                    repo,
                    repo[b'.'],
                    no_backup=True,
                )
                # only remove unknown files that we know we touched or
                # created while patching
                for f in all_files:
                    if f not in repo.dirstate:
                        repo.wvfs.unlinkpath(f, ignoremissing=True)
                self.ui.warn(_(b'done\n'))
                raise

            if not self.applied:
                return ret[0]
            top = self.applied[-1].name
            if ret[0] and ret[0] > 1:
                msg = _(b"errors during apply, please fix and qrefresh %s\n")
                self.ui.write(msg % top)
            else:
                self.ui.write(_(b"now at: %s\n") % top)
            return ret[0]
###END###
def pop(
        self,
        repo,
        patch=None,
        force=False,
        update=True,
        all=False,
        nobackup=False,
        keepchanges=False,
    ):
        self.checkkeepchanges(keepchanges, force)
        with repo.wlock():
            if patch:
                # index, rev, patch
                info = self.isapplied(patch)
                if not info:
                    patch = self.lookup(patch)
                info = self.isapplied(patch)
                if not info:
                    raise error.Abort(_(b"patch %s is not applied") % patch)

            if not self.applied:
                # Allow qpop -a to work repeatedly,
                # but not qpop without an argument
                self.ui.warn(_(b"no patches applied\n"))
                return not all

            if all:
                start = 0
            elif patch:
                start = info[0] + 1
            else:
                start = len(self.applied) - 1

            if start >= len(self.applied):
                self.ui.warn(_(b"qpop: %s is already at the top\n") % patch)
                return

            if not update:
                parents = repo.dirstate.parents()
                rr = [x.node for x in self.applied]
                for p in parents:
                    if p in rr:
                        self.ui.warn(_(b"qpop: forcing dirstate update\n"))
                        update = True
            else:
                parents = [p.node() for p in repo[None].parents()]
                update = any(
                    entry.node in parents for entry in self.applied[start:]
                )

            tobackup = set()
            if update:
                s = self.checklocalchanges(repo, force=force or keepchanges)
                if force:
                    if not nobackup:
                        tobackup.update(s.modified + s.added)
                elif keepchanges:
                    tobackup.update(
                        s.modified + s.added + s.removed + s.deleted
                    )

            self.applieddirty = True
            end = len(self.applied)
            rev = self.applied[start].node

            try:
                heads = repo.changelog.heads(rev)
            except error.LookupError:
                node = short(rev)
                raise error.Abort(_(b'trying to pop unknown node %s') % node)

            if heads != [self.applied[-1].node]:
                raise error.Abort(
                    _(
                        b"popping would remove a revision not "
                        b"managed by this patch queue"
                    )
                )
            if not repo[self.applied[-1].node].mutable():
                raise error.Abort(
                    _(b"popping would remove a public revision"),
                    hint=_(b"see 'hg help phases' for details"),
                )

            # we know there are no local changes, so we can make a simplified
            # form of hg.update.
            if update:
                qp = self.qparents(repo, rev)
                ctx = repo[qp]
                st = repo.status(qp, b'.')
                m, a, r, d = st.modified, st.added, st.removed, st.deleted
                if d:
                    raise error.Abort(_(b"deletions found between repo revs"))

                tobackup = set(a + m + r) & tobackup
                if keepchanges and tobackup:
                    raise error.Abort(_(b"local changes found, qrefresh first"))
                self.backup(repo, tobackup)
                with repo.dirstate.parentchange():
                    for f in a:
                        repo.wvfs.unlinkpath(f, ignoremissing=True)
                        repo.dirstate.update_file(
                            f, p1_tracked=False, wc_tracked=False
                        )
                    for f in m + r:
                        fctx = ctx[f]
                        repo.wwrite(f, fctx.data(), fctx.flags())
                        repo.dirstate.update_file(
                            f, p1_tracked=True, wc_tracked=True
                        )
                    repo.setparents(qp, repo.nullid)
            for patch in reversed(self.applied[start:end]):
                self.ui.status(_(b"popping %s\n") % patch.name)
            del self.applied[start:end]
            strip(self.ui, repo, [rev], update=False, backup=False)
            for s, state in repo[b'.'].substate.items():
                repo[b'.'].sub(s).get(state)
            if self.applied:
                self.ui.write(_(b"now at: %s\n") % self.applied[-1].name)
            else:
                self.ui.write(_(b"patch queue now empty\n"))
###END###
def diff(self, repo, pats, opts):
        top, patch = self.checktoppatch(repo)
        if not top:
            self.ui.write(_(b"no patches applied\n"))
            return
        qp = self.qparents(repo, top)
        if opts.get(b'reverse'):
            node1, node2 = None, qp
        else:
            node1, node2 = qp, None
        diffopts = self.diffopts(opts, patch)
        self.printdiff(repo, diffopts, node1, node2, files=pats, opts=opts)
###END###
def refresh(self, repo, pats=None, **opts):
        opts = pycompat.byteskwargs(opts)
        if not self.applied:
            self.ui.write(_(b"no patches applied\n"))
            return 1
        msg = opts.get(b'msg', b'').rstrip()
        edit = opts.get(b'edit')
        editform = opts.get(b'editform', b'mq.qrefresh')
        newuser = opts.get(b'user')
        newdate = opts.get(b'date')
        if newdate:
            newdate = b'%d %d' % dateutil.parsedate(newdate)
        wlock = repo.wlock()

        try:
            self.checktoppatch(repo)
            (top, patchfn) = (self.applied[-1].node, self.applied[-1].name)
            if repo.changelog.heads(top) != [top]:
                raise error.Abort(
                    _(b"cannot qrefresh a revision with children")
                )
            if not repo[top].mutable():
                raise error.Abort(
                    _(b"cannot qrefresh public revision"),
                    hint=_(b"see 'hg help phases' for details"),
                )

            cparents = repo.changelog.parents(top)
            patchparent = self.qparents(repo, top)

            inclsubs = checksubstate(repo, patchparent)
            if inclsubs:
                substatestate = repo.dirstate[b'.hgsubstate']

            ph = patchheader(self.join(patchfn), self.plainmode)
            diffopts = self.diffopts(
                {b'git': opts.get(b'git')}, patchfn, plain=True
            )
            if newuser:
                ph.setuser(newuser)
            if newdate:
                ph.setdate(newdate)
            ph.setparent(hex(patchparent))

            # only commit new patch when write is complete
            patchf = self.opener(patchfn, b'w', atomictemp=True)

            # update the dirstate in place, strip off the qtip commit
            # and then commit.
            #
            # this should really read:
            #   st = repo.status(top, patchparent)
            # but we do it backwards to take advantage of manifest/changelog
            # caching against the next repo.status call
            st = repo.status(patchparent, top)
            mm, aa, dd = st.modified, st.added, st.removed
            ctx = repo[top]
            aaa = aa[:]
            match1 = scmutil.match(repo[None], pats, opts)
            # in short mode, we only diff the files included in the
            # patch already plus specified files
            if opts.get(b'short'):
                # if amending a patch, we start with existing
                # files plus specified files - unfiltered
                match = scmutil.matchfiles(repo, mm + aa + dd + match1.files())
                # filter with include/exclude options
                match1 = scmutil.match(repo[None], opts=opts)
            else:
                match = scmutil.matchall(repo)
            stb = repo.status(match=match)
            m, a, r, d = stb.modified, stb.added, stb.removed, stb.deleted
            mm = set(mm)
            aa = set(aa)
            dd = set(dd)

            # we might end up with files that were added between
            # qtip and the dirstate parent, but then changed in the
            # local dirstate. in this case, we want them to only
            # show up in the added section
            for x in m:
                if x not in aa:
                    mm.add(x)
            # we might end up with files added by the local dirstate that
            # were deleted by the patch.  In this case, they should only
            # show up in the changed section.
            for x in a:
                if x in dd:
                    dd.remove(x)
                    mm.add(x)
                else:
                    aa.add(x)
            # make sure any files deleted in the local dirstate
            # are not in the add or change column of the patch
            forget = []
            for x in d + r:
                if x in aa:
                    aa.remove(x)
                    forget.append(x)
                    continue
                else:
                    mm.discard(x)
                dd.add(x)

            m = list(mm)
            r = list(dd)
            a = list(aa)

            # create 'match' that includes the files to be recommitted.
            # apply match1 via repo.status to ensure correct case handling.
            st = repo.status(patchparent, match=match1)
            cm, ca, cr, cd = st.modified, st.added, st.removed, st.deleted
            allmatches = set(cm + ca + cr + cd)
            refreshchanges = [x.intersection(allmatches) for x in (mm, aa, dd)]

            files = set(inclsubs)
            for x in refreshchanges:
                files.update(x)
            match = scmutil.matchfiles(repo, files)

            bmlist = repo[top].bookmarks()

            with repo.dirstate.parentchange():
                # XXX do we actually need the dirstateguard
                dsguard = None
                try:
                    dsguard = dirstateguard.dirstateguard(repo, b'mq.refresh')
                    if diffopts.git or diffopts.upgrade:
                        copies = {}
                        for dst in a:
                            src = repo.dirstate.copied(dst)
                            # during qfold, the source file for copies may
                            # be removed. Treat this as a simple add.
                            if src is not None and src in repo.dirstate:
                                copies.setdefault(src, []).append(dst)
                            repo.dirstate.update_file(
                                dst, p1_tracked=False, wc_tracked=True
                            )
                        # remember the copies between patchparent and qtip
                        for dst in aaa:
                            src = ctx[dst].copysource()
                            if src:
                                copies.setdefault(src, []).extend(
                                    copies.get(dst, [])
                                )
                                if dst in a:
                                    copies[src].append(dst)
                            # we can't copy a file created by the patch itself
                            if dst in copies:
                                del copies[dst]
                        for src, dsts in pycompat.iteritems(copies):
                            for dst in dsts:
                                repo.dirstate.copy(src, dst)
                    else:
                        for dst in a:
                            repo.dirstate.update_file(
                                dst, p1_tracked=False, wc_tracked=True
                            )
                        # Drop useless copy information
                        for f in list(repo.dirstate.copies()):
                            repo.dirstate.copy(None, f)
                    for f in r:
                        repo.dirstate.update_file_p1(f, p1_tracked=True)
                    # if the patch excludes a modified file, mark that
                    # file with mtime=0 so status can see it.
                    mm = []
                    for i in pycompat.xrange(len(m) - 1, -1, -1):
                        if not match1(m[i]):
                            mm.append(m[i])
                            del m[i]
                    for f in m:
                        repo.dirstate.update_file_p1(f, p1_tracked=True)
                    for f in mm:
                        repo.dirstate.update_file_p1(f, p1_tracked=True)
                    for f in forget:
                        repo.dirstate.update_file_p1(f, p1_tracked=False)

                    user = ph.user or ctx.user()

                    oldphase = repo[top].phase()

                    # assumes strip can roll itself back if interrupted
                    repo.setparents(*cparents)
                    self.applied.pop()
                    self.applieddirty = True
                    strip(self.ui, repo, [top], update=False, backup=False)
                    dsguard.close()
                finally:
                    release(dsguard)

            try:
                # might be nice to attempt to roll back strip after this

                defaultmsg = b"[mq]: %s" % patchfn
                editor = cmdutil.getcommiteditor(editform=editform)
                if edit:

                    def finishdesc(desc):
                        if desc.rstrip():
                            ph.setmessage(desc)
                            return desc
                        return defaultmsg

                    # i18n: this message is shown in editor with "HG: " prefix
                    extramsg = _(b'Leave message empty to use default message.')
                    editor = cmdutil.getcommiteditor(
                        finishdesc=finishdesc,
                        extramsg=extramsg,
                        editform=editform,
                    )
                    message = msg or b"\n".join(ph.message)
                elif not msg:
                    if not ph.message:
                        message = defaultmsg
                    else:
                        message = b"\n".join(ph.message)
                else:
                    message = msg
                    ph.setmessage(msg)

                # Ensure we create a new changeset in the same phase than
                # the old one.
                lock = tr = None
                try:
                    lock = repo.lock()
                    tr = repo.transaction(b'mq')
                    n = newcommit(
                        repo,
                        oldphase,
                        message,
                        user,
                        ph.date,
                        match=match,
                        force=True,
                        editor=editor,
                    )
                    # only write patch after a successful commit
                    c = [list(x) for x in refreshchanges]
                    if inclsubs:
                        self.putsubstate2changes(substatestate, c)
                    chunks = patchmod.diff(
                        repo, patchparent, changes=c, opts=diffopts
                    )
                    comments = bytes(ph)
                    if comments:
                        patchf.write(comments)
                    for chunk in chunks:
                        patchf.write(chunk)
                    patchf.close()

                    marks = repo._bookmarks
                    marks.applychanges(repo, tr, [(bm, n) for bm in bmlist])
                    tr.close()

                    self.applied.append(statusentry(n, patchfn))
                finally:
                    lockmod.release(tr, lock)
            except:  # re-raises
                ctx = repo[cparents[0]]
                repo.dirstate.rebuild(ctx.node(), ctx.manifest())
                self.savedirty()
                self.ui.warn(
                    _(
                        b'qrefresh interrupted while patch was popped! '
                        b'(revert --all, qpush to recover)\n'
                    )
                )
                raise
        finally:
            wlock.release()
            self.removeundo(repo)
###END###
def init(self, repo, create=False):
        if not create and os.path.isdir(self.path):
            raise error.Abort(_(b"patch queue directory already exists"))
        try:
            os.mkdir(self.path)
        except OSError as inst:
            if inst.errno != errno.EEXIST or not create:
                raise
        if create:
            return self.qrepo(create=True)
###END###
def unapplied(self, repo, patch=None):
        if patch and patch not in self.series:
            raise error.Abort(_(b"patch %s is not in series file") % patch)
        if not patch:
            start = self.seriesend()
        else:
            start = self.series.index(patch) + 1
        unapplied = []
        for i in pycompat.xrange(start, len(self.series)):
            pushable, reason = self.pushable(i)
            if pushable:
                unapplied.append((i, self.series[i]))
            self.explainpushable(i)
        return unapplied
###END###
def qseries(
        self,
        repo,
        missing=None,
        start=0,
        length=None,
        status=None,
        summary=False,
    ):
        def displayname(pfx, patchname, state):
            if pfx:
                self.ui.write(pfx)
            if summary:
                ph = patchheader(self.join(patchname), self.plainmode)
                if ph.message:
                    msg = ph.message[0]
                else:
                    msg = b''

                if self.ui.formatted():
                    width = self.ui.termwidth() - len(pfx) - len(patchname) - 2
                    if width > 0:
                        msg = stringutil.ellipsis(msg, width)
                    else:
                        msg = b''
                self.ui.write(patchname, label=b'qseries.' + state)
                self.ui.write(b': ')
                self.ui.write(msg, label=b'qseries.message.' + state)
            else:
                self.ui.write(patchname, label=b'qseries.' + state)
            self.ui.write(b'\n')

        applied = {p.name for p in self.applied}
        if length is None:
            length = len(self.series) - start
        if not missing:
            if self.ui.verbose:
                idxwidth = len(b"%d" % (start + length - 1))
            for i in pycompat.xrange(start, start + length):
                patch = self.series[i]
                if patch in applied:
                    char, state = b'A', b'applied'
                elif self.pushable(i)[0]:
                    char, state = b'U', b'unapplied'
                else:
                    char, state = b'G', b'guarded'
                pfx = b''
                if self.ui.verbose:
                    pfx = b'%*d %s ' % (idxwidth, i, char)
                elif status and status != char:
                    continue
                displayname(pfx, patch, state)
        else:
            msng_list = []
            for root, dirs, files in os.walk(self.path):
                d = root[len(self.path) + 1 :]
                for f in files:
                    fl = os.path.join(d, f)
                    if (
                        fl not in self.series
                        and fl
                        not in (
                            self.statuspath,
                            self.seriespath,
                            self.guardspath,
                        )
                        and not fl.startswith(b'.')
                    ):
                        msng_list.append(fl)
            for x in sorted(msng_list):
                pfx = self.ui.verbose and b'D ' or b''
                displayname(pfx, x, b'missing')
###END###
def issaveline(self, l):
        if l.name == b'.hg.patches.save.line':
            return True
###END###
def qrepo(self, create=False):
        ui = self.baseui.copy()
        # copy back attributes set by ui.pager()
        if self.ui.pageractive and not ui.pageractive:
            ui.pageractive = self.ui.pageractive
            # internal config: ui.formatted
            ui.setconfig(
                b'ui',
                b'formatted',
                self.ui.config(b'ui', b'formatted'),
                b'mqpager',
            )
            ui.setconfig(
                b'ui',
                b'interactive',
                self.ui.config(b'ui', b'interactive'),
                b'mqpager',
            )
        if create or os.path.isdir(self.join(b".hg")):
            return hg.repository(ui, path=self.path, create=create)
###END###
def restore(self, repo, rev, delete=None, qupdate=None):
        desc = repo[rev].description().strip()
        lines = desc.splitlines()
        datastart = None
        series = []
        applied = []
        qpp = None
        for i, line in enumerate(lines):
            if line == b'Patch Data:':
                datastart = i + 1
            elif line.startswith(b'Dirstate:'):
                l = line.rstrip()
                l = l[10:].split(b' ')
                qpp = [bin(x) for x in l]
            elif datastart is not None:
                l = line.rstrip()
                n, name = l.split(b':', 1)
                if n:
                    applied.append(statusentry(bin(n), name))
                else:
                    series.append(l)
        if datastart is None:
            self.ui.warn(_(b"no saved patch data found\n"))
            return 1
        self.ui.warn(_(b"restoring status: %s\n") % lines[0])
        self.fullseries = series
        self.applied = applied
        self.parseseries()
        self.seriesdirty = True
        self.applieddirty = True
        heads = repo.changelog.heads()
        if delete:
            if rev not in heads:
                self.ui.warn(_(b"save entry has children, leaving it alone\n"))
            else:
                self.ui.warn(_(b"removing save entry %s\n") % short(rev))
                pp = repo.dirstate.parents()
                if rev in pp:
                    update = True
                else:
                    update = False
                strip(self.ui, repo, [rev], update=update, backup=False)
        if qpp:
            self.ui.warn(
                _(b"saved queue repository parents: %s %s\n")
                % (short(qpp[0]), short(qpp[1]))
            )
            if qupdate:
                self.ui.status(_(b"updating queue directory\n"))
                r = self.qrepo()
                if not r:
                    self.ui.warn(_(b"unable to load queue repository\n"))
                    return 1
                hg.clean(r, qpp[0])
###END###
def save(self, repo, msg=None):
        if not self.applied:
            self.ui.warn(_(b"save: no patches applied, exiting\n"))
            return 1
        if self.issaveline(self.applied[-1]):
            self.ui.warn(_(b"status is already saved\n"))
            return 1

        if not msg:
            msg = _(b"hg patches saved state")
        else:
            msg = b"hg patches: " + msg.rstrip(b'\r\n')
        r = self.qrepo()
        if r:
            pp = r.dirstate.parents()
            msg += b"\nDirstate: %s %s" % (hex(pp[0]), hex(pp[1]))
        msg += b"\n\nPatch Data:\n"
        msg += b''.join(b'%s\n' % x for x in self.applied)
        msg += b''.join(b':%s\n' % x for x in self.fullseries)
        n = repo.commit(msg, force=True)
        if not n:
            self.ui.warn(_(b"repo commit failed\n"))
            return 1
        self.applied.append(statusentry(n, b'.hg.patches.save.line'))
        self.applieddirty = True
        self.removeundo(repo)
###END###
def fullseriesend(self):
        if self.applied:
            p = self.applied[-1].name
            end = self.findseries(p)
            if end is None:
                return len(self.fullseries)
            return end + 1
        return 0
###END###
def seriesend(self, all_patches=False):
        """If all_patches is False, return the index of the next pushable patch
        in the series, or the series length. If all_patches is True, return the
        index of the first patch past the last applied one.
        """
        end = 0

        def nextpatch(start):
            if all_patches or start >= len(self.series):
                return start
            for i in pycompat.xrange(start, len(self.series)):
                p, reason = self.pushable(i)
                if p:
                    return i
                self.explainpushable(i)
            return len(self.series)

        if self.applied:
            p = self.applied[-1].name
            try:
                end = self.series.index(p)
            except ValueError:
                return 0
            return nextpatch(end + 1)
        return nextpatch(end)
###END###
def appliedname(self, index):
        pname = self.applied[index].name
        if not self.ui.verbose:
            p = pname
        else:
            p = (b"%d" % self.series.index(pname)) + b" " + pname
        return p
###END###
def qimport(
        self,
        repo,
        files,
        patchname=None,
        rev=None,
        existing=None,
        force=None,
        git=False,
    ):
        def checkseries(patchname):
            if patchname in self.series:
                raise error.Abort(
                    _(b'patch %s is already in the series file') % patchname
                )

        if rev:
            if files:
                raise error.Abort(
                    _(b'option "-r" not valid when importing files')
                )
            rev = scmutil.revrange(repo, rev)
            rev.sort(reverse=True)
        elif not files:
            raise error.Abort(_(b'no files or revisions specified'))
        if (len(files) > 1 or len(rev) > 1) and patchname:
            raise error.Abort(
                _(b'option "-n" not valid when importing multiple patches')
            )
        imported = []
        if rev:
            # If mq patches are applied, we can only import revisions
            # that form a linear path to qbase.
            # Otherwise, they should form a linear path to a head.
            heads = repo.changelog.heads(repo.changelog.node(rev.first()))
            if len(heads) > 1:
                raise error.Abort(
                    _(b'revision %d is the root of more than one branch')
                    % rev.last()
                )
            if self.applied:
                base = repo.changelog.node(rev.first())
                if base in [n.node for n in self.applied]:
                    raise error.Abort(
                        _(b'revision %d is already managed') % rev.first()
                    )
                if heads != [self.applied[-1].node]:
                    raise error.Abort(
                        _(b'revision %d is not the parent of the queue')
                        % rev.first()
                    )
                base = repo.changelog.rev(self.applied[0].node)
                lastparent = repo.changelog.parentrevs(base)[0]
            else:
                if heads != [repo.changelog.node(rev.first())]:
                    raise error.Abort(
                        _(b'revision %d has unmanaged children') % rev.first()
                    )
                lastparent = None

            diffopts = self.diffopts({b'git': git})
            with repo.transaction(b'qimport') as tr:
                for r in rev:
                    if not repo[r].mutable():
                        raise error.Abort(
                            _(b'revision %d is not mutable') % r,
                            hint=_(b"see 'hg help phases' " b'for details'),
                        )
                    p1, p2 = repo.changelog.parentrevs(r)
                    n = repo.changelog.node(r)
                    if p2 != nullrev:
                        raise error.Abort(
                            _(b'cannot import merge revision %d') % r
                        )
                    if lastparent and lastparent != r:
                        raise error.Abort(
                            _(b'revision %d is not the parent of %d')
                            % (r, lastparent)
                        )
                    lastparent = p1

                    if not patchname:
                        patchname = self.makepatchname(
                            repo[r].description().split(b'\n', 1)[0],
                            b'%d.diff' % r,
                        )
                    checkseries(patchname)
                    self.checkpatchname(patchname, force)
                    self.fullseries.insert(0, patchname)

                    with self.opener(patchname, b"w") as fp:
                        cmdutil.exportfile(repo, [n], fp, opts=diffopts)

                    se = statusentry(n, patchname)
                    self.applied.insert(0, se)

                    self.added.append(patchname)
                    imported.append(patchname)
                    patchname = None
                    if rev and repo.ui.configbool(b'mq', b'secret'):
                        # if we added anything with --rev, move the secret root
                        phases.retractboundary(repo, tr, phases.secret, [n])
                    self.parseseries()
                    self.applieddirty = True
                    self.seriesdirty = True

        for i, filename in enumerate(files):
            if existing:
                if filename == b'-':
                    raise error.Abort(
                        _(b'-e is incompatible with import from -')
                    )
                filename = normname(filename)
                self.checkreservedname(filename)
                if urlutil.url(filename).islocal():
                    originpath = self.join(filename)
                    if not os.path.isfile(originpath):
                        raise error.Abort(
                            _(b"patch %s does not exist") % filename
                        )

                if patchname:
                    self.checkpatchname(patchname, force)

                    self.ui.write(
                        _(b'renaming %s to %s\n') % (filename, patchname)
                    )
                    util.rename(originpath, self.join(patchname))
                else:
                    patchname = filename

            else:
                if filename == b'-' and not patchname:
                    raise error.Abort(
                        _(b'need --name to import a patch from -')
                    )
                elif not patchname:
                    patchname = normname(
                        os.path.basename(filename.rstrip(b'/'))
                    )
                self.checkpatchname(patchname, force)
                try:
                    if filename == b'-':
                        text = self.ui.fin.read()
                    else:
                        fp = hg.openpath(self.ui, filename)
                        text = fp.read()
                        fp.close()
                except (OSError, IOError):
                    raise error.Abort(_(b"unable to read file %s") % filename)
                patchf = self.opener(patchname, b"w")
                patchf.write(text)
                patchf.close()
            if not force:
                checkseries(patchname)
            if patchname not in self.series:
                index = self.fullseriesend() + i
                self.fullseries[index:index] = [patchname]
            self.parseseries()
            self.seriesdirty = True
            self.ui.warn(_(b"adding %s to series file\n") % patchname)
            self.added.append(patchname)
            imported.append(patchname)
            patchname = None

        self.removeundo(repo)
        return imported
###END###
def mq(self):
            return queue(self.ui, self.baseui, self.path)
###END###
def invalidateall(self):
            super(mqrepo, self).invalidateall()
            if localrepo.hasunfilteredcache(self, 'mq'):
                # recreate mq in case queue path was changed
                delattr(self.unfiltered(), 'mq')
###END###
def abortifwdirpatched(self, errmsg, force=False):
            if self.mq.applied and self.mq.checkapplied and not force:
                parents = self.dirstate.parents()
                patches = [s.node for s in self.mq.applied]
                if any(p in patches for p in parents):
                    raise error.Abort(errmsg)
###END###
def commit(
            self,
            text=b"",
            user=None,
            date=None,
            match=None,
            force=False,
            editor=False,
            extra=None,
        ):
            if extra is None:
                extra = {}
            self.abortifwdirpatched(
                _(b'cannot commit over an applied mq patch'), force
            )

            return super(mqrepo, self).commit(
                text, user, date, match, force, editor, extra
            )
###END###
def checkpush(self, pushop):
            if self.mq.applied and self.mq.checkapplied and not pushop.force:
                outapplied = [e.node for e in self.mq.applied]
                if pushop.revs:
                    # Assume applied patches have no non-patch descendants and
                    # are not on remote already. Filtering any changeset not
                    # pushed.
                    heads = set(pushop.revs)
                    for node in reversed(outapplied):
                        if node in heads:
                            break
                        else:
                            outapplied.pop()
                # looking for pushed and shared changeset
                for node in outapplied:
                    if self[node].phase() < phases.secret:
                        raise error.Abort(_(b'source has mq patches applied'))
                # no non-secret patches pushed
            super(mqrepo, self).checkpush(pushop)
###END###
def _findtags(self):
            '''augment tags from base class with patch tags'''
            result = super(mqrepo, self)._findtags()

            q = self.mq
            if not q.applied:
                return result

            mqtags = [(patch.node, patch.name) for patch in q.applied]

            try:
                # for now ignore filtering business
                self.unfiltered().changelog.rev(mqtags[-1][0])
            except error.LookupError:
                self.ui.warn(
                    _(b'mq status file refers to unknown node %s\n')
                    % short(mqtags[-1][0])
                )
                return result

            # do not add fake tags for filtered revisions
            included = self.changelog.hasnode
            mqtags = [mqt for mqt in mqtags if included(mqt[0])]
            if not mqtags:
                return result

            mqtags.append((mqtags[-1][0], b'qtip'))
            mqtags.append((mqtags[0][0], b'qbase'))
            mqtags.append((self.changelog.parents(mqtags[0][0])[0], b'qparent'))
            tags = result[0]
            for patch in mqtags:
                if patch[1] in tags:
                    self.ui.warn(
                        _(b'tag %s overrides mq patch of the same name\n')
                        % patch[1]
                    )
                else:
                    tags[patch[1]] = patch[0]

            return result
###END###
def debug(self, msg):
            pass
###END###
def log(self, event, msgfmt, *msgargs, **opts):
            pass
###END###
def fromstorage(cls, line):
        (
            time,
            user,
            command,
            namespace,
            name,
            oldhashes,
            newhashes,
        ) = line.split(b'\n')
        timestamp, tz = time.split()
        timestamp, tz = float(timestamp), int(tz)
        oldhashes = tuple(bin(hash) for hash in oldhashes.split(b','))
        newhashes = tuple(bin(hash) for hash in newhashes.split(b','))
        return cls(
            (timestamp, tz),
            user,
            command,
            namespace,
            name,
            oldhashes,
            newhashes,
        )
###END###
def __bytes__(self):
        """bytes representation for storage"""
        time = b' '.join(map(pycompat.bytestr, self.timestamp))
        oldhashes = b','.join([hex(hash) for hash in self.oldhashes])
        newhashes = b','.join([hex(hash) for hash in self.newhashes])
        return b'\n'.join(
            (
                time,
                self.user,
                self.command,
                self.namespace,
                self.name,
                oldhashes,
                newhashes,
            )
        )
###END###
def __init__(self, repo):
        self.user = procutil.getuser()
        self.ui = repo.ui
        self.vfs = repo.vfs

        # is this working copy using a shared storage?
        self.sharedfeatures = self.sharedvfs = None
        if repo.shared():
            features = _readsharedfeatures(repo)
            sharedrepo = hg.sharedreposource(repo)
            if sharedrepo is not None and b'journal' in features:
                self.sharedvfs = sharedrepo.vfs
                self.sharedfeatures = features
###END###
def command(self):
        commandstr = b' '.join(
            map(procutil.shellquote, journalstorage._currentcommand)
        )
        if b'\n' in commandstr:
            # truncate multi-line commands
            commandstr = commandstr.partition(b'\n')[0] + b' ...'
        return commandstr
###END###
def recordcommand(cls, *fullargs):
        """Set the current hg arguments, stored with recorded entries"""
        # Set the current command on the class because we may have started
        # with a non-local repo (cloning for example).
        cls._currentcommand = fullargs
###END###
def _currentlock(self, lockref):
        """Returns the lock if it's held, or None if it's not.

        (This is copied from the localrepo class)
        """
        if lockref is None:
            return None
        l = lockref()
        if l is None or not l.held:
            return None
        return l
###END###
def jlock(self, vfs):
        """Create a lock for the journal file"""
        if self._currentlock(self._lockref) is not None:
            raise error.Abort(_(b'journal lock does not support nesting'))
        desc = _(b'journal of %s') % vfs.base
        try:
            l = lock.lock(vfs, b'namejournal.lock', 0, desc=desc)
        except error.LockHeld as inst:
            self.ui.warn(
                _(b"waiting for lock on %s held by %r\n") % (desc, inst.locker)
            )
            # default to 600 seconds timeout
            l = lock.lock(
                vfs,
                b'namejournal.lock',
                self.ui.configint(b"ui", b"timeout"),
                desc=desc,
            )
            self.ui.warn(_(b"got lock after %s seconds\n") % l.delay)
        self._lockref = weakref.ref(l)
        return l
###END###
def record(self, namespace, name, oldhashes, newhashes):
        """Record a new journal entry

        * namespace: an opaque string; this can be used to filter on the type
          of recorded entries.
        * name: the name defining this entry; for bookmarks, this is the
          bookmark name. Can be filtered on when retrieving entries.
        * oldhashes and newhashes: each a single binary hash, or a list of
          binary hashes. These represent the old and new position of the named
          item.

        """
        if not isinstance(oldhashes, list):
            oldhashes = [oldhashes]
        if not isinstance(newhashes, list):
            newhashes = [newhashes]

        entry = journalentry(
            dateutil.makedate(),
            self.user,
            self.command,
            namespace,
            name,
            oldhashes,
            newhashes,
        )

        vfs = self.vfs
        if self.sharedvfs is not None:
            # write to the shared repository if this feature is being
            # shared between working copies.
            if sharednamespaces.get(namespace) in self.sharedfeatures:
                vfs = self.sharedvfs

        self._write(vfs, entry)
###END###
def _write(self, vfs, entry):
        with self.jlock(vfs):
            # open file in amend mode to ensure it is created if missing
            with vfs(b'namejournal', mode=b'a+b') as f:
                f.seek(0, os.SEEK_SET)
                # Read just enough bytes to get a version number (up to 2
                # digits plus separator)
                version = f.read(3).partition(b'\0')[0]
                if version and version != b"%d" % storageversion:
                    # different version of the storage. Exit early (and not
                    # write anything) if this is not a version we can handle or
                    # the file is corrupt. In future, perhaps rotate the file
                    # instead?
                    self.ui.warn(
                        _(b"unsupported journal file version '%s'\n") % version
                    )
                    return
                if not version:
                    # empty file, write version first
                    f.write((b"%d" % storageversion) + b'\0')
                f.seek(0, os.SEEK_END)
                f.write(bytes(entry) + b'\0')
###END###
def filtered(self, namespace=None, name=None):
        """Yield all journal entries with the given namespace or name

        Both the namespace and the name are optional; if neither is given all
        entries in the journal are produced.

        Matching supports regular expressions by using the `re:` prefix
        (use `literal:` to match names or namespaces that start with `re:`)

        """
        if namespace is not None:
            namespace = stringutil.stringmatcher(namespace)[-1]
        if name is not None:
            name = stringutil.stringmatcher(name)[-1]
        for entry in self:
            if namespace is not None and not namespace(entry.namespace):
                continue
            if name is not None and not name(entry.name):
                continue
            yield entry
###END###
def __iter__(self):
        """Iterate over the storage

        Yields journalentry instances for each contained journal record.

        """
        local = self._open(self.vfs)

        if self.sharedvfs is None:
            return local

        # iterate over both local and shared entries, but only those
        # shared entries that are among the currently shared features
        shared = (
            e
            for e in self._open(self.sharedvfs)
            if sharednamespaces.get(e.namespace) in self.sharedfeatures
        )
        return _mergeentriesiter(local, shared)
###END###
def _open(self, vfs, filename=b'namejournal', _newestfirst=True):
        if not vfs.exists(filename):
            return

        with vfs(filename) as f:
            raw = f.read()

        lines = raw.split(b'\0')
        version = lines and lines[0]
        if version != b"%d" % storageversion:
            version = version or _(b'not available')
            raise error.Abort(_(b"unknown journal file version '%s'") % version)

        # Skip the first line, it's a version number. Normally we iterate over
        # these in reverse order to list newest first; only when copying across
        # a shared storage do we forgo reversing.
        lines = lines[1:]
        if _newestfirst:
            lines = reversed(lines)
        for line in lines:
            if not line:
                continue
            yield journalentry.fromstorage(line)
###END###
def _bookmarks(self):
            '''Parse .hg/bookmarks file and return a dictionary

            Bookmarks are stored as {HASH}\\s{NAME}\\n (localtags format) values
            in the .hg/bookmarks file. They are read returned as a dictionary
            with name => hash values.
            '''
            try:
                bookmarks = {}
                for line in self.opener('bookmarks'):
                    sha, refspec = line.strip().split(' ', 1)
                    bookmarks[refspec] = super(bookmark_repo, self).lookup(sha)
            except:
                pass
            return bookmarks
###END###
def _bookmarkcurrent(self):
            '''Get the current bookmark

            If we use gittishsh branches we have a current bookmark that
            we are on. This function returns the name of the bookmark. It
            is stored in .hg/bookmarks.current
            '''
            mark = None
            if os.path.exists(self.join('bookmarks.current')):
                file = self.opener('bookmarks.current')
                # No readline() in posixfile_nt, reading everything is cheap
                mark = (file.readlines() or [''])[0]
                if mark == '':
                    mark = None
                file.close()
            return mark
###END###
def rollback(self):
            if os.path.exists(self.join('undo.bookmarks')):
                util.rename(self.join('undo.bookmarks'), self.join('bookmarks'))
            return super(bookmark_repo, self).rollback()
###END###
def lookup(self, key):
            if key in self._bookmarks:
                key = self._bookmarks[key]
            return super(bookmark_repo, self).lookup(key)
###END###
def _bookmarksupdate(self, parents, node):
            marks = self._bookmarks
            update = False
            if ui.configbool('bookmarks', 'track.current'):
                mark = self._bookmarkcurrent
                if mark and marks[mark] in parents:
                    marks[mark] = node
                    update = True
            else:
                for mark, n in marks.items():
                    if n in parents:
                        marks[mark] = node
                        update = True
            if update:
                write(self)
###END###
def commitctx(self, ctx, error=False):
            """Add a revision to the repository and
            move the bookmark"""
            wlock = self.wlock() # do both commit and bookmark with lock held
            try:
                node  = super(bookmark_repo, self).commitctx(ctx, error)
                if node is None:
                    return None
                parents = self.changelog.parents(node)
                if parents[1] == nullid:
                    parents = (parents[0],)

                self._bookmarksupdate(parents, node)
                return node
            finally:
                wlock.release()
###END###
def addchangegroup(self, source, srctype, url, emptyok=False):
            parents = self.dirstate.parents()

            result = super(bookmark_repo, self).addchangegroup(
                source, srctype, url, emptyok)
            if result > 1:
                # We have more heads than before
                return result
            node = self.changelog.tip()

            self._bookmarksupdate(parents, node)
            return result
###END###
def _findtags(self):
            """Merge bookmarks with normal tags"""
            (tags, tagtypes) = super(bookmark_repo, self)._findtags()
            tags.update(self._bookmarks)
            return (tags, tagtypes)
###END###
def __init__(self, ui, repo, inc, exc):
        self.ui = ui
        self._repo = weakref.ref(repo)
        self.match = match.match(repo.root, b'', [], inc, exc)
        self.restrict = kwtools[b'hgcmd'] in restricted.split()
        self.postcommit = False

        kwmaps = self.ui.configitems(b'keywordmaps')
        if kwmaps:  # override default templates
            self.templates = dict(kwmaps)
        else:
            self.templates = _defaultkwmaps(self.ui)
###END###
def repo(self):
        return self._repo()
###END###
def escape(self):
        '''Returns bar-separated and escaped keywords.'''
        return b'|'.join(map(stringutil.reescape, self.templates.keys()))
###END###
def rekw(self):
        '''Returns regex for unexpanded keywords.'''
        return re.compile(br'\$(%s)\$' % self.escape)
###END###
def rekwexp(self):
        '''Returns regex for expanded keywords.'''
        return re.compile(br'\$(%s): [^$\n\r]*? \$' % self.escape)
###END###
def substitute(self, data, path, ctx, subfunc):
        '''Replaces keywords in data with expanded template.'''

        def kwsub(mobj):
            kw = mobj.group(1)
            ct = logcmdutil.maketemplater(
                self.ui, self.repo, self.templates[kw]
            )
            self.ui.pushbuffer()
            ct.show(ctx, root=self.repo.root, file=path)
            ekw = templatefilters.firstline(self.ui.popbuffer())
            return b'$%s: %s $' % (kw, ekw)

        return subfunc(kwsub, data)
###END###
def linkctx(self, path, fileid):
        '''Similar to filelog.linkrev, but returns a changectx.'''
        return self.repo.filectx(path, fileid=fileid).changectx()
###END###
def expand(self, path, node, data):
        '''Returns data with keywords expanded.'''
        if (
            not self.restrict
            and self.match(path)
            and not stringutil.binary(data)
        ):
            ctx = self.linkctx(path, node)
            return self.substitute(data, path, ctx, self.rekw.sub)
        return data
###END###
def iskwfile(self, cand, ctx):
        """Returns subset of candidates which are configured for keyword
        expansion but are not symbolic links."""
        return [f for f in cand if self.match(f) and b'l' not in ctx.flags(f)]
###END###
def overwrite(self, ctx, candidates, lookup, expand, rekw=False):
        '''Overwrites selected files expanding/shrinking keywords.'''
        if self.restrict or lookup or self.postcommit:  # exclude kw_copy
            candidates = self.iskwfile(candidates, ctx)
        if not candidates:
            return
        kwcmd = self.restrict and lookup  # kwexpand/kwshrink
        if self.restrict or expand and lookup:
            mf = ctx.manifest()
        if self.restrict or rekw:
            re_kw = self.rekw
        else:
            re_kw = self.rekwexp
        if expand:
            msg = _(b'overwriting %s expanding keywords\n')
        else:
            msg = _(b'overwriting %s shrinking keywords\n')
        for f in candidates:
            if self.restrict:
                data = self.repo.file(f).read(mf[f])
            else:
                data = self.repo.wread(f)
            if stringutil.binary(data):
                continue
            if expand:
                parents = ctx.parents()
                if lookup:
                    ctx = self.linkctx(f, mf[f])
                elif self.restrict and len(parents) > 1:
                    # merge commit
                    # in case of conflict f is in modified state during
                    # merge, even if f does not differ from f in parent
                    for p in parents:
                        if f in p and not p[f].cmp(ctx[f]):
                            ctx = p[f].changectx()
                            break
                data, found = self.substitute(data, f, ctx, re_kw.subn)
            elif self.restrict:
                found = re_kw.search(data)
            else:
                data, found = _shrinktext(data, re_kw.subn)
            if found:
                self.ui.note(msg % f)
                fp = self.repo.wvfs(f, b"wb", atomictemp=True)
                fp.write(data)
                fp.close()
                if kwcmd:
                    self.repo.dirstate.set_clean(f)
                elif self.postcommit:
                    self.repo.dirstate.update_file_p1(f, p1_tracked=True)
###END###
def shrink(self, fname, text):
        '''Returns text with all keyword substitutions removed.'''
        if self.match(fname) and not stringutil.binary(text):
            return _shrinktext(text, self.rekwexp.sub)
        return text
###END###
def shrinklines(self, fname, lines):
        '''Returns lines with keyword substitutions removed.'''
        if self.match(fname):
            text = b''.join(lines)
            if not stringutil.binary(text):
                return _shrinktext(text, self.rekwexp.sub).splitlines(True)
        return lines
###END###
def wread(self, fname, data):
        """If in restricted mode returns data read from wdir with
        keyword substitutions removed."""
        if self.restrict:
            return self.shrink(fname, data)
        return data
###END###
def __init__(self, opener, kwt, path):
        super(kwfilelog, self).__init__(opener, path)
        self.kwt = kwt
        self.path = path
###END###
def read(self, node):
        '''Expands keywords when reading filelog.'''
        data = super(kwfilelog, self).read(node)
        if self.renamed(node):
            return data
        return self.kwt.expand(self.path, node, data)
###END###
def add(self, text, meta, tr, link, p1=None, p2=None):
        '''Removes keyword substitutions when adding to filelog.'''
        text = self.kwt.shrink(self.path, text)
        return super(kwfilelog, self).add(text, meta, tr, link, p1, p2)
###END###
def cmp(self, node, text):
        '''Removes keyword substitutions for comparison.'''
        text = self.kwt.shrink(self.path, text)
        return super(kwfilelog, self).cmp(node, text)
###END###
def file(self, f):
            if f[0] == b'/':
                f = f[1:]
            return kwfilelog(self.svfs, kwt, f)
###END###
def wread(self, filename):
            data = super(kwrepo, self).wread(filename)
            return kwt.wread(filename, data)
###END###
def commit(self, *args, **opts):
            # use custom commitctx for user commands
            # other extensions can still wrap repo.commitctx directly
            self.commitctx = self.kwcommitctx
            try:
                return super(kwrepo, self).commit(*args, **opts)
            finally:
                del self.commitctx
###END###
def kwcommitctx(self, ctx, error=False, origctx=None):
            n = super(kwrepo, self).commitctx(ctx, error, origctx)
            # no lock needed, only called from repo.commit() which already locks
            if not kwt.postcommit:
                restrict = kwt.restrict
                kwt.restrict = True
                kwt.overwrite(
                    self[n], sorted(ctx.added() + ctx.modified()), False, True
                )
                kwt.restrict = restrict
            return n
###END###
def rollback(self, dryrun=False, force=False):
            with self.wlock():
                origrestrict = kwt.restrict
                try:
                    if not dryrun:
                        changed = self[b'.'].files()
                    ret = super(kwrepo, self).rollback(dryrun, force)
                    if not dryrun:
                        ctx = self[b'.']
                        modified, added = _preselect(ctx.status(), changed)
                        kwt.restrict = False
                        kwt.overwrite(ctx, modified, True, True)
                        kwt.overwrite(ctx, added, True, False)
                    return ret
                finally:
                    kwt.restrict = origrestrict
###END###
def __init__(self, path, key=None):
        self.path = path
        self.key = (key and b" --local-user \"%s\"" % key) or b""
###END###
def sign(self, data):
        gpgcmd = b"%s --sign --detach-sign%s" % (self.path, self.key)
        return procutil.filter(data, gpgcmd)
###END###
def verify(self, data, sig):
        """returns of the good and bad signatures"""
        sigfile = datafile = None
        try:
            # create temporary files
            fd, sigfile = pycompat.mkstemp(prefix=b"hg-gpg-", suffix=b".sig")
            fp = os.fdopen(fd, 'wb')
            fp.write(sig)
            fp.close()
            fd, datafile = pycompat.mkstemp(prefix=b"hg-gpg-", suffix=b".txt")
            fp = os.fdopen(fd, 'wb')
            fp.write(data)
            fp.close()
            gpgcmd = (
                b"%s --logger-fd 1 --status-fd 1 --verify \"%s\" \"%s\""
                % (
                    self.path,
                    sigfile,
                    datafile,
                )
            )
            ret = procutil.filter(b"", gpgcmd)
        finally:
            for f in (sigfile, datafile):
                try:
                    if f:
                        os.unlink(f)
                except OSError:
                    pass
        keys = []
        key, fingerprint = None, None
        for l in ret.splitlines():
            # see DETAILS in the gnupg documentation
            # filter the logger output
            if not l.startswith(b"[GNUPG:]"):
                continue
            l = l[9:]
            if l.startswith(b"VALIDSIG"):
                # fingerprint of the primary key
                fingerprint = l.split()[10]
            elif l.startswith(b"ERRSIG"):
                key = l.split(b" ", 3)[:2]
                key.append(b"")
                fingerprint = None
            elif (
                l.startswith(b"GOODSIG")
                or l.startswith(b"EXPSIG")
                or l.startswith(b"EXPKEYSIG")
                or l.startswith(b"BADSIG")
            ):
                if key is not None:
                    keys.append(key + [fingerprint])
                key = l.split(b" ", 2)
                fingerprint = None
        if key is not None:
            keys.append(key + [fingerprint])
        return keys
###END###
def __init__(self, ui, repo):
        self._repo = repo
        self._trackedevents = set(ui.configlist(b'blackbox', b'track'))
        self._ignoredevents = set(ui.configlist(b'blackbox', b'ignore'))
        self._maxfiles = ui.configint(b'blackbox', b'maxfiles')
        self._maxsize = ui.configbytes(b'blackbox', b'maxsize')
        self._inlog = False
###END###
def tracked(self, event):
        return (
            b'*' in self._trackedevents and event not in self._ignoredevents
        ) or event in self._trackedevents
###END###
def log(self, ui, event, msg, opts):
        # self._log() -> ctx.dirty() may create new subrepo instance, which
        # ui is derived from baseui. So the recursion guard in ui.log()
        # doesn't work as it's local to the ui instance.
        if self._inlog:
            return
        self._inlog = True
        try:
            self._log(ui, event, msg, opts)
        finally:
            self._inlog = False
###END###
def _log(self, ui, event, msg, opts):
        default = ui.configdate(b'devel', b'default-date')
        date = dateutil.datestr(default, ui.config(b'blackbox', b'date-format'))
        user = procutil.getuser()
        pid = b'%d' % procutil.getpid()
        changed = b''
        ctx = self._repo[None]
        parents = ctx.parents()
        rev = b'+'.join([hex(p.node()) for p in parents])
        if ui.configbool(b'blackbox', b'dirty') and ctx.dirty(
            missing=True, merge=False, branch=False
        ):
            changed = b'+'
        if ui.configbool(b'blackbox', b'logsource'):
            src = b' [%s]' % event
        else:
            src = b''
        try:
            fmt = b'%s %s @%s%s (%s)%s> %s'
            args = (date, user, rev, changed, pid, src, msg)
            with loggingutil.openlogfile(
                ui,
                self._repo.vfs,
                name=b'blackbox.log',
                maxfiles=self._maxfiles,
                maxsize=self._maxsize,
            ) as fp:
                fp.write(fmt % args)
        except (IOError, OSError) as err:
            # deactivate this to avoid failed logging again
            self._trackedevents.clear()
            ui.debug(
                b'warning: cannot write to blackbox.log: %s\n'
                % encoding.strtolocal(err.strerror)
            )
            return
        _lastlogger.logger = self
###END###
def __init__(self, name):
        self.name = name
        self.args = []
        self.opts = {}
###END###
def __bytes__(self):
        cmd = b"hg " + self.name
        if self.opts:
            for k, values in sorted(pycompat.iteritems(self.opts)):
                for v in values:
                    if v:
                        if isinstance(v, int):
                            fmt = b' %s %d'
                        else:
                            fmt = b' %s %s'

                        cmd += fmt % (k, v)
                    else:
                        cmd += b" %s" % (k,)
        if self.args:
            cmd += b" "
            cmd += b" ".join(self.args)
        return cmd
###END###
def append(self, value):
        self.args.append(value)
###END###
def extend(self, values):
        self.args.extend(values)
###END###
def __setitem__(self, key, value):
        values = self.opts.setdefault(key, [])
        values.append(value)
###END###
def __and__(self, other):
        return AndCommand(self, other)
###END###
def __init__(self, left, right):
        self.left = left
        self.right = right
###END###
def __str__(self):
        return b"%s && %s" % (self.left, self.right)
###END###
def __and__(self, other):
        return AndCommand(self, other)
###END###
def __init__(self, repo):
        self.repo = repo
        self.actions = None
        self.keep = None
        self.topmost = None
        self.parentctxnode = None
        self.lock = None
        self.wlock = None
        self.backupfile = None
        self.stateobj = statemod.cmdstate(repo, b'histedit-state')
        self.replacements = []
###END###
def read(self):
        """Load histedit state from disk and set fields appropriately."""
        if not self.stateobj.exists():
            cmdutil.wrongtooltocontinue(self.repo, _(b'histedit'))

        data = self._read()

        self.parentctxnode = data[b'parentctxnode']
        actions = parserules(data[b'rules'], self)
        self.actions = actions
        self.keep = data[b'keep']
        self.topmost = data[b'topmost']
        self.replacements = data[b'replacements']
        self.backupfile = data[b'backupfile']
###END###
def _read(self):
        fp = self.repo.vfs.read(b'histedit-state')
        if fp.startswith(b'v1\n'):
            data = self._load()
            parentctxnode, rules, keep, topmost, replacements, backupfile = data
        else:
            data = pickle.loads(fp)
            parentctxnode, rules, keep, topmost, replacements = data
            backupfile = None
        rules = b"\n".join([b"%s %s" % (verb, rest) for [verb, rest] in rules])

        return {
            b'parentctxnode': parentctxnode,
            b"rules": rules,
            b"keep": keep,
            b"topmost": topmost,
            b"replacements": replacements,
            b"backupfile": backupfile,
        }
###END###
def write(self, tr=None):
        if tr:
            tr.addfilegenerator(
                b'histedit-state',
                (b'histedit-state',),
                self._write,
                location=b'plain',
            )
        else:
            with self.repo.vfs(b"histedit-state", b"w") as f:
                self._write(f)
###END###
def _write(self, fp):
        fp.write(b'v1\n')
        fp.write(b'%s\n' % hex(self.parentctxnode))
        fp.write(b'%s\n' % hex(self.topmost))
        fp.write(b'%s\n' % (b'True' if self.keep else b'False'))
        fp.write(b'%d\n' % len(self.actions))
        for action in self.actions:
            fp.write(b'%s\n' % action.tostate())
        fp.write(b'%d\n' % len(self.replacements))
        for replacement in self.replacements:
            fp.write(
                b'%s%s\n'
                % (
                    hex(replacement[0]),
                    b''.join(hex(r) for r in replacement[1]),
                )
            )
        backupfile = self.backupfile
        if not backupfile:
            backupfile = b''
        fp.write(b'%s\n' % backupfile)
###END###
def _load(self):
        fp = self.repo.vfs(b'histedit-state', b'r')
        lines = [l[:-1] for l in fp.readlines()]

        index = 0
        lines[index]  # version number
        index += 1

        parentctxnode = bin(lines[index])
        index += 1

        topmost = bin(lines[index])
        index += 1

        keep = lines[index] == b'True'
        index += 1

        # Rules
        rules = []
        rulelen = int(lines[index])
        index += 1
        for i in pycompat.xrange(rulelen):
            ruleaction = lines[index]
            index += 1
            rule = lines[index]
            index += 1
            rules.append((ruleaction, rule))

        # Replacements
        replacements = []
        replacementlen = int(lines[index])
        index += 1
        for i in pycompat.xrange(replacementlen):
            replacement = lines[index]
            original = bin(replacement[:40])
            succ = [
                bin(replacement[i : i + 40])
                for i in range(40, len(replacement), 40)
            ]
            replacements.append((original, succ))
            index += 1

        backupfile = lines[index]
        index += 1

        fp.close()

        return parentctxnode, rules, keep, topmost, replacements, backupfile
###END###
def clear(self):
        if self.inprogress():
            self.repo.vfs.unlink(b'histedit-state')
###END###
def inprogress(self):
        return self.repo.vfs.exists(b'histedit-state')
###END###
def __init__(self, state, node):
        self.state = state
        self.repo = state.repo
        self.node = node
###END###
def fromrule(cls, state, rule):
        """Parses the given rule, returning an instance of the histeditaction."""
        ruleid = rule.strip().split(b' ', 1)[0]
        # ruleid can be anything from rev numbers, hashes, "bookmarks" etc
        # Check for validation of rule ids and get the rulehash
        try:
            rev = bin(ruleid)
        except TypeError:
            try:
                _ctx = scmutil.revsingle(state.repo, ruleid)
                rulehash = _ctx.hex()
                rev = bin(rulehash)
            except error.RepoLookupError:
                raise error.ParseError(_(b"invalid changeset %s") % ruleid)
        return cls(state, rev)
###END###
def verify(self, prev, expected, seen):
        """Verifies semantic correctness of the rule"""
        repo = self.repo
        ha = hex(self.node)
        self.node = scmutil.resolvehexnodeidprefix(repo, ha)
        if self.node is None:
            raise error.ParseError(_(b'unknown changeset %s listed') % ha[:12])
        self._verifynodeconstraints(prev, expected, seen)
###END###
def _verifynodeconstraints(self, prev, expected, seen):
        # by default command need a node in the edited list
        if self.node not in expected:
            raise error.ParseError(
                _(b'%s "%s" changeset was not a candidate')
                % (self.verb, short(self.node)),
                hint=_(b'only use listed changesets'),
            )
        # and only one command per node
        if self.node in seen:
            raise error.ParseError(
                _(b'duplicated command for changeset %s') % short(self.node)
            )
###END###
def torule(self):
        """build a histedit rule line for an action

        by default lines are in the form:
        <hash> <rev> <summary>
        """
        ctx = self.repo[self.node]
        ui = self.repo.ui
        # We don't want color codes in the commit message template, so
        # disable the label() template function while we render it.
        with ui.configoverride(
            {(b'templatealias', b'label(l,x)'): b"x"}, b'histedit'
        ):
            summary = cmdutil.rendertemplate(
                ctx, ui.config(b'histedit', b'summary-template')
            )
        # Handle the fact that `''.splitlines() => []`
        summary = summary.splitlines()[0] if summary else b''
        line = b'%s %s %s' % (self.verb, ctx, summary)
        # trim to 75 columns by default so it's not stupidly wide in my editor
        # (the 5 more are left for verb)
        maxlen = self.repo.ui.configint(b'histedit', b'linelen')
        maxlen = max(maxlen, 22)  # avoid truncating hash
        return stringutil.ellipsis(line, maxlen)
###END###
def tostate(self):
        """Print an action in format used by histedit state files
        (the first line is a verb, the remainder is the second)
        """
        return b"%s\n%s" % (self.verb, hex(self.node))
###END###
def run(self):
        """Runs the action. The default behavior is simply apply the action's
        rulectx onto the current parentctx."""
        self.applychange()
        self.continuedirty()
        return self.continueclean()
###END###
def applychange(self):
        """Applies the changes from this action's rulectx onto the current
        parentctx, but does not commit them."""
        repo = self.repo
        rulectx = repo[self.node]
        with repo.ui.silent():
            hg.update(repo, self.state.parentctxnode, quietempty=True)
        stats = applychanges(repo.ui, repo, rulectx, {})
        repo.dirstate.setbranch(rulectx.branch())
        if stats.unresolvedcount:
            raise error.InterventionRequired(
                _(b'Fix up the change (%s %s)') % (self.verb, short(self.node)),
                hint=_(b'hg histedit --continue to resume'),
            )
###END###
def continuedirty(self):
        """Continues the action when changes have been applied to the working
        copy. The default behavior is to commit the dirty changes."""
        repo = self.repo
        rulectx = repo[self.node]

        editor = self.commiteditor()
        commit = commitfuncfor(repo, rulectx)
        if repo.ui.configbool(b'rewrite', b'update-timestamp'):
            date = dateutil.makedate()
        else:
            date = rulectx.date()
        commit(
            text=rulectx.description(),
            user=rulectx.user(),
            date=date,
            extra=rulectx.extra(),
            editor=editor,
        )
###END###
def commiteditor(self):
        """The editor to be used to edit the commit message."""
        return False
###END###
def continueclean(self):
        """Continues the action when the working copy is clean. The default
        behavior is to accept the current commit as the new version of the
        rulectx."""
        ctx = self.repo[b'.']
        if ctx.node() == self.state.parentctxnode:
            self.repo.ui.warn(
                _(b'%s: skipping changeset (no changes)\n') % short(self.node)
            )
            return ctx, [(self.node, tuple())]
        if ctx.node() == self.node:
            # Nothing changed
            return ctx, []
        return ctx, [(self.node, (ctx.node(),))]
###END###
def run(self):
        rulectx = self.repo[self.node]
        if rulectx.p1().node() == self.state.parentctxnode:
            self.repo.ui.debug(b'node %s unchanged\n' % short(self.node))
            return rulectx, []

        return super(pick, self).run()
###END###
def run(self):
        repo = self.repo
        rulectx = repo[self.node]
        hg.update(repo, self.state.parentctxnode, quietempty=True)
        applychanges(repo.ui, repo, rulectx, {})
        hint = _(b'to edit %s, `hg histedit --continue` after making changes')
        raise error.InterventionRequired(
            _(b'Editing (%s), commit as needed now to split the change')
            % short(self.node),
            hint=hint % short(self.node),
        )
###END###
def commiteditor(self):
        return cmdutil.getcommiteditor(edit=True, editform=b'histedit.edit')
###END###
def verify(self, prev, expected, seen):
        """Verifies semantic correctness of the fold rule"""
        super(fold, self).verify(prev, expected, seen)
        repo = self.repo
        if not prev:
            c = repo[self.node].p1()
        elif not prev.verb in (b'pick', b'base'):
            return
        else:
            c = repo[prev.node]
        if not c.mutable():
            raise error.ParseError(
                _(b"cannot fold into public change %s") % short(c.node())
            )
###END###
def continuedirty(self):
        repo = self.repo
        rulectx = repo[self.node]

        commit = commitfuncfor(repo, rulectx)
        commit(
            text=b'fold-temp-revision %s' % short(self.node),
            user=rulectx.user(),
            date=rulectx.date(),
            extra=rulectx.extra(),
        )
###END###
def continueclean(self):
        repo = self.repo
        ctx = repo[b'.']
        rulectx = repo[self.node]
        parentctxnode = self.state.parentctxnode
        if ctx.node() == parentctxnode:
            repo.ui.warn(_(b'%s: empty changeset\n') % short(self.node))
            return ctx, [(self.node, (parentctxnode,))]

        parentctx = repo[parentctxnode]
        newcommits = {
            c.node()
            for c in repo.set(b'(%d::. - %d)', parentctx.rev(), parentctx.rev())
        }
        if not newcommits:
            repo.ui.warn(
                _(
                    b'%s: cannot fold - working copy is not a '
                    b'descendant of previous commit %s\n'
                )
                % (short(self.node), short(parentctxnode))
            )
            return ctx, [(self.node, (ctx.node(),))]

        middlecommits = newcommits.copy()
        middlecommits.discard(ctx.node())

        return self.finishfold(
            repo.ui, repo, parentctx, rulectx, ctx.node(), middlecommits
        )
###END###
def skipprompt(self):
        """Returns true if the rule should skip the message editor.

        For example, 'fold' wants to show an editor, but 'rollup'
        doesn't want to.
        """
        return False
###END###
def mergedescs(self):
        """Returns true if the rule should merge messages of multiple changes.

        This exists mainly so that 'rollup' rules can be a subclass of
        'fold'.
        """
        return True
###END###
def firstdate(self):
        """Returns true if the rule should preserve the date of the first
        change.

        This exists mainly so that 'rollup' rules can be a subclass of
        'fold'.
        """
        return False
###END###
def finishfold(self, ui, repo, ctx, oldctx, newnode, internalchanges):
        mergemod.update(ctx.p1())
        ### prepare new commit data
        commitopts = {}
        commitopts[b'user'] = ctx.user()
        # commit message
        if not self.mergedescs():
            newmessage = ctx.description()
        else:
            newmessage = (
                b'\n***\n'.join(
                    [ctx.description()]
                    + [repo[r].description() for r in internalchanges]
                    + [oldctx.description()]
                )
                + b'\n'
            )
        commitopts[b'message'] = newmessage
        # date
        if self.firstdate():
            commitopts[b'date'] = ctx.date()
        else:
            commitopts[b'date'] = max(ctx.date(), oldctx.date())
        # if date is to be updated to current
        if ui.configbool(b'rewrite', b'update-timestamp'):
            commitopts[b'date'] = dateutil.makedate()

        extra = ctx.extra().copy()
        # histedit_source
        # note: ctx is likely a temporary commit but that the best we can do
        #       here. This is sufficient to solve issue3681 anyway.
        extra[b'histedit_source'] = b'%s,%s' % (ctx.hex(), oldctx.hex())
        commitopts[b'extra'] = extra
        phasemin = max(ctx.phase(), oldctx.phase())
        overrides = {(b'phases', b'new-commit'): phasemin}
        with repo.ui.configoverride(overrides, b'histedit'):
            n = collapse(
                repo,
                ctx,
                repo[newnode],
                commitopts,
                skipprompt=self.skipprompt(),
            )
        if n is None:
            return ctx, []
        mergemod.update(repo[n])
        replacements = [
            (oldctx.node(), (newnode,)),
            (ctx.node(), (n,)),
            (newnode, (n,)),
        ]
        for ich in internalchanges:
            replacements.append((ich, (n,)))
        return repo[n], replacements
###END###
def run(self):
        if self.repo[b'.'].node() != self.node:
            mergemod.clean_update(self.repo[self.node])
        return self.continueclean()
###END###
def continuedirty(self):
        abortdirty()
###END###
def continueclean(self):
        basectx = self.repo[b'.']
        return basectx, []
###END###
def _verifynodeconstraints(self, prev, expected, seen):
        # base can only be use with a node not in the edited set
        if self.node in expected:
            msg = _(b'%s "%s" changeset was an edited list candidate')
            raise error.ParseError(
                msg % (self.verb, short(self.node)),
                hint=_(b'base must only use unlisted changesets'),
            )
###END###
def skipprompt(self):
        return True
###END###
def mergedescs(self):
        return False
###END###
def skipprompt(self):
        return True
###END###
def firstdate(self):
        return True
###END###
def run(self):
        parentctx = self.repo[self.state.parentctxnode]
        return parentctx, [(self.node, tuple())]
###END###
def commiteditor(self):
        return cmdutil.getcommiteditor(edit=True, editform=b'histedit.mess')
###END###
def __init__(self, ui, ctx, pos, action=b'pick'):
        self.ui = ui
        self.ctx = ctx
        self.action = action
        self.origpos = pos
        self.pos = pos
        self.conflicts = []
###END###
def __bytes__(self):
        # Example display of several histeditrules:
        #
        #  #10 pick   316392:06a16c25c053   add option to skip tests
        #  #11 ^roll  316393:71313c964cc5   <RED>oops a fixup commit</RED>
        #  #12 pick   316394:ab31f3973b0d   include mfbt for mozilla-config.h
        #  #13 ^fold  316395:14ce5803f4c3   fix warnings
        #
        # The carets point to the changeset being folded into ("roll this
        # changeset into the changeset above").
        return b'%s%s' % (self.prefix, self.desc)
###END###
def prefix(self):
        # Some actions ('fold' and 'roll') combine a patch with a
        # previous one. Add a marker showing which patch they apply
        # to.
        action = ACTION_LABELS.get(self.action, self.action)

        h = self.ctx.hex()[0:12]
        r = self.ctx.rev()

        return b"#%s %s %d:%s   " % (
            (b'%d' % self.origpos).ljust(2),
            action.ljust(6),
            r,
            h,
        )
###END###
def desc(self):
        summary = cmdutil.rendertemplate(
            self.ctx, self.ui.config(b'histedit', b'summary-template')
        )
        if summary:
            return summary
        # This is split off from the prefix property so that we can
        # separately make the description for 'roll' red (since it
        # will get discarded).
        return self.ctx.description().splitlines()[0].strip()
###END###
def checkconflicts(self, other):
        if other.pos > self.pos and other.origpos <= self.origpos:
            if set(other.ctx.files()) & set(self.ctx.files()) != set():
                self.conflicts.append(other)
                return self.conflicts

        if other in self.conflicts:
            self.conflicts.remove(other)
        return self.conflicts
###END###
def _extrasetup(self, name, func, fmtopic=None, csettopic=None):
        """Called with decorator arguments to register a show view.

        ``name`` is the sub-command name.

        ``func`` is the function being decorated.

        ``fmtopic`` is the topic in the style that will be rendered for
        this view.

        ``csettopic`` is the topic in the style to be used for a changeset
        printer.

        If ``fmtopic`` is specified, the view function will receive a
        formatter instance. If ``csettopic`` is specified, the view
        function will receive a changeset printer.
        """
        func._fmtopic = fmtopic
        func._csettopic = csettopic
###END###
def __init__(self, ui, repo, hooktype):
        self.ui = ui
        cfg = self.ui.config(b'notify', b'config')
        if cfg:
            self.ui.readconfig(cfg, sections=[b'usersubs', b'reposubs'])
        self.repo = repo
        self.stripcount = int(self.ui.config(b'notify', b'strip'))
        self.root = self.strip(self.repo.root)
        self.domain = self.ui.config(b'notify', b'domain')
        self.mbox = self.ui.config(b'notify', b'mbox')
        self.test = self.ui.configbool(b'notify', b'test')
        self.charsets = mail._charsets(self.ui)
        self.subs = self.subscribers()
        self.merge = self.ui.configbool(b'notify', b'merge')
        self.showfunc = self.ui.configbool(b'notify', b'showfunc')
        self.messageidseed = self.ui.config(b'notify', b'messageidseed')
        self.reply = self.ui.configbool(b'notify', b'reply-to-predecessor')

        if self.reply and not self.messageidseed:
            raise error.Abort(
                _(
                    b'notify.reply-to-predecessor used without '
                    b'notify.messageidseed'
                )
            )

        if self.showfunc is None:
            self.showfunc = self.ui.configbool(b'diff', b'showfunc')

        mapfile = None
        template = self.ui.config(b'notify', hooktype) or self.ui.config(
            b'notify', b'template'
        )
        if not template:
            mapfile = self.ui.config(b'notify', b'style')
        if not mapfile and not template:
            template = deftemplates.get(hooktype) or single_template
        spec = logcmdutil.templatespec(template, mapfile)
        self.t = logcmdutil.changesettemplater(self.ui, self.repo, spec)
###END###
def strip(self, path):
        '''strip leading slashes from local path, turn into web-safe path.'''

        path = util.pconvert(path)
        count = self.stripcount
        while count > 0:
            c = path.find(b'/')
            if c == -1:
                break
            path = path[c + 1 :]
            count -= 1
        return path
###END###
def fixmail(self, addr):
        '''try to clean up email addresses.'''

        addr = stringutil.email(addr.strip())
        if self.domain:
            a = addr.find(b'@localhost')
            if a != -1:
                addr = addr[:a]
            if b'@' not in addr:
                return addr + b'@' + self.domain
        return addr
###END###
def subscribers(self):
        '''return list of email addresses of subscribers to this repo.'''
        subs = set()
        for user, pats in self.ui.configitems(b'usersubs'):
            for pat in pats.split(b','):
                if b'#' in pat:
                    pat, revs = pat.split(b'#', 1)
                else:
                    revs = None
                if fnmatch.fnmatch(self.repo.root, pat.strip()):
                    subs.add((self.fixmail(user), revs))
        for pat, users in self.ui.configitems(b'reposubs'):
            if b'#' in pat:
                pat, revs = pat.split(b'#', 1)
            else:
                revs = None
            if fnmatch.fnmatch(self.repo.root, pat):
                for user in users.split(b','):
                    subs.add((self.fixmail(user), revs))
        return [
            (mail.addressencode(self.ui, s, self.charsets, self.test), r)
            for s, r in sorted(subs)
        ]
###END###
def node(self, ctx, **props):
        '''format one changeset, unless it is a suppressed merge.'''
        if not self.merge and len(ctx.parents()) > 1:
            return False
        self.t.show(
            ctx,
            changes=ctx.changeset(),
            baseurl=self.ui.config(b'web', b'baseurl'),
            root=self.repo.root,
            webroot=self.root,
            **props
        )
        return True
###END###
def skipsource(self, source):
        '''true if incoming changes from this source should be skipped.'''
        ok_sources = self.ui.config(b'notify', b'sources').split()
        return source not in ok_sources
###END###
def send(self, ctx, count, data):
        '''send message.'''

        # Select subscribers by revset
        subs = set()
        for sub, spec in self.subs:
            if spec is None:
                subs.add(sub)
                continue
            revs = self.repo.revs(b'%r and %d:', spec, ctx.rev())
            if len(revs):
                subs.add(sub)
                continue
        if len(subs) == 0:
            self.ui.debug(
                b'notify: no subscribers to selected repo and revset\n'
            )
            return

        try:
            msg = mail.parsebytes(data)
        except emailerrors.MessageParseError as inst:
            raise error.Abort(inst)

        # store sender and subject
        sender = msg['From']
        subject = msg['Subject']
        if sender is not None:
            sender = mail.headdecode(sender)
        if subject is not None:
            subject = mail.headdecode(subject)
        del msg['From'], msg['Subject']

        if not msg.is_multipart():
            # create fresh mime message from scratch
            # (multipart templates must take care of this themselves)
            headers = msg.items()
            payload = msg.get_payload(decode=pycompat.ispy3)
            # for notification prefer readability over data precision
            msg = mail.mimeencode(self.ui, payload, self.charsets, self.test)
            # reinstate custom headers
            for k, v in headers:
                msg[k] = v

        msg['Date'] = encoding.strfromlocal(
            dateutil.datestr(format=b"%a, %d %b %Y %H:%M:%S %1%2")
        )

        # try to make subject line exist and be useful
        if not subject:
            if count > 1:
                subject = _(b'%s: %d new changesets') % (self.root, count)
            else:
                s = ctx.description().lstrip().split(b'\n', 1)[0].rstrip()
                subject = b'%s: %s' % (self.root, s)
        maxsubject = int(self.ui.config(b'notify', b'maxsubject'))
        if maxsubject:
            subject = stringutil.ellipsis(subject, maxsubject)
        msg['Subject'] = mail.headencode(
            self.ui, subject, self.charsets, self.test
        )

        # try to make message have proper sender
        if not sender:
            sender = self.ui.config(b'email', b'from') or self.ui.username()
        if b'@' not in sender or b'@localhost' in sender:
            sender = self.fixmail(sender)
        msg['From'] = mail.addressencode(
            self.ui, sender, self.charsets, self.test
        )

        msg['X-Hg-Notification'] = 'changeset %s' % ctx
        if not msg['Message-Id']:
            msg['Message-Id'] = messageid(ctx, self.domain, self.messageidseed)
        if self.reply:
            unfi = self.repo.unfiltered()
            has_node = unfi.changelog.index.has_node
            predecessors = [
                unfi[ctx2]
                for ctx2 in obsutil.allpredecessors(unfi.obsstore, [ctx.node()])
                if ctx2 != ctx.node() and has_node(ctx2)
            ]
            if predecessors:
                # There is at least one predecessor, so which to pick?
                # Ideally, there is a unique root because changesets have
                # been evolved/rebased one step at a time. In this case,
                # just picking the oldest known changeset provides a stable
                # base. It doesn't help when changesets are folded. Any
                # better solution would require storing more information
                # in the repository.
                pred = min(predecessors, key=lambda ctx: ctx.rev())
                msg['In-Reply-To'] = messageid(
                    pred, self.domain, self.messageidseed
                )
        msg['To'] = ', '.join(sorted(subs))

        msgtext = msg.as_bytes() if pycompat.ispy3 else msg.as_string()
        if self.test:
            self.ui.write(msgtext)
            if not msgtext.endswith(b'\n'):
                self.ui.write(b'\n')
        else:
            self.ui.status(
                _(b'notify: sending %d subscribers %d changes\n')
                % (len(subs), count)
            )
            mail.sendmail(
                self.ui,
                emailutils.parseaddr(msg['From'])[1],
                subs,
                msgtext,
                mbox=self.mbox,
            )
###END###
def diff(self, ctx, ref=None):

        maxdiff = int(self.ui.config(b'notify', b'maxdiff'))
        prev = ctx.p1().node()
        if ref:
            ref = ref.node()
        else:
            ref = ctx.node()
        diffopts = patch.diffallopts(self.ui)
        diffopts.showfunc = self.showfunc
        chunks = patch.diff(self.repo, prev, ref, opts=diffopts)
        difflines = b''.join(chunks).splitlines()

        if self.ui.configbool(b'notify', b'diffstat'):
            maxdiffstat = int(self.ui.config(b'notify', b'maxdiffstat'))
            s = patch.diffstat(difflines)
            # s may be nil, don't include the header if it is
            if s:
                if maxdiffstat >= 0 and s.count(b"\n") > maxdiffstat + 1:
                    s = s.split(b"\n")
                    msg = _(b'\ndiffstat (truncated from %d to %d lines):\n\n')
                    self.ui.write(msg % (len(s) - 2, maxdiffstat))
                    self.ui.write(b"\n".join(s[:maxdiffstat] + s[-2:]))
                else:
                    self.ui.write(_(b'\ndiffstat:\n\n%s') % s)

        if maxdiff == 0:
            return
        elif maxdiff > 0 and len(difflines) > maxdiff:
            msg = _(b'\ndiffs (truncated from %d to %d lines):\n\n')
            self.ui.write(msg % (len(difflines), maxdiff))
            difflines = difflines[:maxdiff]
        elif difflines:
            self.ui.write(_(b'\ndiffs (%d lines):\n\n') % len(difflines))

        self.ui.write(b"\n".join(difflines))
###END###
def __init__(self, ui):
        self.ui = ui
        usermap = self.ui.config(b'bugzilla', b'usermap')
        if usermap:
            self.ui.readconfig(usermap, sections=[b'usermap'])
###END###
def map_committer(self, user):
        '''map name of committer to Bugzilla user name.'''
        for committer, bzuser in self.ui.configitems(b'usermap'):
            if committer.lower() == user.lower():
                return bzuser
        return user
###END###
def filter_real_bug_ids(self, bugs):
        '''remove bug IDs that do not exist in Bugzilla from bugs.'''
###END###
def filter_cset_known_bug_ids(self, node, bugs):
        '''remove bug IDs where node occurs in comment text from bugs.'''
###END###
def updatebug(self, bugid, newstate, text, committer):
        """update the specified bug. Add comment text and set new states.

        If possible add the comment as being from the committer of
        the changeset. Otherwise use the default Bugzilla user.
        """
###END###
def notify(self, bugs, committer):
        """Force sending of Bugzilla notification emails.

        Only required if the access method does not trigger notification
        emails automatically.
        """
###END###
def sql_buglist(ids):
        '''return SQL-friendly list of bug ids'''
        return b'(' + b','.join(map(str, ids)) + b')'
###END###
def __init__(self, ui):
        try:
            import MySQLdb as mysql

            bzmysql._MySQLdb = mysql
        except ImportError as err:
            raise error.Abort(
                _(b'python mysql support not available: %s') % err
            )

        bzaccess.__init__(self, ui)

        host = self.ui.config(b'bugzilla', b'host')
        user = self.ui.config(b'bugzilla', b'user')
        passwd = self.ui.config(b'bugzilla', b'password')
        db = self.ui.config(b'bugzilla', b'db')
        timeout = int(self.ui.config(b'bugzilla', b'timeout'))
        self.ui.note(
            _(b'connecting to %s:%s as %s, password %s\n')
            % (host, db, user, b'*' * len(passwd))
        )
        self.conn = bzmysql._MySQLdb.connect(
            host=host, user=user, passwd=passwd, db=db, connect_timeout=timeout
        )
        self.cursor = self.conn.cursor()
        self.longdesc_id = self.get_longdesc_id()
        self.user_ids = {}
        self.default_notify = b"cd %(bzdir)s && ./processmail %(id)s %(user)s"
###END###
def run(self, *args, **kwargs):
        '''run a query.'''
        self.ui.note(_(b'query: %s %s\n') % (args, kwargs))
        try:
            self.cursor.execute(*args, **kwargs)
        except bzmysql._MySQLdb.MySQLError:
            self.ui.note(_(b'failed query: %s %s\n') % (args, kwargs))
            raise
###END###
def get_longdesc_id(self):
        '''get identity of longdesc field'''
        self.run(b'select fieldid from fielddefs where name = "longdesc"')
        ids = self.cursor.fetchall()
        if len(ids) != 1:
            raise error.Abort(_(b'unknown database schema'))
        return ids[0][0]
###END###
def filter_real_bug_ids(self, bugs):
        '''filter not-existing bugs from set.'''
        self.run(
            b'select bug_id from bugs where bug_id in %s'
            % bzmysql.sql_buglist(bugs.keys())
        )
        existing = [id for (id,) in self.cursor.fetchall()]
        for id in bugs.keys():
            if id not in existing:
                self.ui.status(_(b'bug %d does not exist\n') % id)
                del bugs[id]
###END###
def filter_cset_known_bug_ids(self, node, bugs):
        '''filter bug ids that already refer to this changeset from set.'''
        self.run(
            '''select bug_id from longdescs where
                    bug_id in %s and thetext like "%%%s%%"'''
            % (bzmysql.sql_buglist(bugs.keys()), short(node))
        )
        for (id,) in self.cursor.fetchall():
            self.ui.status(
                _(b'bug %d already knows about changeset %s\n')
                % (id, short(node))
            )
            del bugs[id]
###END###
def notify(self, bugs, committer):
        '''tell bugzilla to send mail.'''
        self.ui.status(_(b'telling bugzilla to send mail:\n'))
        (user, userid) = self.get_bugzilla_user(committer)
        for id in bugs.keys():
            self.ui.status(_(b'  bug %s\n') % id)
            cmdfmt = self.ui.config(b'bugzilla', b'notify', self.default_notify)
            bzdir = self.ui.config(b'bugzilla', b'bzdir')
            try:
                # Backwards-compatible with old notify string, which
                # took one string. This will throw with a new format
                # string.
                cmd = cmdfmt % id
            except TypeError:
                cmd = cmdfmt % {b'bzdir': bzdir, b'id': id, b'user': user}
            self.ui.note(_(b'running notify command %s\n') % cmd)
            fp = procutil.popen(b'(%s) 2>&1' % cmd, b'rb')
            out = util.fromnativeeol(fp.read())
            ret = fp.close()
            if ret:
                self.ui.warn(out)
                raise error.Abort(
                    _(b'bugzilla notify command %s') % procutil.explainexit(ret)
                )
        self.ui.status(_(b'done\n'))
###END###
def get_user_id(self, user):
        '''look up numeric bugzilla user id.'''
        try:
            return self.user_ids[user]
        except KeyError:
            try:
                userid = int(user)
            except ValueError:
                self.ui.note(_(b'looking up user %s\n') % user)
                self.run(
                    '''select userid from profiles
                            where login_name like %s''',
                    user,
                )
                all = self.cursor.fetchall()
                if len(all) != 1:
                    raise KeyError(user)
                userid = int(all[0][0])
            self.user_ids[user] = userid
            return userid
###END###
def get_bugzilla_user(self, committer):
        """See if committer is a registered bugzilla user. Return
        bugzilla username and userid if so. If not, return default
        bugzilla username and userid."""
        user = self.map_committer(committer)
        try:
            userid = self.get_user_id(user)
        except KeyError:
            try:
                defaultuser = self.ui.config(b'bugzilla', b'bzuser')
                if not defaultuser:
                    raise error.Abort(
                        _(b'cannot find bugzilla user id for %s') % user
                    )
                userid = self.get_user_id(defaultuser)
                user = defaultuser
            except KeyError:
                raise error.Abort(
                    _(b'cannot find bugzilla user id for %s or %s')
                    % (user, defaultuser)
                )
        return (user, userid)
###END###
def updatebug(self, bugid, newstate, text, committer):
        """update bug state with comment text.

        Try adding comment as committer of changeset, otherwise as
        default bugzilla user."""
        if len(newstate) > 0:
            self.ui.warn(_(b"Bugzilla/MySQL cannot update bug state\n"))

        (user, userid) = self.get_bugzilla_user(committer)
        now = time.strftime('%Y-%m-%d %H:%M:%S')
        self.run(
            '''insert into longdescs
                    (bug_id, who, bug_when, thetext)
                    values (%s, %s, %s, %s)''',
            (bugid, userid, now, text),
        )
        self.run(
            '''insert into bugs_activity (bug_id, who, bug_when, fieldid)
                    values (%s, %s, %s, %s)''',
            (bugid, userid, now, self.longdesc_id),
        )
        self.conn.commit()
###END###
def __init__(self, ui):
        bzmysql.__init__(self, ui)
        self.default_notify = (
            b"cd %(bzdir)s && perl -T contrib/sendbugmail.pl %(id)s %(user)s"
        )
###END###
def __init__(self, ui):
        bzmysql_2_18.__init__(self, ui)
###END###
def get_longdesc_id(self):
        '''get identity of longdesc field'''
        self.run(b'select id from fielddefs where name = "longdesc"')
        ids = self.cursor.fetchall()
        if len(ids) != 1:
            raise error.Abort(_(b'unknown database schema'))
        return ids[0][0]
###END###
def send_cookies(self, connection):
        if self.cookies:
            for cookie in self.cookies:
                connection.putheader(b"Cookie", cookie)
###END###
def request(self, host, handler, request_body, verbose=0):
        self.verbose = verbose
        self.accept_gzip_encoding = False

        # issue XML-RPC request
        h = self.make_connection(host)
        if verbose:
            h.set_debuglevel(1)

        self.send_request(h, handler, request_body)
        self.send_host(h, host)
        self.send_cookies(h)
        self.send_user_agent(h)
        self.send_content(h, request_body)

        # Deal with differences between Python 2.6 and 2.7.
        # In the former h is a HTTP(S). In the latter it's a
        # HTTP(S)Connection. Luckily, the 2.6 implementation of
        # HTTP(S) has an underlying HTTP(S)Connection, so extract
        # that and use it.
        try:
            response = h.getresponse()
        except AttributeError:
            response = h._conn.getresponse()

        # Add any cookie definitions to our list.
        for header in response.msg.getallmatchingheaders(b"Set-Cookie"):
            val = header.split(b": ", 1)[1]
            cookie = val.split(b";", 1)[0]
            self.cookies.append(cookie)

        if response.status != 200:
            raise xmlrpclib.ProtocolError(
                host + handler,
                response.status,
                response.reason,
                response.msg.headers,
            )

        payload = response.read()
        parser, unmarshaller = self.getparser()
        parser.feed(payload)
        parser.close()

        return unmarshaller.close()
###END###
def __init__(self, use_datetime=0):
        if util.safehasattr(xmlrpclib.Transport, "__init__"):
            xmlrpclib.Transport.__init__(self, use_datetime)
###END###
def __init__(self, use_datetime=0):
        if util.safehasattr(xmlrpclib.Transport, "__init__"):
            xmlrpclib.SafeTransport.__init__(self, use_datetime)
###END###
def __init__(self, ui):
        bzaccess.__init__(self, ui)

        bzweb = self.ui.config(b'bugzilla', b'bzurl')
        bzweb = bzweb.rstrip(b"/") + b"/xmlrpc.cgi"

        user = self.ui.config(b'bugzilla', b'user')
        passwd = self.ui.config(b'bugzilla', b'password')

        self.fixstatus = self.ui.config(b'bugzilla', b'fixstatus')
        self.fixresolution = self.ui.config(b'bugzilla', b'fixresolution')

        self.bzproxy = xmlrpclib.ServerProxy(
            pycompat.strurl(bzweb), self.transport(bzweb)
        )
        ver = self.bzproxy.Bugzilla.version()[b'version'].split(b'.')
        self.bzvermajor = int(ver[0])
        self.bzverminor = int(ver[1])
        login = self.bzproxy.User.login(
            {b'login': user, b'password': passwd, b'restrict_login': True}
        )
        self.bztoken = login.get(b'token', b'')
###END###
def transport(self, uri):
        if util.urlreq.urlparse(uri, b"http")[0] == b"https":
            return cookiesafetransport()
        else:
            return cookietransport()
###END###
def get_bug_comments(self, id):
        """Return a string with all comment text for a bug."""
        c = self.bzproxy.Bug.comments(
            {b'ids': [id], b'include_fields': [b'text'], b'token': self.bztoken}
        )
        return b''.join(
            [t[b'text'] for t in c[b'bugs'][b'%d' % id][b'comments']]
        )
###END###
def filter_real_bug_ids(self, bugs):
        probe = self.bzproxy.Bug.get(
            {
                b'ids': sorted(bugs.keys()),
                b'include_fields': [],
                b'permissive': True,
                b'token': self.bztoken,
            }
        )
        for badbug in probe[b'faults']:
            id = badbug[b'id']
            self.ui.status(_(b'bug %d does not exist\n') % id)
            del bugs[id]
###END###
def filter_cset_known_bug_ids(self, node, bugs):
        for id in sorted(bugs.keys()):
            if self.get_bug_comments(id).find(short(node)) != -1:
                self.ui.status(
                    _(b'bug %d already knows about changeset %s\n')
                    % (id, short(node))
                )
                del bugs[id]
###END###
def updatebug(self, bugid, newstate, text, committer):
        args = {}
        if b'hours' in newstate:
            args[b'work_time'] = newstate[b'hours']

        if self.bzvermajor >= 4:
            args[b'ids'] = [bugid]
            args[b'comment'] = {b'body': text}
            if b'fix' in newstate:
                args[b'status'] = self.fixstatus
                args[b'resolution'] = self.fixresolution
            args[b'token'] = self.bztoken
            self.bzproxy.Bug.update(args)
        else:
            if b'fix' in newstate:
                self.ui.warn(
                    _(
                        b"Bugzilla/XMLRPC needs Bugzilla 4.0 or later "
                        b"to mark bugs fixed\n"
                    )
                )
            args[b'id'] = bugid
            args[b'comment'] = text
            self.bzproxy.Bug.add_comment(args)
###END###
def __init__(self, ui):
        bzxmlrpc.__init__(self, ui)

        self.bzemail = self.ui.config(b'bugzilla', b'bzemail')
        if not self.bzemail:
            raise error.Abort(_(b"configuration 'bzemail' missing"))
        mail.validateconfig(self.ui)
###END###
def makecommandline(self, fieldname, value):
        if self.bzvermajor >= 4:
            return b"@%s %s" % (fieldname, pycompat.bytestr(value))
        else:
            if fieldname == b"id":
                fieldname = b"bug_id"
            return b"@%s = %s" % (fieldname, pycompat.bytestr(value))
###END###
def send_bug_modify_email(self, bugid, commands, comment, committer):
        """send modification message to Bugzilla bug via email.

        The message format is documented in the Bugzilla email_in.pl
        specification. commands is a list of command lines, comment is the
        comment text.

        To stop users from crafting commit comments with
        Bugzilla commands, specify the bug ID via the message body, rather
        than the subject line, and leave a blank line after it.
        """
        user = self.map_committer(committer)
        matches = self.bzproxy.User.get(
            {b'match': [user], b'token': self.bztoken}
        )
        if not matches[b'users']:
            user = self.ui.config(b'bugzilla', b'user')
            matches = self.bzproxy.User.get(
                {b'match': [user], b'token': self.bztoken}
            )
            if not matches[b'users']:
                raise error.Abort(
                    _(b"default bugzilla user %s email not found") % user
                )
        user = matches[b'users'][0][b'email']
        commands.append(self.makecommandline(b"id", bugid))

        text = b"\n".join(commands) + b"\n\n" + comment

        _charsets = mail._charsets(self.ui)
        user = mail.addressencode(self.ui, user, _charsets)
        bzemail = mail.addressencode(self.ui, self.bzemail, _charsets)
        msg = mail.mimeencode(self.ui, text, _charsets)
        msg[b'From'] = user
        msg[b'To'] = bzemail
        msg[b'Subject'] = mail.headencode(
            self.ui, b"Bug modification", _charsets
        )
        sendmail = mail.connect(self.ui)
        sendmail(user, bzemail, msg.as_string())
###END###
def updatebug(self, bugid, newstate, text, committer):
        cmds = []
        if b'hours' in newstate:
            cmds.append(self.makecommandline(b"work_time", newstate[b'hours']))
        if b'fix' in newstate:
            cmds.append(self.makecommandline(b"bug_status", self.fixstatus))
            cmds.append(self.makecommandline(b"resolution", self.fixresolution))
        self.send_bug_modify_email(bugid, cmds, text, committer)
###END###
def __init__(self, ui):
        bzaccess.__init__(self, ui)
        bz = self.ui.config(b'bugzilla', b'bzurl')
        self.bzroot = b'/'.join([bz, b'rest'])
        self.apikey = self.ui.config(b'bugzilla', b'apikey')
        self.user = self.ui.config(b'bugzilla', b'user')
        self.passwd = self.ui.config(b'bugzilla', b'password')
        self.fixstatus = self.ui.config(b'bugzilla', b'fixstatus')
        self.fixresolution = self.ui.config(b'bugzilla', b'fixresolution')
###END###
def apiurl(self, targets, include_fields=None):
        url = b'/'.join([self.bzroot] + [pycompat.bytestr(t) for t in targets])
        qv = {}
        if self.apikey:
            qv[b'api_key'] = self.apikey
        elif self.user and self.passwd:
            qv[b'login'] = self.user
            qv[b'password'] = self.passwd
        if include_fields:
            qv[b'include_fields'] = include_fields
        if qv:
            url = b'%s?%s' % (url, util.urlreq.urlencode(qv))
        return url
###END###
def _fetch(self, burl):
        try:
            resp = url.open(self.ui, burl)
            return pycompat.json_loads(resp.read())
        except util.urlerr.httperror as inst:
            if inst.code == 401:
                raise error.Abort(_(b'authorization failed'))
            if inst.code == 404:
                raise NotFound()
            else:
                raise
###END###
def _submit(self, burl, data, method=b'POST'):
        data = json.dumps(data)
        if method == b'PUT':

            class putrequest(util.urlreq.request):
                def get_method(self):
                    return b'PUT'

            request_type = putrequest
        else:
            request_type = util.urlreq.request
        req = request_type(burl, data, {b'Content-Type': b'application/json'})
        try:
            resp = url.opener(self.ui).open(req)
            return pycompat.json_loads(resp.read())
        except util.urlerr.httperror as inst:
            if inst.code == 401:
                raise error.Abort(_(b'authorization failed'))
            if inst.code == 404:
                raise NotFound()
            else:
                raise
###END###
def filter_real_bug_ids(self, bugs):
        '''remove bug IDs that do not exist in Bugzilla from bugs.'''
        badbugs = set()
        for bugid in bugs:
            burl = self.apiurl((b'bug', bugid), include_fields=b'status')
            try:
                self._fetch(burl)
            except NotFound:
                badbugs.add(bugid)
        for bugid in badbugs:
            del bugs[bugid]
###END###
def filter_cset_known_bug_ids(self, node, bugs):
        '''remove bug IDs where node occurs in comment text from bugs.'''
        sn = short(node)
        for bugid in bugs.keys():
            burl = self.apiurl(
                (b'bug', bugid, b'comment'), include_fields=b'text'
            )
            result = self._fetch(burl)
            comments = result[b'bugs'][pycompat.bytestr(bugid)][b'comments']
            if any(sn in c[b'text'] for c in comments):
                self.ui.status(
                    _(b'bug %d already knows about changeset %s\n')
                    % (bugid, sn)
                )
                del bugs[bugid]
###END###
def updatebug(self, bugid, newstate, text, committer):
        """update the specified bug. Add comment text and set new states.

        If possible add the comment as being from the committer of
        the changeset. Otherwise use the default Bugzilla user.
        """
        bugmod = {}
        if b'hours' in newstate:
            bugmod[b'work_time'] = newstate[b'hours']
        if b'fix' in newstate:
            bugmod[b'status'] = self.fixstatus
            bugmod[b'resolution'] = self.fixresolution
        if bugmod:
            # if we have to change the bugs state do it here
            bugmod[b'comment'] = {
                b'comment': text,
                b'is_private': False,
                b'is_markdown': False,
            }
            burl = self.apiurl((b'bug', bugid))
            self._submit(burl, bugmod, method=b'PUT')
            self.ui.debug(b'updated bug %s\n' % bugid)
        else:
            burl = self.apiurl((b'bug', bugid, b'comment'))
            self._submit(
                burl,
                {
                    b'comment': text,
                    b'is_private': False,
                    b'is_markdown': False,
                },
            )
            self.ui.debug(b'added comment to bug %s\n' % bugid)
###END###
def notify(self, bugs, committer):
        """Force sending of Bugzilla notification emails.

        Only required if the access method does not trigger notification
        emails automatically.
        """
        pass
###END###
def __init__(self, ui, repo):
        self.ui = ui
        self.repo = repo

        bzversion = self.ui.config(b'bugzilla', b'version')
        try:
            bzclass = bugzilla._versions[bzversion]
        except KeyError:
            raise error.Abort(
                _(b'bugzilla version %s not supported') % bzversion
            )
        self.bzdriver = bzclass(self.ui)

        self.bug_re = re.compile(
            self.ui.config(b'bugzilla', b'regexp'), re.IGNORECASE
        )
        self.fix_re = re.compile(
            self.ui.config(b'bugzilla', b'fixregexp'), re.IGNORECASE
        )
        self.split_re = re.compile(br'\D+')
###END###
def find_bugs(self, ctx):
        """return bugs dictionary created from commit comment.

        Extract bug info from changeset comments. Filter out any that are
        not known to Bugzilla, and any that already have a reference to
        the given changeset in their comments.
        """
        start = 0
        bugs = {}
        bugmatch = self.bug_re.search(ctx.description(), start)
        fixmatch = self.fix_re.search(ctx.description(), start)
        while True:
            bugattribs = {}
            if not bugmatch and not fixmatch:
                break
            if not bugmatch:
                m = fixmatch
            elif not fixmatch:
                m = bugmatch
            else:
                if bugmatch.start() < fixmatch.start():
                    m = bugmatch
                else:
                    m = fixmatch
            start = m.end()
            if m is bugmatch:
                bugmatch = self.bug_re.search(ctx.description(), start)
                if b'fix' in bugattribs:
                    del bugattribs[b'fix']
            else:
                fixmatch = self.fix_re.search(ctx.description(), start)
                bugattribs[b'fix'] = None

            try:
                ids = m.group(b'ids')
            except IndexError:
                ids = m.group(1)
            try:
                hours = float(m.group(b'hours'))
                bugattribs[b'hours'] = hours
            except IndexError:
                pass
            except TypeError:
                pass
            except ValueError:
                self.ui.status(_(b"%s: invalid hours\n") % m.group(b'hours'))

            for id in self.split_re.split(ids):
                if not id:
                    continue
                bugs[int(id)] = bugattribs
        if bugs:
            self.bzdriver.filter_real_bug_ids(bugs)
        if bugs:
            self.bzdriver.filter_cset_known_bug_ids(ctx.node(), bugs)
        return bugs
###END###
def update(self, bugid, newstate, ctx):
        '''update bugzilla bug with reference to changeset.'''

        def webroot(root):
            """strip leading prefix of repo root and turn into
            url-safe path."""
            count = int(self.ui.config(b'bugzilla', b'strip'))
            root = util.pconvert(root)
            while count > 0:
                c = root.find(b'/')
                if c == -1:
                    break
                root = root[c + 1 :]
                count -= 1
            return root

        mapfile = None
        tmpl = self.ui.config(b'bugzilla', b'template')
        if not tmpl:
            mapfile = self.ui.config(b'bugzilla', b'style')
        if not mapfile and not tmpl:
            tmpl = _(
                b'changeset {node|short} in repo {root} refers '
                b'to bug {bug}.\ndetails:\n\t{desc|tabindent}'
            )
        spec = logcmdutil.templatespec(tmpl, mapfile)
        t = logcmdutil.changesettemplater(self.ui, self.repo, spec)
        self.ui.pushbuffer()
        t.show(
            ctx,
            changes=ctx.changeset(),
            bug=pycompat.bytestr(bugid),
            hgweb=self.ui.config(b'web', b'baseurl'),
            root=self.repo.root,
            webroot=webroot(self.repo.root),
        )
        data = self.ui.popbuffer()
        self.bzdriver.updatebug(
            bugid, newstate, data, stringutil.email(ctx.user())
        )
###END###
def notify(self, bugs, committer):
        '''ensure Bugzilla users are notified of bug change.'''
        self.bzdriver.notify(bugs, committer)
###END###
def get_method(self):
                    return b'PUT'
###END###
def __init__(self, path, cmdline, isgui):
        # We can't pass non-ASCII through docstrings (and path is
        # in an unknown encoding anyway), but avoid double separators on
        # Windows
        docpath = stringutil.escapestr(path).replace(b'\\\\', b'\\')
        self.__doc__ %= {'path': pycompat.sysstr(stringutil.uirepr(docpath))}
        self._cmdline = cmdline
        self._isgui = isgui
###END###
def __call__(self, ui, repo, *pats, **opts):
        opts = pycompat.byteskwargs(opts)
        options = b' '.join(map(procutil.shellquote, opts[b'option']))
        if options:
            options = b' ' + options
        return dodiff(
            ui, repo, self._cmdline + options, pats, opts, guitool=self._isgui
        )
###END###
def lookup(self, key):
            try:
                _super = super(parentrevspecrepo, self)
                return _super.lookup(key)
            except error.RepoError:
                pass

            circ = key.find('^')
            tilde = key.find('~')
            if circ < 0 and tilde < 0:
                raise
            elif circ >= 0 and tilde >= 0:
                end = min(circ, tilde)
            else:
                end = max(circ, tilde)

            cl = self.changelog
            base = key[:end]
            try:
                node = _super.lookup(base)
            except error.RepoError:
                # eek - reraise the first error
                return _super.lookup(key)

            rev = cl.rev(node)
            suffix = key[end:]
            i = 0
            while i < len(suffix):
                # foo^N => Nth parent of foo
                # foo^0 == foo
                # foo^1 == foo^ == 1st parent of foo
                # foo^2 == 2nd parent of foo
                if suffix[i] == '^':
                    j = i + 1
                    p = cl.parentrevs(rev)
                    if j < len(suffix) and suffix[j].isdigit():
                        j += 1
                        n = int(suffix[i + 1:j])
                        if n > 2 or n == 2 and p[1] == -1:
                            raise
                    else:
                        n = 1
                    if n:
                        rev = p[n - 1]
                    i = j
                # foo~N => Nth first grandparent of foo
                # foo~0 = foo
                # foo~1 = foo^1 == foo^ == 1st parent of foo
                # foo~2 = foo^1^1 == foo^^ == 1st parent of 1st parent of foo
                elif suffix[i] == '~':
                    j = i + 1
                    while j < len(suffix) and suffix[j].isdigit():
                        j += 1
                    if j == i + 1:
                        raise
                    n = int(suffix[i + 1:j])
                    for k in xrange(n):
                        rev = cl.parentrevs(rev)[0]
                    i = j
                else:
                    raise
            return cl.node(rev)
###END###
def __getitem__(name):
        def nullfunc(*args, **kwds):
            return

        return nullfunc
###END###
def __init__(self, repo):
        self._repo = repo
###END###
def data(self):
        return b''
###END###
def node(self):
        return self._repo.nullid
###END###
def __init__(self, basectx, memworkingcopy):
        self.basectx = basectx
        self.memworkingcopy = memworkingcopy
###END###
def getfile(self, path):
        """comply with mercurial.patch.filestore.getfile"""
        if path not in self.basectx:
            return None, None, None
        fctx = self.basectx[path]
        if path in self.memworkingcopy:
            content = self.memworkingcopy[path]
        else:
            content = fctx.data()
        mode = (fctx.islink(), fctx.isexec())
        copy = fctx.copysource()
        return content, mode, copy
###END###
def __init__(self, fctxs, path, ui=None, opts=None):
        """([fctx], ui or None) -> None

        fctxs should be linear, and sorted by topo order - oldest first.
        fctxs[0] will be considered as "immutable" and will not be changed.
        """
        self.fctxs = fctxs
        self.path = path
        self.ui = ui or nullui()
        self.opts = opts or {}

        # following fields are built from fctxs. they exist for perf reason
        self.contents = [f.data() for f in fctxs]
        self.contentlines = pycompat.maplist(mdiff.splitnewlines, self.contents)
        self.linelog = self._buildlinelog()
        if self.ui.debugflag:
            assert self._checkoutlinelog() == self.contents

        # following fields will be filled later
        self.chunkstats = [0, 0]  # [adopted, total : int]
        self.targetlines = []  # [str]
        self.fixups = []  # [(linelog rev, a1, a2, b1, b2)]
        self.finalcontents = []  # [str]
        self.ctxaffected = set()
###END###
def diffwith(self, targetfctx, fm=None):
        """calculate fixups needed by examining the differences between
        self.fctxs[-1] and targetfctx, chunk by chunk.

        targetfctx is the target state we move towards. we may or may not be
        able to get there because not all modified chunks can be amended into
        a non-public fctx unambiguously.

        call this only once, before apply().

        update self.fixups, self.chunkstats, and self.targetlines.
        """
        a = self.contents[-1]
        alines = self.contentlines[-1]
        b = targetfctx.data()
        blines = mdiff.splitnewlines(b)
        self.targetlines = blines

        self.linelog.annotate(self.linelog.maxrev)
        annotated = self.linelog.annotateresult  # [(linelog rev, linenum)]
        assert len(annotated) == len(alines)
        # add a dummy end line to make insertion at the end easier
        if annotated:
            dummyendline = (annotated[-1][0], annotated[-1][1] + 1)
            annotated.append(dummyendline)

        # analyse diff blocks
        for chunk in self._alldiffchunks(a, b, alines, blines):
            newfixups = self._analysediffchunk(chunk, annotated)
            self.chunkstats[0] += bool(newfixups)  # 1 or 0
            self.chunkstats[1] += 1
            self.fixups += newfixups
            if fm is not None:
                self._showchanges(fm, alines, blines, chunk, newfixups)
###END###
def apply(self):
        """apply self.fixups. update self.linelog, self.finalcontents.

        call this only once, before getfinalcontent(), after diffwith().
        """
        # the following is unnecessary, as it's done by "diffwith":
        #   self.linelog.annotate(self.linelog.maxrev)
        for rev, a1, a2, b1, b2 in reversed(self.fixups):
            blines = self.targetlines[b1:b2]
            if self.ui.debugflag:
                idx = (max(rev - 1, 0)) // 2
                self.ui.write(
                    _(b'%s: chunk %d:%d -> %d lines\n')
                    % (short(self.fctxs[idx].node()), a1, a2, len(blines))
                )
            self.linelog.replacelines(rev, a1, a2, b1, b2)
        if self.opts.get(b'edit_lines', False):
            self.finalcontents = self._checkoutlinelogwithedits()
        else:
            self.finalcontents = self._checkoutlinelog()
###END###
def getfinalcontent(self, fctx):
        """(fctx) -> str. get modified file content for a given filecontext"""
        idx = self.fctxs.index(fctx)
        return self.finalcontents[idx]
###END###
def _analysediffchunk(self, chunk, annotated):
        """analyse a different chunk and return new fixups found

        return [] if no lines from the chunk can be safely applied.

        the chunk (or lines) cannot be safely applied, if, for example:
          - the modified (deleted) lines belong to a public changeset
            (self.fctxs[0])
          - the chunk is a pure insertion and the adjacent lines (at most 2
            lines) belong to different non-public changesets, or do not belong
            to any non-public changesets.
          - the chunk is modifying lines from different changesets.
            in this case, if the number of lines deleted equals to the number
            of lines added, assume it's a simple 1:1 map (could be wrong).
            otherwise, give up.
          - the chunk is modifying lines from a single non-public changeset,
            but other revisions touch the area as well. i.e. the lines are
            not continuous as seen from the linelog.
        """
        a1, a2, b1, b2 = chunk
        # find involved indexes from annotate result
        involved = annotated[a1:a2]
        if not involved and annotated:  # a1 == a2 and a is not empty
            # pure insertion, check nearby lines. ignore lines belong
            # to the public (first) changeset (i.e. annotated[i][0] == 1)
            nearbylinenums = {a2, max(0, a1 - 1)}
            involved = [
                annotated[i] for i in nearbylinenums if annotated[i][0] != 1
            ]
        involvedrevs = list({r for r, l in involved})
        newfixups = []
        if len(involvedrevs) == 1 and self._iscontinuous(a1, a2 - 1, True):
            # chunk belongs to a single revision
            rev = involvedrevs[0]
            if rev > 1:
                fixuprev = rev + 1
                newfixups.append((fixuprev, a1, a2, b1, b2))
        elif a2 - a1 == b2 - b1 or b1 == b2:
            # 1:1 line mapping, or chunk was deleted
            for i in pycompat.xrange(a1, a2):
                rev, linenum = annotated[i]
                if rev > 1:
                    if b1 == b2:  # deletion, simply remove that single line
                        nb1 = nb2 = 0
                    else:  # 1:1 line mapping, change the corresponding rev
                        nb1 = b1 + i - a1
                        nb2 = nb1 + 1
                    fixuprev = rev + 1
                    newfixups.append((fixuprev, i, i + 1, nb1, nb2))
        return self._optimizefixups(newfixups)
###END###
def _alldiffchunks(a, b, alines, blines):
        """like mdiff.allblocks, but only care about differences"""
        blocks = mdiff.allblocks(a, b, lines1=alines, lines2=blines)
        for chunk, btype in blocks:
            if btype != b'!':
                continue
            yield chunk
###END###
def _buildlinelog(self):
        """calculate the initial linelog based on self.content{,line}s.
        this is similar to running a partial "annotate".
        """
        llog = linelog.linelog()
        a, alines = b'', []
        for i in pycompat.xrange(len(self.contents)):
            b, blines = self.contents[i], self.contentlines[i]
            llrev = i * 2 + 1
            chunks = self._alldiffchunks(a, b, alines, blines)
            for a1, a2, b1, b2 in reversed(list(chunks)):
                llog.replacelines(llrev, a1, a2, b1, b2)
            a, alines = b, blines
        return llog
###END###
def _checkoutlinelog(self):
        """() -> [str]. check out file contents from linelog"""
        contents = []
        for i in pycompat.xrange(len(self.contents)):
            rev = (i + 1) * 2
            self.linelog.annotate(rev)
            content = b''.join(map(self._getline, self.linelog.annotateresult))
            contents.append(content)
        return contents
###END###
def _checkoutlinelogwithedits(self):
        """() -> [str]. prompt all lines for edit"""
        alllines = self.linelog.getalllines()
        # header
        editortext = (
            _(
                b'HG: editing %s\nHG: "y" means the line to the right '
                b'exists in the changeset to the top\nHG:\n'
            )
            % self.fctxs[-1].path()
        )
        # [(idx, fctx)]. hide the dummy emptyfilecontext
        visiblefctxs = [
            (i, f)
            for i, f in enumerate(self.fctxs)
            if not isinstance(f, emptyfilecontext)
        ]
        for i, (j, f) in enumerate(visiblefctxs):
            editortext += _(b'HG: %s/%s %s %s\n') % (
                b'|' * i,
                b'-' * (len(visiblefctxs) - i + 1),
                short(f.node()),
                f.description().split(b'\n', 1)[0],
            )
        editortext += _(b'HG: %s\n') % (b'|' * len(visiblefctxs))
        # figure out the lifetime of a line, this is relatively inefficient,
        # but probably fine
        lineset = defaultdict(lambda: set())  # {(llrev, linenum): {llrev}}
        for i, f in visiblefctxs:
            self.linelog.annotate((i + 1) * 2)
            for l in self.linelog.annotateresult:
                lineset[l].add(i)
        # append lines
        for l in alllines:
            editortext += b'    %s : %s' % (
                b''.join(
                    [
                        (b'y' if i in lineset[l] else b' ')
                        for i, _f in visiblefctxs
                    ]
                ),
                self._getline(l),
            )
        # run editor
        editedtext = self.ui.edit(editortext, b'', action=b'absorb')
        if not editedtext:
            raise error.InputError(_(b'empty editor text'))
        # parse edited result
        contents = [b''] * len(self.fctxs)
        leftpadpos = 4
        colonpos = leftpadpos + len(visiblefctxs) + 1
        for l in mdiff.splitnewlines(editedtext):
            if l.startswith(b'HG:'):
                continue
            if l[colonpos - 1 : colonpos + 2] != b' : ':
                raise error.InputError(_(b'malformed line: %s') % l)
            linecontent = l[colonpos + 2 :]
            for i, ch in enumerate(
                pycompat.bytestr(l[leftpadpos : colonpos - 1])
            ):
                if ch == b'y':
                    contents[visiblefctxs[i][0]] += linecontent
        # chunkstats is hard to calculate if anything changes, therefore
        # set them to just a simple value (1, 1).
        if editedtext != editortext:
            self.chunkstats = [1, 1]
        return contents
###END###
def _getline(self, lineinfo):
        """((rev, linenum)) -> str. convert rev+line number to line content"""
        rev, linenum = lineinfo
        if rev & 1:  # odd: original line taken from fctxs
            return self.contentlines[rev // 2][linenum]
        else:  # even: fixup line from targetfctx
            return self.targetlines[linenum]
###END###
def _iscontinuous(self, a1, a2, closedinterval=False):
        """(a1, a2 : int) -> bool

        check if these lines are continuous. i.e. no other insertions or
        deletions (from other revisions) among these lines.

        closedinterval decides whether a2 should be included or not. i.e. is
        it [a1, a2), or [a1, a2] ?
        """
        if a1 >= a2:
            return True
        llog = self.linelog
        offset1 = llog.getoffset(a1)
        offset2 = llog.getoffset(a2) + int(closedinterval)
        linesinbetween = llog.getalllines(offset1, offset2)
        return len(linesinbetween) == a2 - a1 + int(closedinterval)
###END###
def _optimizefixups(self, fixups):
        """[(rev, a1, a2, b1, b2)] -> [(rev, a1, a2, b1, b2)].
        merge adjacent fixups to make them less fragmented.
        """
        result = []
        pcurrentchunk = [[-1, -1, -1, -1, -1]]

        def pushchunk():
            if pcurrentchunk[0][0] != -1:
                result.append(tuple(pcurrentchunk[0]))

        for i, chunk in enumerate(fixups):
            rev, a1, a2, b1, b2 = chunk
            lastrev = pcurrentchunk[0][0]
            lasta2 = pcurrentchunk[0][2]
            lastb2 = pcurrentchunk[0][4]
            if (
                a1 == lasta2
                and b1 == lastb2
                and rev == lastrev
                and self._iscontinuous(max(a1 - 1, 0), a1)
            ):
                # merge into currentchunk
                pcurrentchunk[0][2] = a2
                pcurrentchunk[0][4] = b2
            else:
                pushchunk()
                pcurrentchunk[0] = list(chunk)
        pushchunk()
        return result
###END###
def _showchanges(self, fm, alines, blines, chunk, fixups):
        def trim(line):
            if line.endswith(b'\n'):
                line = line[:-1]
            return line

        # this is not optimized for perf but _showchanges only gets executed
        # with an extra command-line flag.
        a1, a2, b1, b2 = chunk
        aidxs, bidxs = [0] * (a2 - a1), [0] * (b2 - b1)
        for idx, fa1, fa2, fb1, fb2 in fixups:
            for i in pycompat.xrange(fa1, fa2):
                aidxs[i - a1] = (max(idx, 1) - 1) // 2
            for i in pycompat.xrange(fb1, fb2):
                bidxs[i - b1] = (max(idx, 1) - 1) // 2

        fm.startitem()
        fm.write(
            b'hunk',
            b'        %s\n',
            b'@@ -%d,%d +%d,%d @@' % (a1, a2 - a1, b1, b2 - b1),
            label=b'diff.hunk',
        )
        fm.data(path=self.path, linetype=b'hunk')

        def writeline(idx, diffchar, line, linetype, linelabel):
            fm.startitem()
            node = b''
            if idx:
                ctx = self.fctxs[idx]
                fm.context(fctx=ctx)
                node = ctx.hex()
                self.ctxaffected.add(ctx.changectx())
            fm.write(b'node', b'%-7.7s ', node, label=b'absorb.node')
            fm.write(
                b'diffchar ' + linetype,
                b'%s%s\n',
                diffchar,
                line,
                label=linelabel,
            )
            fm.data(path=self.path, linetype=linetype)

        for i in pycompat.xrange(a1, a2):
            writeline(
                aidxs[i - a1],
                b'-',
                trim(alines[i]),
                b'deleted',
                b'diff.deleted',
            )
        for i in pycompat.xrange(b1, b2):
            writeline(
                bidxs[i - b1],
                b'+',
                trim(blines[i]),
                b'inserted',
                b'diff.inserted',
            )
###END###
def __init__(self, stack, ui=None, opts=None):
        """([ctx], ui or None) -> None

        stack: should be linear, and sorted by topo order - oldest first.
        all commits in stack are considered mutable.
        """
        assert stack
        self.ui = ui or nullui()
        self.opts = opts or {}
        self.stack = stack
        self.repo = stack[-1].repo().unfiltered()

        # following fields will be filled later
        self.paths = []  # [str]
        self.status = None  # ctx.status output
        self.fctxmap = {}  # {path: {ctx: fctx}}
        self.fixupmap = {}  # {path: filefixupstate}
        self.replacemap = {}  # {oldnode: newnode or None}
        self.finalnode = None  # head after all fixups
        self.ctxaffected = set()
###END###
def diffwith(self, targetctx, match=None, fm=None):
        """diff and prepare fixups. update self.fixupmap, self.paths"""
        # only care about modified files
        self.status = self.stack[-1].status(targetctx, match)
        self.paths = []
        # but if --edit-lines is used, the user may want to edit files
        # even if they are not modified
        editopt = self.opts.get(b'edit_lines')
        if not self.status.modified and editopt and match:
            interestingpaths = match.files()
        else:
            interestingpaths = self.status.modified
        # prepare the filefixupstate
        seenfctxs = set()
        # sorting is necessary to eliminate ambiguity for the "double move"
        # case: "hg cp A B; hg cp A C; hg rm A", then only "B" can affect "A".
        for path in sorted(interestingpaths):
            self.ui.debug(b'calculating fixups for %s\n' % path)
            targetfctx = targetctx[path]
            fctxs, ctx2fctx = getfilestack(self.stack, path, seenfctxs)
            # ignore symbolic links or binary, or unchanged files
            if any(
                f.islink() or stringutil.binary(f.data())
                for f in [targetfctx] + fctxs
                if not isinstance(f, emptyfilecontext)
            ):
                continue
            if targetfctx.data() == fctxs[-1].data() and not editopt:
                continue
            seenfctxs.update(fctxs[1:])
            self.fctxmap[path] = ctx2fctx
            fstate = filefixupstate(fctxs, path, ui=self.ui, opts=self.opts)
            if fm is not None:
                fm.startitem()
                fm.plain(b'showing changes for ')
                fm.write(b'path', b'%s\n', path, label=b'absorb.path')
                fm.data(linetype=b'path')
            fstate.diffwith(targetfctx, fm)
            self.fixupmap[path] = fstate
            self.paths.append(path)
            self.ctxaffected.update(fstate.ctxaffected)
###END###
def apply(self):
        """apply fixups to individual filefixupstates"""
        for path, state in pycompat.iteritems(self.fixupmap):
            if self.ui.debugflag:
                self.ui.write(_(b'applying fixups to %s\n') % path)
            state.apply()
###END###
def chunkstats(self):
        """-> {path: chunkstats}. collect chunkstats from filefixupstates"""
        return {
            path: state.chunkstats
            for path, state in pycompat.iteritems(self.fixupmap)
        }
###END###
def commit(self):
        """commit changes. update self.finalnode, self.replacemap"""
        with self.repo.transaction(b'absorb') as tr:
            self._commitstack()
            self._movebookmarks(tr)
            if self.repo[b'.'].node() in self.replacemap:
                self._moveworkingdirectoryparent()
            self._cleanupoldcommits()
        return self.finalnode
###END###
def printchunkstats(self):
        """print things like '1 of 2 chunk(s) applied'"""
        ui = self.ui
        chunkstats = self.chunkstats
        if ui.verbose:
            # chunkstats for each file
            for path, stat in pycompat.iteritems(chunkstats):
                if stat[0]:
                    ui.write(
                        _(b'%s: %d of %d chunk(s) applied\n')
                        % (path, stat[0], stat[1])
                    )
        elif not ui.quiet:
            # a summary for all files
            stats = chunkstats.values()
            applied, total = (sum(s[i] for s in stats) for i in (0, 1))
            ui.write(_(b'%d of %d chunk(s) applied\n') % (applied, total))
###END###
def _commitstack(self):
        """make new commits. update self.finalnode, self.replacemap.
        it is splitted from "commit" to avoid too much indentation.
        """
        # last node (20-char) committed by us
        lastcommitted = None
        # p1 which overrides the parent of the next commit, "None" means use
        # the original parent unchanged
        nextp1 = None
        for ctx in self.stack:
            memworkingcopy = self._getnewfilecontents(ctx)
            if not memworkingcopy and not lastcommitted:
                # nothing changed, nothing commited
                nextp1 = ctx
                continue
            willbecomenoop = ctx.files() and self._willbecomenoop(
                memworkingcopy, ctx, nextp1
            )
            if self.skip_empty_successor and willbecomenoop:
                # changeset is no longer necessary
                self.replacemap[ctx.node()] = None
                msg = _(b'became empty and was dropped')
            else:
                # changeset needs re-commit
                nodestr = self._commitsingle(memworkingcopy, ctx, p1=nextp1)
                lastcommitted = self.repo[nodestr]
                nextp1 = lastcommitted
                self.replacemap[ctx.node()] = lastcommitted.node()
                if memworkingcopy:
                    if willbecomenoop:
                        msg = _(b'%d file(s) changed, became empty as %s')
                    else:
                        msg = _(b'%d file(s) changed, became %s')
                    msg = msg % (
                        len(memworkingcopy),
                        self._ctx2str(lastcommitted),
                    )
                else:
                    msg = _(b'became %s') % self._ctx2str(lastcommitted)
            if self.ui.verbose and msg:
                self.ui.write(_(b'%s: %s\n') % (self._ctx2str(ctx), msg))
        self.finalnode = lastcommitted and lastcommitted.node()
###END###
def _ctx2str(self, ctx):
        if self.ui.debugflag:
            return b'%d:%s' % (ctx.rev(), ctx.hex())
        else:
            return b'%d:%s' % (ctx.rev(), short(ctx.node()))
###END###
def _getnewfilecontents(self, ctx):
        """(ctx) -> {path: str}

        fetch file contents from filefixupstates.
        return the working copy overrides - files different from ctx.
        """
        result = {}
        for path in self.paths:
            ctx2fctx = self.fctxmap[path]  # {ctx: fctx}
            if ctx not in ctx2fctx:
                continue
            fctx = ctx2fctx[ctx]
            content = fctx.data()
            newcontent = self.fixupmap[path].getfinalcontent(fctx)
            if content != newcontent:
                result[fctx.path()] = newcontent
        return result
###END###
def _movebookmarks(self, tr):
        repo = self.repo
        needupdate = [
            (name, self.replacemap[hsh])
            for name, hsh in pycompat.iteritems(repo._bookmarks)
            if hsh in self.replacemap
        ]
        changes = []
        for name, hsh in needupdate:
            if hsh:
                changes.append((name, hsh))
                if self.ui.verbose:
                    self.ui.write(
                        _(b'moving bookmark %s to %s\n') % (name, hex(hsh))
                    )
            else:
                changes.append((name, None))
                if self.ui.verbose:
                    self.ui.write(_(b'deleting bookmark %s\n') % name)
        repo._bookmarks.applychanges(repo, tr, changes)
###END###
def _moveworkingdirectoryparent(self):
        if not self.finalnode:
            # Find the latest not-{obsoleted,stripped} parent.
            revs = self.repo.revs(b'max(::. - %ln)', self.replacemap.keys())
            ctx = self.repo[revs.first()]
            self.finalnode = ctx.node()
        else:
            ctx = self.repo[self.finalnode]

        dirstate = self.repo.dirstate
        # dirstate.rebuild invalidates fsmonitorstate, causing "hg status" to
        # be slow. in absorb's case, no need to invalidate fsmonitorstate.
        noop = lambda: 0
        restore = noop
        if util.safehasattr(dirstate, '_fsmonitorstate'):
            bak = dirstate._fsmonitorstate.invalidate

            def restore():
                dirstate._fsmonitorstate.invalidate = bak

            dirstate._fsmonitorstate.invalidate = noop
        try:
            with dirstate.parentchange():
                dirstate.rebuild(ctx.node(), ctx.manifest(), self.paths)
        finally:
            restore()
###END###
def _willbecomenoop(memworkingcopy, ctx, pctx=None):
        """({path: content}, ctx, ctx) -> bool. test if a commit will be noop

        if it will become an empty commit (does not change anything, after the
        memworkingcopy overrides), return True. otherwise return False.
        """
        if not pctx:
            parents = ctx.parents()
            if len(parents) != 1:
                return False
            pctx = parents[0]
        if ctx.branch() != pctx.branch():
            return False
        if ctx.extra().get(b'close'):
            return False
        # ctx changes more files (not a subset of memworkingcopy)
        if not set(ctx.files()).issubset(set(memworkingcopy)):
            return False
        for path, content in pycompat.iteritems(memworkingcopy):
            if path not in pctx or path not in ctx:
                return False
            fctx = ctx[path]
            pfctx = pctx[path]
            if pfctx.flags() != fctx.flags():
                return False
            if pfctx.data() != content:
                return False
        return True
###END###
def _commitsingle(self, memworkingcopy, ctx, p1=None):
        """(ctx, {path: content}, node) -> node. make a single commit

        the commit is a clone from ctx, with a (optionally) different p1, and
        different file contents replaced by memworkingcopy.
        """
        parents = p1 and (p1, self.repo.nullid)
        extra = ctx.extra()
        if self._useobsolete and self.ui.configbool(b'absorb', b'add-noise'):
            extra[b'absorb_source'] = ctx.hex()

        desc = rewriteutil.update_hash_refs(
            ctx.repo(),
            ctx.description(),
            {
                oldnode: [newnode]
                for oldnode, newnode in self.replacemap.items()
            },
        )
        mctx = overlaycontext(
            memworkingcopy, ctx, parents, extra=extra, desc=desc
        )
        return mctx.commit()
###END###
def _useobsolete(self):
        """() -> bool"""
        return obsolete.isenabled(self.repo, obsolete.createmarkersopt)
###END###
def _cleanupoldcommits(self):
        replacements = {
            k: ([v] if v is not None else [])
            for k, v in pycompat.iteritems(self.replacemap)
        }
        if replacements:
            scmutil.cleanupnodes(
                self.repo, replacements, operation=b'absorb', fixphase=True
            )
###END###
def skip_empty_successor(self):
        return rewriteutil.skip_empty_successor(self.ui, b'absorb')
###END###
def __init__(self, repo, ui, inmemory=False, dryrun=False, opts=None):
        if opts is None:
            opts = {}

        # prepared: whether we have rebasestate prepared or not. Currently it
        # decides whether "self.repo" is unfiltered or not.
        # The rebasestate has explicit hash to hash instructions not depending
        # on visibility. If rebasestate exists (in-memory or on-disk), use
        # unfiltered repo to avoid visibility issues.
        # Before knowing rebasestate (i.e. when starting a new rebase (not
        # --continue or --abort)), the original repo should be used so
        # visibility-dependent revsets are correct.
        self.prepared = False
        self.resume = False
        self._repo = repo

        self.ui = ui
        self.opts = opts
        self.originalwd = None
        self.external = nullrev
        # Mapping between the old revision id and either what is the new rebased
        # revision or what needs to be done with the old revision. The state
        # dict will be what contains most of the rebase progress state.
        self.state = {}
        self.activebookmark = None
        self.destmap = {}
        self.skipped = set()

        self.collapsef = opts.get('collapse', False)
        self.collapsemsg = cmdutil.logmessage(ui, pycompat.byteskwargs(opts))
        self.date = opts.get('date', None)

        e = opts.get('extrafn')  # internal, used by e.g. hgsubversion
        self.extrafns = [_savegraft]
        if e:
            self.extrafns = [e]

        self.backupf = ui.configbool(b'rewrite', b'backup-bundle')
        self.keepf = opts.get('keep', False)
        self.keepbranchesf = opts.get('keepbranches', False)
        self.skipemptysuccessorf = rewriteutil.skip_empty_successor(
            repo.ui, b'rebase'
        )
        self.obsolete_with_successor_in_destination = {}
        self.obsolete_with_successor_in_rebase_set = set()
        self.inmemory = inmemory
        self.dryrun = dryrun
        self.stateobj = statemod.cmdstate(repo, b'rebasestate')
###END###
def repo(self):
        if self.prepared:
            return self._repo.unfiltered()
        else:
            return self._repo
###END###
def storestatus(self, tr=None):
        """Store the current status to allow recovery"""
        if tr:
            tr.addfilegenerator(
                b'rebasestate',
                (b'rebasestate',),
                self._writestatus,
                location=b'plain',
            )
        else:
            with self.repo.vfs(b"rebasestate", b"w") as f:
                self._writestatus(f)
###END###
def _writestatus(self, f):
        repo = self.repo
        assert repo.filtername is None
        f.write(repo[self.originalwd].hex() + b'\n')
        # was "dest". we now write dest per src root below.
        f.write(b'\n')
        f.write(repo[self.external].hex() + b'\n')
        f.write(b'%d\n' % int(self.collapsef))
        f.write(b'%d\n' % int(self.keepf))
        f.write(b'%d\n' % int(self.keepbranchesf))
        f.write(b'%s\n' % (self.activebookmark or b''))
        destmap = self.destmap
        for d, v in pycompat.iteritems(self.state):
            oldrev = repo[d].hex()
            if v >= 0:
                newrev = repo[v].hex()
            else:
                newrev = b"%d" % v
            destnode = repo[destmap[d]].hex()
            f.write(b"%s:%s:%s\n" % (oldrev, newrev, destnode))
        repo.ui.debug(b'rebase status stored\n')
###END###
def restorestatus(self):
        """Restore a previously stored status"""
        if not self.stateobj.exists():
            cmdutil.wrongtooltocontinue(self.repo, _(b'rebase'))

        data = self._read()
        self.repo.ui.debug(b'rebase status resumed\n')

        self.originalwd = data[b'originalwd']
        self.destmap = data[b'destmap']
        self.state = data[b'state']
        self.skipped = data[b'skipped']
        self.collapsef = data[b'collapse']
        self.keepf = data[b'keep']
        self.keepbranchesf = data[b'keepbranches']
        self.external = data[b'external']
        self.activebookmark = data[b'activebookmark']
###END###
def _read(self):
        self.prepared = True
        repo = self.repo
        assert repo.filtername is None
        data = {
            b'keepbranches': None,
            b'collapse': None,
            b'activebookmark': None,
            b'external': nullrev,
            b'keep': None,
            b'originalwd': None,
        }
        legacydest = None
        state = {}
        destmap = {}

        if True:
            f = repo.vfs(b"rebasestate")
            for i, l in enumerate(f.read().splitlines()):
                if i == 0:
                    data[b'originalwd'] = repo[l].rev()
                elif i == 1:
                    # this line should be empty in newer version. but legacy
                    # clients may still use it
                    if l:
                        legacydest = repo[l].rev()
                elif i == 2:
                    data[b'external'] = repo[l].rev()
                elif i == 3:
                    data[b'collapse'] = bool(int(l))
                elif i == 4:
                    data[b'keep'] = bool(int(l))
                elif i == 5:
                    data[b'keepbranches'] = bool(int(l))
                elif i == 6 and not (len(l) == 81 and b':' in l):
                    # line 6 is a recent addition, so for backwards
                    # compatibility check that the line doesn't look like the
                    # oldrev:newrev lines
                    data[b'activebookmark'] = l
                else:
                    args = l.split(b':')
                    oldrev = repo[args[0]].rev()
                    newrev = args[1]
                    if newrev in legacystates:
                        continue
                    if len(args) > 2:
                        destrev = repo[args[2]].rev()
                    else:
                        destrev = legacydest
                    destmap[oldrev] = destrev
                    if newrev == revtodostr:
                        state[oldrev] = revtodo
                        # Legacy compat special case
                    else:
                        state[oldrev] = repo[newrev].rev()

        if data[b'keepbranches'] is None:
            raise error.Abort(_(b'.hg/rebasestate is incomplete'))

        data[b'destmap'] = destmap
        data[b'state'] = state
        skipped = set()
        # recompute the set of skipped revs
        if not data[b'collapse']:
            seen = set(destmap.values())
            for old, new in sorted(state.items()):
                if new != revtodo and new in seen:
                    skipped.add(old)
                seen.add(new)
        data[b'skipped'] = skipped
        repo.ui.debug(
            b'computed skipped revs: %s\n'
            % (b' '.join(b'%d' % r for r in sorted(skipped)) or b'')
        )

        return data
###END###
def _handleskippingobsolete(self):
        """Compute structures necessary for skipping obsolete revisions"""
        if self.keepf:
            return
        if not self.ui.configbool(b'experimental', b'rebaseskipobsolete'):
            return
        obsoleteset = {r for r in self.state if self.repo[r].obsolete()}
        (
            self.obsolete_with_successor_in_destination,
            self.obsolete_with_successor_in_rebase_set,
        ) = _compute_obsolete_sets(self.repo, obsoleteset, self.destmap)
        skippedset = set(self.obsolete_with_successor_in_destination)
        skippedset.update(self.obsolete_with_successor_in_rebase_set)
        _checkobsrebase(self.repo, self.ui, obsoleteset, skippedset)
        if obsolete.isenabled(self.repo, obsolete.allowdivergenceopt):
            self.obsolete_with_successor_in_rebase_set = set()
        else:
            for rev in self.repo.revs(
                b'descendants(%ld) and not %ld',
                self.obsolete_with_successor_in_rebase_set,
                self.obsolete_with_successor_in_rebase_set,
            ):
                self.state.pop(rev, None)
                self.destmap.pop(rev, None)
###END###
def _prepareabortorcontinue(
        self, isabort, backup=True, suppwarns=False, dryrun=False, confirm=False
    ):
        self.resume = True
        try:
            self.restorestatus()
            # Calculate self.obsolete_* sets
            self._handleskippingobsolete()
            self.collapsemsg = restorecollapsemsg(self.repo, isabort)
        except error.RepoLookupError:
            if isabort:
                clearstatus(self.repo)
                clearcollapsemsg(self.repo)
                self.repo.ui.warn(
                    _(
                        b'rebase aborted (no revision is removed,'
                        b' only broken state is cleared)\n'
                    )
                )
                return 0
            else:
                msg = _(b'cannot continue inconsistent rebase')
                hint = _(b'use "hg rebase --abort" to clear broken state')
                raise error.Abort(msg, hint=hint)

        if isabort:
            backup = backup and self.backupf
            return self._abort(
                backup=backup,
                suppwarns=suppwarns,
                dryrun=dryrun,
                confirm=confirm,
            )
###END###
def _preparenewrebase(self, destmap):
        if not destmap:
            return _nothingtorebase()

        result = buildstate(self.repo, destmap, self.collapsef)

        if not result:
            # Empty state built, nothing to rebase
            self.ui.status(_(b'nothing to rebase\n'))
            return _nothingtorebase()

        (self.originalwd, self.destmap, self.state) = result
        if self.collapsef:
            dests = set(self.destmap.values())
            if len(dests) != 1:
                raise error.InputError(
                    _(b'--collapse does not work with multiple destinations')
                )
            destrev = next(iter(dests))
            destancestors = self.repo.changelog.ancestors(
                [destrev], inclusive=True
            )
            self.external = externalparent(self.repo, self.state, destancestors)

        for destrev in sorted(set(destmap.values())):
            dest = self.repo[destrev]
            if dest.closesbranch() and not self.keepbranchesf:
                self.ui.status(_(b'reopening closed branch head %s\n') % dest)

        # Calculate self.obsolete_* sets
        self._handleskippingobsolete()

        if not self.keepf:
            rebaseset = set(destmap.keys())
            rebaseset -= set(self.obsolete_with_successor_in_destination)
            rebaseset -= self.obsolete_with_successor_in_rebase_set
            # We have our own divergence-checking in the rebase extension
            overrides = {}
            if obsolete.isenabled(self.repo, obsolete.createmarkersopt):
                overrides = {
                    (b'experimental', b'evolution.allowdivergence'): b'true'
                }
            try:
                with self.ui.configoverride(overrides):
                    rewriteutil.precheck(self.repo, rebaseset, action=b'rebase')
            except error.Abort as e:
                if e.hint is None:
                    e.hint = _(b'use --keep to keep original changesets')
                raise e

        self.prepared = True
###END###
def _assignworkingcopy(self):
        if self.inmemory:
            from mercurial.context import overlayworkingctx

            self.wctx = overlayworkingctx(self.repo)
            self.repo.ui.debug(b"rebasing in memory\n")
        else:
            self.wctx = self.repo[None]
            self.repo.ui.debug(b"rebasing on disk\n")
        self.repo.ui.log(
            b"rebase",
            b"using in-memory rebase: %r\n",
            self.inmemory,
            rebase_imm_used=self.inmemory,
        )
###END###
def _performrebase(self, tr):
        self._assignworkingcopy()
        repo, ui = self.repo, self.ui
        if self.keepbranchesf:
            # insert _savebranch at the start of extrafns so if
            # there's a user-provided extrafn it can clobber branch if
            # desired
            self.extrafns.insert(0, _savebranch)
            if self.collapsef:
                branches = set()
                for rev in self.state:
                    branches.add(repo[rev].branch())
                    if len(branches) > 1:
                        raise error.InputError(
                            _(b'cannot collapse multiple named branches')
                        )

        # Keep track of the active bookmarks in order to reset them later
        self.activebookmark = self.activebookmark or repo._activebookmark
        if self.activebookmark:
            bookmarks.deactivate(repo)

        # Store the state before we begin so users can run 'hg rebase --abort'
        # if we fail before the transaction closes.
        self.storestatus()
        if tr:
            # When using single transaction, store state when transaction
            # commits.
            self.storestatus(tr)

        cands = [k for k, v in pycompat.iteritems(self.state) if v == revtodo]
        p = repo.ui.makeprogress(
            _(b"rebasing"), unit=_(b'changesets'), total=len(cands)
        )

        def progress(ctx):
            p.increment(item=(b"%d:%s" % (ctx.rev(), ctx)))

        for subset in sortsource(self.destmap):
            sortedrevs = self.repo.revs(b'sort(%ld, -topo)', subset)
            for rev in sortedrevs:
                self._rebasenode(tr, rev, progress)
        p.complete()
        ui.note(_(b'rebase merging completed\n'))
###END###
def _concludenode(self, rev, editor, commitmsg=None):
        """Commit the wd changes with parents p1 and p2.

        Reuse commit info from rev but also store useful information in extra.
        Return node of committed revision."""
        repo = self.repo
        ctx = repo[rev]
        if commitmsg is None:
            commitmsg = ctx.description()

        # Skip replacement if collapsing, as that degenerates to p1 for all
        # nodes.
        if not self.collapsef:
            cl = repo.changelog
            commitmsg = rewriteutil.update_hash_refs(
                repo,
                commitmsg,
                {
                    cl.node(oldrev): [cl.node(newrev)]
                    for oldrev, newrev in self.state.items()
                    if newrev != revtodo
                },
            )

        date = self.date
        if date is None:
            date = ctx.date()
        extra = {b'rebase_source': ctx.hex()}
        for c in self.extrafns:
            c(ctx, extra)
        destphase = max(ctx.phase(), phases.draft)
        overrides = {
            (b'phases', b'new-commit'): destphase,
            (b'ui', b'allowemptycommit'): not self.skipemptysuccessorf,
        }
        with repo.ui.configoverride(overrides, b'rebase'):
            if self.inmemory:
                newnode = commitmemorynode(
                    repo,
                    wctx=self.wctx,
                    extra=extra,
                    commitmsg=commitmsg,
                    editor=editor,
                    user=ctx.user(),
                    date=date,
                )
            else:
                newnode = commitnode(
                    repo,
                    extra=extra,
                    commitmsg=commitmsg,
                    editor=editor,
                    user=ctx.user(),
                    date=date,
                )

            return newnode
###END###
def _rebasenode(self, tr, rev, progressfn):
        repo, ui, opts = self.repo, self.ui, self.opts
        ctx = repo[rev]
        desc = _ctxdesc(ctx)
        if self.state[rev] == rev:
            ui.status(_(b'already rebased %s\n') % desc)
        elif rev in self.obsolete_with_successor_in_rebase_set:
            msg = (
                _(
                    b'note: not rebasing %s and its descendants as '
                    b'this would cause divergence\n'
                )
                % desc
            )
            repo.ui.status(msg)
            self.skipped.add(rev)
        elif rev in self.obsolete_with_successor_in_destination:
            succ = self.obsolete_with_successor_in_destination[rev]
            if succ is None:
                msg = _(b'note: not rebasing %s, it has no successor\n') % desc
            else:
                succdesc = _ctxdesc(repo[succ])
                msg = _(
                    b'note: not rebasing %s, already in destination as %s\n'
                ) % (desc, succdesc)
            repo.ui.status(msg)
            # Make clearrebased aware state[rev] is not a true successor
            self.skipped.add(rev)
            # Record rev as moved to its desired destination in self.state.
            # This helps bookmark and working parent movement.
            dest = max(
                adjustdest(repo, rev, self.destmap, self.state, self.skipped)
            )
            self.state[rev] = dest
        elif self.state[rev] == revtodo:
            ui.status(_(b'rebasing %s\n') % desc)
            progressfn(ctx)
            p1, p2, base = defineparents(
                repo,
                rev,
                self.destmap,
                self.state,
                self.skipped,
                self.obsolete_with_successor_in_destination,
            )
            if self.resume and self.wctx.p1().rev() == p1:
                repo.ui.debug(b'resuming interrupted rebase\n')
                self.resume = False
            else:
                overrides = {(b'ui', b'forcemerge'): opts.get('tool', b'')}
                with ui.configoverride(overrides, b'rebase'):
                    try:
                        rebasenode(
                            repo,
                            rev,
                            p1,
                            p2,
                            base,
                            self.collapsef,
                            wctx=self.wctx,
                        )
                    except error.InMemoryMergeConflictsError:
                        if self.dryrun:
                            raise error.ConflictResolutionRequired(b'rebase')
                        if self.collapsef:
                            # TODO: Make the overlayworkingctx reflected
                            # in the working copy here instead of re-raising
                            # so the entire rebase operation is retried.
                            raise
                        ui.status(
                            _(
                                b"hit merge conflicts; rebasing that "
                                b"commit again in the working copy\n"
                            )
                        )
                        try:
                            cmdutil.bailifchanged(repo)
                        except error.Abort:
                            clearstatus(repo)
                            clearcollapsemsg(repo)
                            raise
                        self.inmemory = False
                        self._assignworkingcopy()
                        mergemod.update(repo[p1], wc=self.wctx)
                        rebasenode(
                            repo,
                            rev,
                            p1,
                            p2,
                            base,
                            self.collapsef,
                            wctx=self.wctx,
                        )
            if not self.collapsef:
                merging = p2 != nullrev
                editform = cmdutil.mergeeditform(merging, b'rebase')
                editor = cmdutil.getcommiteditor(editform=editform, **opts)
                # We need to set parents again here just in case we're continuing
                # a rebase started with an old hg version (before 9c9cfecd4600),
                # because those old versions would have left us with two dirstate
                # parents, and we don't want to create a merge commit here (unless
                # we're rebasing a merge commit).
                self.wctx.setparents(repo[p1].node(), repo[p2].node())
                newnode = self._concludenode(rev, editor)
            else:
                # Skip commit if we are collapsing
                newnode = None
            # Update the state
            if newnode is not None:
                self.state[rev] = repo[newnode].rev()
                ui.debug(b'rebased as %s\n' % short(newnode))
                if repo[newnode].isempty():
                    ui.warn(
                        _(
                            b'note: created empty successor for %s, its '
                            b'destination already has all its changes\n'
                        )
                        % desc
                    )
            else:
                if not self.collapsef:
                    ui.warn(
                        _(
                            b'note: not rebasing %s, its destination already '
                            b'has all its changes\n'
                        )
                        % desc
                    )
                    self.skipped.add(rev)
                self.state[rev] = p1
                ui.debug(b'next revision set to %d\n' % p1)
        else:
            ui.status(
                _(b'already rebased %s as %s\n') % (desc, repo[self.state[rev]])
            )
        if not tr:
            # When not using single transaction, store state after each
            # commit is completely done. On InterventionRequired, we thus
            # won't store the status. Instead, we'll hit the "len(parents) == 2"
            # case and realize that the commit was in progress.
            self.storestatus()
###END###
def _finishrebase(self):
        repo, ui, opts = self.repo, self.ui, self.opts
        fm = ui.formatter(b'rebase', pycompat.byteskwargs(opts))
        fm.startitem()
        if self.collapsef:
            p1, p2, _base = defineparents(
                repo,
                min(self.state),
                self.destmap,
                self.state,
                self.skipped,
                self.obsolete_with_successor_in_destination,
            )
            editopt = opts.get('edit')
            editform = b'rebase.collapse'
            if self.collapsemsg:
                commitmsg = self.collapsemsg
            else:
                commitmsg = b'Collapsed revision'
                for rebased in sorted(self.state):
                    if rebased not in self.skipped:
                        commitmsg += b'\n* %s' % repo[rebased].description()
                editopt = True
            editor = cmdutil.getcommiteditor(edit=editopt, editform=editform)
            revtoreuse = max(self.state)

            self.wctx.setparents(repo[p1].node(), repo[self.external].node())
            newnode = self._concludenode(
                revtoreuse, editor, commitmsg=commitmsg
            )

            if newnode is not None:
                newrev = repo[newnode].rev()
                for oldrev in self.state:
                    self.state[oldrev] = newrev

        if b'qtip' in repo.tags():
            updatemq(repo, self.state, self.skipped, **opts)

        # restore original working directory
        # (we do this before stripping)
        newwd = self.state.get(self.originalwd, self.originalwd)
        if newwd < 0:
            # original directory is a parent of rebase set root or ignored
            newwd = self.originalwd
        if newwd not in [c.rev() for c in repo[None].parents()]:
            ui.note(_(b"update back to initial working directory parent\n"))
            mergemod.update(repo[newwd])

        collapsedas = None
        if self.collapsef and not self.keepf:
            collapsedas = newnode
        clearrebased(
            ui,
            repo,
            self.destmap,
            self.state,
            self.skipped,
            collapsedas,
            self.keepf,
            fm=fm,
            backup=self.backupf,
        )

        clearstatus(repo)
        clearcollapsemsg(repo)

        ui.note(_(b"rebase completed\n"))
        util.unlinkpath(repo.sjoin(b'undo'), ignoremissing=True)
        if self.skipped:
            skippedlen = len(self.skipped)
            ui.note(_(b"%d revisions have been skipped\n") % skippedlen)
        fm.end()

        if (
            self.activebookmark
            and self.activebookmark in repo._bookmarks
            and repo[b'.'].node() == repo._bookmarks[self.activebookmark]
        ):
            bookmarks.activate(repo, self.activebookmark)
###END###
def _abort(self, backup=True, suppwarns=False, dryrun=False, confirm=False):
        '''Restore the repository to its original state.'''

        repo = self.repo
        try:
            # If the first commits in the rebased set get skipped during the
            # rebase, their values within the state mapping will be the dest
            # rev id. The rebased list must must not contain the dest rev
            # (issue4896)
            rebased = [
                s
                for r, s in self.state.items()
                if s >= 0 and s != r and s != self.destmap[r]
            ]
            immutable = [d for d in rebased if not repo[d].mutable()]
            cleanup = True
            if immutable:
                repo.ui.warn(
                    _(b"warning: can't clean up public changesets %s\n")
                    % b', '.join(bytes(repo[r]) for r in immutable),
                    hint=_(b"see 'hg help phases' for details"),
                )
                cleanup = False

            descendants = set()
            if rebased:
                descendants = set(repo.changelog.descendants(rebased))
            if descendants - set(rebased):
                repo.ui.warn(
                    _(
                        b"warning: new changesets detected on "
                        b"destination branch, can't strip\n"
                    )
                )
                cleanup = False

            if cleanup:
                if rebased:
                    strippoints = [
                        c.node() for c in repo.set(b'roots(%ld)', rebased)
                    ]

                updateifonnodes = set(rebased)
                updateifonnodes.update(self.destmap.values())

                if not dryrun and not confirm:
                    updateifonnodes.add(self.originalwd)

                shouldupdate = repo[b'.'].rev() in updateifonnodes

                # Update away from the rebase if necessary
                if shouldupdate:
                    mergemod.clean_update(repo[self.originalwd])

                # Strip from the first rebased revision
                if rebased:
                    repair.strip(repo.ui, repo, strippoints, backup=backup)

            if self.activebookmark and self.activebookmark in repo._bookmarks:
                bookmarks.activate(repo, self.activebookmark)

        finally:
            clearstatus(repo)
            clearcollapsemsg(repo)
            if not suppwarns:
                repo.ui.warn(_(b'rebase aborted\n'))
        return 0
###END###
def __init__(self, **opts):
        opts = pycompat.byteskwargs(opts)
        for k, v in pycompat.iteritems(self.defaults):
            setattr(self, k, opts.get(k, v))
###END###
def shortstr(self):
        """represent opts in a short string, suitable for a directory name"""
        result = b''
        if not self.followrename:
            result += b'r0'
        if not self.followmerge:
            result += b'm0'
        if self.diffopts is not None:
            assert isinstance(self.diffopts, mdiff.diffopts)
            diffopthash = hashdiffopts(self.diffopts)
            if diffopthash != _defaultdiffopthash:
                result += b'i' + diffopthash
        return result or b'default'
###END###
def __init__(self, repo, path, linelogpath, revmappath, opts):
        self.repo = repo
        self.ui = repo.ui
        self.path = path
        self.opts = opts
        self.linelogpath = linelogpath
        self.revmappath = revmappath
        self._linelog = None
        self._revmap = None
        self._node2path = {}
###END###
def linelog(self):
        if self._linelog is None:
            if os.path.exists(self.linelogpath):
                with open(self.linelogpath, b'rb') as f:
                    try:
                        self._linelog = linelogmod.linelog.fromdata(f.read())
                    except linelogmod.LineLogError:
                        self._linelog = linelogmod.linelog()
            else:
                self._linelog = linelogmod.linelog()
        return self._linelog
###END###
def revmap(self):
        if self._revmap is None:
            self._revmap = revmapmod.revmap(self.revmappath)
        return self._revmap
###END###
def close(self):
        if self._revmap is not None:
            self._revmap.flush()
            self._revmap = None
        if self._linelog is not None:
            with open(self.linelogpath, b'wb') as f:
                f.write(self._linelog.encode())
            self._linelog = None
###END###
def rebuild(self):
        """delete linelog and revmap, useful for rebuilding"""
        self.close()
        self._node2path.clear()
        _unlinkpaths([self.revmappath, self.linelogpath])
###END###
def lastnode(self):
        """return last node in revmap, or None if revmap is empty"""
        if self._revmap is None:
            # fast path, read revmap without loading its full content
            return revmapmod.getlastnode(self.revmappath)
        else:
            return self._revmap.rev2hsh(self._revmap.maxrev)
###END###
def isuptodate(self, master, strict=True):
        """return True if the revmap / linelog is up-to-date, or the file
        does not exist in the master revision. False otherwise.

        it tries to be fast and could return false negatives, because of the
        use of linkrev instead of introrev.

        useful for both server and client to decide whether to update
        fastannotate cache or not.

        if strict is True, even if fctx exists in the revmap, but is not the
        last node, isuptodate will return False. it's good for performance - no
        expensive check was done.

        if strict is False, if fctx exists in the revmap, this function may
        return True. this is useful for the client to skip downloading the
        cache if the client's master is behind the server's.
        """
        lastnode = self.lastnode
        try:
            f = self._resolvefctx(master, resolverev=True)
            # choose linkrev instead of introrev as the check is meant to be
            # *fast*.
            linknode = self.repo.changelog.node(f.linkrev())
            if not strict and lastnode and linknode != lastnode:
                # check if f.node() is in the revmap. note: this loads the
                # revmap and can be slow.
                return self.revmap.hsh2rev(linknode) is not None
            # avoid resolving old manifest, or slow adjustlinkrev to be fast,
            # false negatives are acceptable in this case.
            return linknode == lastnode
        except LookupError:
            # master does not have the file, or the revmap is ahead
            return True
###END###
def annotate(self, rev, master=None, showpath=False, showlines=False):
        """incrementally update the cache so it includes revisions in the main
        branch till 'master'. and run annotate on 'rev', which may or may not be
        included in the main branch.

        if master is None, do not update linelog.

        the first value returned is the annotate result, it is [(node, linenum)]
        by default. [(node, linenum, path)] if showpath is True.

        if showlines is True, a second value will be returned, it is a list of
        corresponding line contents.
        """

        # the fast path test requires commit hash, convert rev number to hash,
        # so it may hit the fast path. note: in the "fctx" mode, the "annotate"
        # command could give us a revision number even if the user passes a
        # commit hash.
        if isinstance(rev, int):
            rev = hex(self.repo.changelog.node(rev))

        # fast path: if rev is in the main branch already
        directly, revfctx = self.canannotatedirectly(rev)
        if directly:
            if self.ui.debugflag:
                self.ui.debug(
                    b'fastannotate: %s: using fast path '
                    b'(resolved fctx: %s)\n'
                    % (
                        self.path,
                        stringutil.pprint(util.safehasattr(revfctx, b'node')),
                    )
                )
            return self.annotatedirectly(revfctx, showpath, showlines)

        # resolve master
        masterfctx = None
        if master:
            try:
                masterfctx = self._resolvefctx(
                    master, resolverev=True, adjustctx=True
                )
            except LookupError:  # master does not have the file
                pass
            else:
                if masterfctx in self.revmap:  # no need to update linelog
                    masterfctx = None

        #                  ... - @ <- rev (can be an arbitrary changeset,
        #                 /                not necessarily a descendant
        #      master -> o                 of master)
        #                |
        #     a merge -> o         'o': new changesets in the main branch
        #                |\        '#': revisions in the main branch that
        #                o *            exist in linelog / revmap
        #                | .       '*': changesets in side branches, or
        # last master -> # .            descendants of master
        #                | .
        #                # *       joint: '#', and is a parent of a '*'
        #                |/
        #     a joint -> # ^^^^ --- side branches
        #                |
        #                ^ --- main branch (in linelog)

        # these DFSes are similar to the traditional annotate algorithm.
        # we cannot really reuse the code for perf reason.

        # 1st DFS calculates merges, joint points, and needed.
        # "needed" is a simple reference counting dict to free items in
        # "hist", reducing its memory usage otherwise could be huge.
        initvisit = [revfctx]
        if masterfctx:
            if masterfctx.rev() is None:
                raise error.Abort(
                    _(b'cannot update linelog to wdir()'),
                    hint=_(b'set fastannotate.mainbranch'),
                )
            initvisit.append(masterfctx)
        visit = initvisit[:]
        pcache = {}
        needed = {revfctx: 1}
        hist = {}  # {fctx: ([(llrev or fctx, linenum)], text)}
        while visit:
            f = visit.pop()
            if f in pcache or f in hist:
                continue
            if f in self.revmap:  # in the old main branch, it's a joint
                llrev = self.revmap.hsh2rev(f.node())
                self.linelog.annotate(llrev)
                result = self.linelog.annotateresult
                hist[f] = (result, f.data())
                continue
            pl = self._parentfunc(f)
            pcache[f] = pl
            for p in pl:
                needed[p] = needed.get(p, 0) + 1
                if p not in pcache:
                    visit.append(p)

        # 2nd (simple) DFS calculates new changesets in the main branch
        # ('o' nodes in # the above graph), so we know when to update linelog.
        newmainbranch = set()
        f = masterfctx
        while f and f not in self.revmap:
            newmainbranch.add(f)
            pl = pcache[f]
            if pl:
                f = pl[0]
            else:
                f = None
                break

        # f, if present, is the position where the last build stopped at, and
        # should be the "master" last time. check to see if we can continue
        # building the linelog incrementally. (we cannot if diverged)
        if masterfctx is not None:
            self._checklastmasterhead(f)

        if self.ui.debugflag:
            if newmainbranch:
                self.ui.debug(
                    b'fastannotate: %s: %d new changesets in the main'
                    b' branch\n' % (self.path, len(newmainbranch))
                )
            elif not hist:  # no joints, no updates
                self.ui.debug(
                    b'fastannotate: %s: linelog cannot help in '
                    b'annotating this revision\n' % self.path
                )

        # prepare annotateresult so we can update linelog incrementally
        self.linelog.annotate(self.linelog.maxrev)

        # 3rd DFS does the actual annotate
        visit = initvisit[:]
        progress = self.ui.makeprogress(
            b'building cache', total=len(newmainbranch)
        )
        while visit:
            f = visit[-1]
            if f in hist:
                visit.pop()
                continue

            ready = True
            pl = pcache[f]
            for p in pl:
                if p not in hist:
                    ready = False
                    visit.append(p)
            if not ready:
                continue

            visit.pop()
            blocks = None  # mdiff blocks, used for appending linelog
            ismainbranch = f in newmainbranch
            # curr is the same as the traditional annotate algorithm,
            # if we only care about linear history (do not follow merge),
            # then curr is not actually used.
            assert f not in hist
            curr = _decorate(f)
            for i, p in enumerate(pl):
                bs = list(self._diffblocks(hist[p][1], curr[1]))
                if i == 0 and ismainbranch:
                    blocks = bs
                curr = _pair(hist[p], curr, bs)
                if needed[p] == 1:
                    del hist[p]
                    del needed[p]
                else:
                    needed[p] -= 1

            hist[f] = curr
            del pcache[f]

            if ismainbranch:  # need to write to linelog
                progress.increment()
                bannotated = None
                if len(pl) == 2 and self.opts.followmerge:  # merge
                    bannotated = curr[0]
                if blocks is None:  # no parents, add an empty one
                    blocks = list(self._diffblocks(b'', curr[1]))
                self._appendrev(f, blocks, bannotated)
            elif showpath:  # not append linelog, but we need to record path
                self._node2path[f.node()] = f.path()

        progress.complete()

        result = [
            ((self.revmap.rev2hsh(fr) if isinstance(fr, int) else fr.node()), l)
            for fr, l in hist[revfctx][0]
        ]  # [(node, linenumber)]
        return self._refineannotateresult(result, revfctx, showpath, showlines)
###END###
def canannotatedirectly(self, rev):
        """(str) -> bool, fctx or node.
        return (True, f) if we can annotate without updating the linelog, pass
        f to annotatedirectly.
        return (False, f) if we need extra calculation. f is the fctx resolved
        from rev.
        """
        result = True
        f = None
        if not isinstance(rev, int) and rev is not None:
            hsh = {20: bytes, 40: bin}.get(len(rev), lambda x: None)(rev)
            if hsh is not None and (hsh, self.path) in self.revmap:
                f = hsh
        if f is None:
            adjustctx = b'linkrev' if self._perfhack else True
            f = self._resolvefctx(rev, adjustctx=adjustctx, resolverev=True)
            result = f in self.revmap
            if not result and self._perfhack:
                # redo the resolution without perfhack - as we are going to
                # do write operations, we need a correct fctx.
                f = self._resolvefctx(rev, adjustctx=True, resolverev=True)
        return result, f
###END###
def annotatealllines(self, rev, showpath=False, showlines=False):
        """(rev : str) -> [(node : str, linenum : int, path : str)]

        the result has the same format with annotate, but include all (including
        deleted) lines up to rev. call this after calling annotate(rev, ...) for
        better performance and accuracy.
        """
        revfctx = self._resolvefctx(rev, resolverev=True, adjustctx=True)

        # find a chain from rev to anything in the mainbranch
        if revfctx not in self.revmap:
            chain = [revfctx]
            a = b''
            while True:
                f = chain[-1]
                pl = self._parentfunc(f)
                if not pl:
                    break
                if pl[0] in self.revmap:
                    a = pl[0].data()
                    break
                chain.append(pl[0])

            # both self.linelog and self.revmap is backed by filesystem. now
            # we want to modify them but do not want to write changes back to
            # files. so we create in-memory objects and copy them. it's like
            # a "fork".
            linelog = linelogmod.linelog()
            linelog.copyfrom(self.linelog)
            linelog.annotate(linelog.maxrev)
            revmap = revmapmod.revmap()
            revmap.copyfrom(self.revmap)

            for f in reversed(chain):
                b = f.data()
                blocks = list(self._diffblocks(a, b))
                self._doappendrev(linelog, revmap, f, blocks)
                a = b
        else:
            # fastpath: use existing linelog, revmap as we don't write to them
            linelog = self.linelog
            revmap = self.revmap

        lines = linelog.getalllines()
        hsh = revfctx.node()
        llrev = revmap.hsh2rev(hsh)
        result = [(revmap.rev2hsh(r), l) for r, l in lines if r <= llrev]
        # cannot use _refineannotateresult since we need custom logic for
        # resolving line contents
        if showpath:
            result = self._addpathtoresult(result, revmap)
        if showlines:
            linecontents = self._resolvelines(result, revmap, linelog)
            result = (result, linecontents)
        return result
###END###
def _resolvelines(self, annotateresult, revmap, linelog):
        """(annotateresult) -> [line]. designed for annotatealllines.
        this is probably the most inefficient code in the whole fastannotate
        directory. but we have made a decision that the linelog does not
        store line contents. so getting them requires random accesses to
        the revlog data, since they can be many, it can be very slow.
        """
        # [llrev]
        revs = [revmap.hsh2rev(l[0]) for l in annotateresult]
        result = [None] * len(annotateresult)
        # {(rev, linenum): [lineindex]}
        key2idxs = collections.defaultdict(list)
        for i in pycompat.xrange(len(result)):
            key2idxs[(revs[i], annotateresult[i][1])].append(i)
        while key2idxs:
            # find an unresolved line and its linelog rev to annotate
            hsh = None
            try:
                for (rev, _linenum), idxs in pycompat.iteritems(key2idxs):
                    if revmap.rev2flag(rev) & revmapmod.sidebranchflag:
                        continue
                    hsh = annotateresult[idxs[0]][0]
                    break
            except StopIteration:  # no more unresolved lines
                return result
            if hsh is None:
                # the remaining key2idxs are not in main branch, resolving them
                # using the hard way...
                revlines = {}
                for (rev, linenum), idxs in pycompat.iteritems(key2idxs):
                    if rev not in revlines:
                        hsh = annotateresult[idxs[0]][0]
                        if self.ui.debugflag:
                            self.ui.debug(
                                b'fastannotate: reading %s line #%d '
                                b'to resolve lines %r\n'
                                % (short(hsh), linenum, idxs)
                            )
                        fctx = self._resolvefctx(hsh, revmap.rev2path(rev))
                        lines = mdiff.splitnewlines(fctx.data())
                        revlines[rev] = lines
                    for idx in idxs:
                        result[idx] = revlines[rev][linenum]
                assert all(x is not None for x in result)
                return result

            # run the annotate and the lines should match to the file content
            self.ui.debug(
                b'fastannotate: annotate %s to resolve lines\n' % short(hsh)
            )
            linelog.annotate(rev)
            fctx = self._resolvefctx(hsh, revmap.rev2path(rev))
            annotated = linelog.annotateresult
            lines = mdiff.splitnewlines(fctx.data())
            if len(lines) != len(annotated):
                raise faerror.CorruptedFileError(b'unexpected annotated lines')
            # resolve lines from the annotate result
            for i, line in enumerate(lines):
                k = annotated[i]
                if k in key2idxs:
                    for idx in key2idxs[k]:
                        result[idx] = line
                    del key2idxs[k]
        return result
###END###
def annotatedirectly(self, f, showpath, showlines):
        """like annotate, but when we know that f is in linelog.
        f can be either a 20-char str (node) or a fctx. this is for perf - in
        the best case, the user provides a node and we don't need to read the
        filelog or construct any filecontext.
        """
        if isinstance(f, bytes):
            hsh = f
        else:
            hsh = f.node()
        llrev = self.revmap.hsh2rev(hsh)
        if not llrev:
            raise faerror.CorruptedFileError(b'%s is not in revmap' % hex(hsh))
        if (self.revmap.rev2flag(llrev) & revmapmod.sidebranchflag) != 0:
            raise faerror.CorruptedFileError(
                b'%s is not in revmap mainbranch' % hex(hsh)
            )
        self.linelog.annotate(llrev)
        result = [
            (self.revmap.rev2hsh(r), l) for r, l in self.linelog.annotateresult
        ]
        return self._refineannotateresult(result, f, showpath, showlines)
###END###
def _refineannotateresult(self, result, f, showpath, showlines):
        """add the missing path or line contents, they can be expensive.
        f could be either node or fctx.
        """
        if showpath:
            result = self._addpathtoresult(result)
        if showlines:
            if isinstance(f, bytes):  # f: node or fctx
                llrev = self.revmap.hsh2rev(f)
                fctx = self._resolvefctx(f, self.revmap.rev2path(llrev))
            else:
                fctx = f
            lines = mdiff.splitnewlines(fctx.data())
            if len(lines) != len(result):  # linelog is probably corrupted
                raise faerror.CorruptedFileError()
            result = (result, lines)
        return result
###END###
def _appendrev(self, fctx, blocks, bannotated=None):
        self._doappendrev(self.linelog, self.revmap, fctx, blocks, bannotated)
###END###
def _diffblocks(self, a, b):
        return mdiff.allblocks(a, b, self.opts.diffopts)
###END###
def _doappendrev(linelog, revmap, fctx, blocks, bannotated=None):
        """append a revision to linelog and revmap"""

        def getllrev(f):
            """(fctx) -> int"""
            # f should not be a linelog revision
            if isinstance(f, int):
                raise error.ProgrammingError(b'f should not be an int')
            # f is a fctx, allocate linelog rev on demand
            hsh = f.node()
            rev = revmap.hsh2rev(hsh)
            if rev is None:
                rev = revmap.append(hsh, sidebranch=True, path=f.path())
            return rev

        # append sidebranch revisions to revmap
        siderevs = []
        siderevmap = {}  # node: int
        if bannotated is not None:
            for (a1, a2, b1, b2), op in blocks:
                if op != b'=':
                    # f could be either linelong rev, or fctx.
                    siderevs += [
                        f
                        for f, l in bannotated[b1:b2]
                        if not isinstance(f, int)
                    ]
        siderevs = set(siderevs)
        if fctx in siderevs:  # mainnode must be appended seperately
            siderevs.remove(fctx)
        for f in siderevs:
            siderevmap[f] = getllrev(f)

        # the changeset in the main branch, could be a merge
        llrev = revmap.append(fctx.node(), path=fctx.path())
        siderevmap[fctx] = llrev

        for (a1, a2, b1, b2), op in reversed(blocks):
            if op == b'=':
                continue
            if bannotated is None:
                linelog.replacelines(llrev, a1, a2, b1, b2)
            else:
                blines = [
                    ((r if isinstance(r, int) else siderevmap[r]), l)
                    for r, l in bannotated[b1:b2]
                ]
                linelog.replacelines_vec(llrev, a1, a2, blines)
###END###
def _addpathtoresult(self, annotateresult, revmap=None):
        """(revmap, [(node, linenum)]) -> [(node, linenum, path)]"""
        if revmap is None:
            revmap = self.revmap

        def _getpath(nodeid):
            path = self._node2path.get(nodeid)
            if path is None:
                path = revmap.rev2path(revmap.hsh2rev(nodeid))
                self._node2path[nodeid] = path
            return path

        return [(n, l, _getpath(n)) for n, l in annotateresult]
###END###
def _checklastmasterhead(self, fctx):
        """check if fctx is the master's head last time, raise if not"""
        if fctx is None:
            llrev = 0
        else:
            llrev = self.revmap.hsh2rev(fctx.node())
            if not llrev:
                raise faerror.CannotReuseError()
        if self.linelog.maxrev != llrev:
            raise faerror.CannotReuseError()
###END###
def _parentfunc(self):
        """-> (fctx) -> [fctx]"""
        followrename = self.opts.followrename
        followmerge = self.opts.followmerge

        def parents(f):
            pl = _parents(f, follow=followrename)
            if not followmerge:
                pl = pl[:1]
            return pl

        return parents
###END###
def _perfhack(self):
        return self.ui.configbool(b'fastannotate', b'perfhack')
###END###
def _resolvefctx(self, rev, path=None, **kwds):
        return resolvefctx(self.repo, rev, (path or self.path), **kwds)
###END###
def __init__(self, repo, path, opts=defaultopts):
        # different options use different directories
        self._vfspath = os.path.join(
            b'fastannotate', opts.shortstr, encodedir(path)
        )
        self._repo = repo
###END###
def dirname(self):
        return os.path.dirname(self._repo.vfs.join(self._vfspath))
###END###
def linelogpath(self):
        return self._repo.vfs.join(self._vfspath + b'.l')
###END###
def lock(self):
        return lockmod.lock(self._repo.vfs, self._vfspath + b'.lock')
###END###
def revmappath(self):
        return self._repo.vfs.join(self._vfspath + b'.m')
###END###
def __init__(self, ui, repo, opts):
        self.ui = ui
        self.opts = opts

        if ui.quiet:
            datefunc = dateutil.shortdate
        else:
            datefunc = dateutil.datestr
        datefunc = util.cachefunc(datefunc)
        getctx = util.cachefunc(lambda x: repo[x[0]])
        hexfunc = self._hexfunc

        # special handling working copy "changeset" and "rev" functions
        if self.opts.get(b'rev') == b'wdir()':
            orig = hexfunc
            hexfunc = lambda x: None if x is None else orig(x)
            wnode = hexfunc(repo[b'.'].node()) + b'+'
            wrev = b'%d' % repo[b'.'].rev()
            wrevpad = b''
            if not opts.get(b'changeset'):  # only show + if changeset is hidden
                wrev += b'+'
                wrevpad = b' '
            revenc = lambda x: wrev if x is None else (b'%d' % x) + wrevpad

            def csetenc(x):
                if x is None:
                    return wnode
                return pycompat.bytestr(x) + b' '

        else:
            revenc = csetenc = pycompat.bytestr

        # opt name, separator, raw value (for json/plain), encoder (for plain)
        opmap = [
            (b'user', b' ', lambda x: getctx(x).user(), ui.shortuser),
            (b'number', b' ', lambda x: getctx(x).rev(), revenc),
            (b'changeset', b' ', lambda x: hexfunc(x[0]), csetenc),
            (b'date', b' ', lambda x: getctx(x).date(), datefunc),
            (b'file', b' ', lambda x: x[2], pycompat.bytestr),
            (b'line_number', b':', lambda x: x[1] + 1, pycompat.bytestr),
        ]
        fieldnamemap = {b'number': b'rev', b'changeset': b'node'}
        funcmap = [
            (get, sep, fieldnamemap.get(op, op), enc)
            for op, sep, get, enc in opmap
            if opts.get(op)
        ]
        # no separator for first column
        funcmap[0] = list(funcmap[0])
        funcmap[0][1] = b''
        self.funcmap = funcmap
###END###
def write(self, annotatedresult, lines=None, existinglines=None):
        """(annotateresult, [str], set([rev, linenum])) -> None. write output.
        annotateresult can be [(node, linenum, path)], or [(node, linenum)]
        """
        pieces = []  # [[str]]
        maxwidths = []  # [int]

        # calculate padding
        for f, sep, name, enc in self.funcmap:
            l = [enc(f(x)) for x in annotatedresult]
            pieces.append(l)
            if name in [b'node', b'date']:  # node and date has fixed size
                l = l[:1]
            widths = pycompat.maplist(encoding.colwidth, set(l))
            maxwidth = max(widths) if widths else 0
            maxwidths.append(maxwidth)

        # buffered output
        result = b''
        for i in pycompat.xrange(len(annotatedresult)):
            for j, p in enumerate(pieces):
                sep = self.funcmap[j][1]
                padding = b' ' * (maxwidths[j] - len(p[i]))
                result += sep + padding + p[i]
            if lines:
                if existinglines is None:
                    result += b': ' + lines[i]
                else:  # extra formatting showing whether a line exists
                    key = (annotatedresult[i][0], annotatedresult[i][1])
                    if key in existinglines:
                        result += b':  ' + lines[i]
                    else:
                        result += b': ' + self.ui.label(
                            b'-' + lines[i], b'diff.deleted'
                        )

            if result[-1:] != b'\n':
                result += b'\n'

        self.ui.write(result)
###END###
def _hexfunc(self):
        if self.ui.debugflag or self.opts.get(b'long_hash'):
            return hex
        else:
            return short
###END###
def end(self):
        pass
###END###
def __init__(self, ui, repo, opts):
        super(jsonformatter, self).__init__(ui, repo, opts)
        self.ui.write(b'[')
        self.needcomma = False
###END###
def write(self, annotatedresult, lines=None, existinglines=None):
        if annotatedresult:
            self._writecomma()

        pieces = [
            (name, pycompat.maplist(f, annotatedresult))
            for f, sep, name, enc in self.funcmap
        ]
        if lines is not None:
            pieces.append((b'line', lines))
        pieces.sort()

        seps = [b','] * len(pieces[:-1]) + [b'']

        result = b''
        lasti = len(annotatedresult) - 1
        for i in pycompat.xrange(len(annotatedresult)):
            result += b'\n {\n'
            for j, p in enumerate(pieces):
                k, vs = p
                result += b'  "%s": %s%s\n' % (
                    k,
                    templatefilters.json(vs[i], paranoid=False),
                    seps[j],
                )
            result += b' }%s' % (b'' if i == lasti else b',')
        if lasti >= 0:
            self.needcomma = True

        self.ui.write(result)
###END###
def _writecomma(self):
        if self.needcomma:
            self.ui.write(b',')
            self.needcomma = False
###END###
def _hexfunc(self):
        return hex
###END###
def end(self):
        self.ui.write(b'\n]\n')
###END###
def getannotate(self, path, lastnode=None):
            if not self.capable(b'getannotate'):
                ui.warn(_(b'remote peer cannot provide annotate cache\n'))
                yield None, None
            else:
                args = {b'path': path, b'lastnode': lastnode or b''}
                f = wireprotov1peer.future()
                yield args, f
                yield _parseresponse(f.value)
###END###
def prefetchfastannotate(self, paths, peer=None):
            master = _getmaster(self.ui)
            needupdatepaths = []
            lastnodemap = {}
            try:
                for path in _filterfetchpaths(self, paths):
                    with context.annotatecontext(self, path) as actx:
                        if not actx.isuptodate(master, strict=False):
                            needupdatepaths.append(path)
                            lastnodemap[path] = actx.lastnode
                if needupdatepaths:
                    clientfetch(self, needupdatepaths, lastnodemap, peer)
            except Exception as ex:
                # could be directory not writable or so, not fatal
                self.ui.debug(b'fastannotate: prefetch failed: %r\n' % ex)
###END###
def __init__(self, ui, repotype, path, revs=None):
        super(bzr_source, self).__init__(ui, repotype, path, revs=revs)

        if not os.path.exists(os.path.join(path, b'.bzr')):
            raise common.NoRepo(
                _(b'%s does not look like a Bazaar repository') % path
            )

        try:
            # access breezy stuff
            bzrdir
        except NameError:
            raise common.NoRepo(_(b'Bazaar modules could not be loaded'))

        path = util.abspath(path)
        self._checkrepotype(path)
        try:
            bzr_dir = bzrdir.BzrDir.open(path.decode())
            self.sourcerepo = bzr_dir.open_repository()
        except errors.NoRepositoryPresent:
            raise common.NoRepo(
                _(b'%s does not look like a Bazaar repository') % path
            )
        self._parentids = {}
        self._saverev = ui.configbool(b'convert', b'bzr.saverev')
###END###
def _checkrepotype(self, path):
        # Lightweight checkouts detection is informational but probably
        # fragile at API level. It should not terminate the conversion.
        try:
            dir = bzrdir.BzrDir.open_containing(path.decode())[0]
            try:
                tree = dir.open_workingtree(recommend_upgrade=False)
                branch = tree.branch
            except (errors.NoWorkingTree, errors.NotLocalUrl):
                tree = None
                branch = dir.open_branch()
            if (
                tree is not None
                and tree.controldir.root_transport.base
                != branch.controldir.root_transport.base
            ):
                self.ui.warn(
                    _(
                        b'warning: lightweight checkouts may cause '
                        b'conversion failures, try with a regular '
                        b'branch instead.\n'
                    )
                )
        except Exception:
            self.ui.note(_(b'bzr source type could not be determined\n'))
###END###
def before(self):
        """Before the conversion begins, acquire a read lock
        for all the operations that might need it. Fortunately
        read locks don't block other reads or writes to the
        repository, so this shouldn't have any impact on the usage of
        the source repository.

        The alternative would be locking on every operation that
        needs locks (there are currently two: getting the file and
        getting the parent map) and releasing immediately after,
        but this approach can take even 40% longer."""
        self.sourcerepo.lock_read()
###END###
def after(self):
        self.sourcerepo.unlock()
###END###
def _bzrbranches(self):
        return self.sourcerepo.find_branches(using=True)
###END###
def getheads(self):
        if not self.revs:
            # Set using=True to avoid nested repositories (see issue3254)
            heads = sorted([b.last_revision() for b in self._bzrbranches()])
        else:
            revid = None
            for branch in self._bzrbranches():
                try:
                    revspec = self.revs[0].decode()
                    r = revisionspec.RevisionSpec.from_string(revspec)
                    info = r.in_history(branch)
                except errors.BzrError:
                    pass
                revid = info.rev_id
            if revid is None:
                raise error.Abort(
                    _(b'%s is not a valid revision') % self.revs[0]
                )
            heads = [revid]
        # Empty repositories return 'null:', which cannot be retrieved
        heads = [h for h in heads if h != b'null:']
        return heads
###END###
def getfile(self, name, rev):
        name = name.decode()
        revtree = self.sourcerepo.revision_tree(rev)

        try:
            kind = revtree.kind(name)
        except breezy.errors.NoSuchFile:
            return None, None
        if kind not in supportedkinds:
            # the file is not available anymore - was deleted
            return None, None
        mode = self._modecache[(name.encode(), rev)]
        if kind == 'symlink':
            target = revtree.get_symlink_target(name)
            if target is None:
                raise error.Abort(
                    _(b'%s.%s symlink has no target') % (name, rev)
                )
            return target.encode(), mode
        else:
            sio = revtree.get_file(name)
            return sio.read(), mode
###END###
def getchanges(self, version, full):
        if full:
            raise error.Abort(_(b"convert from cvs does not support --full"))
        self._modecache = {}
        self._revtree = self.sourcerepo.revision_tree(version)
        # get the parentids from the cache
        parentids = self._parentids.pop(version)
        # only diff against first parent id
        prevtree = self.sourcerepo.revision_tree(parentids[0])
        files, changes = self._gettreechanges(self._revtree, prevtree)
        return files, changes, set()
###END###
def getcommit(self, version):
        rev = self.sourcerepo.get_revision(version)
        # populate parent id cache
        if not rev.parent_ids:
            parents = []
            self._parentids[version] = (revision.NULL_REVISION,)
        else:
            parents = self._filterghosts(rev.parent_ids)
            self._parentids[version] = parents

        branch = rev.properties.get('branch-nick', 'default')
        if branch == 'trunk':
            branch = 'default'
        return common.commit(
            parents=parents,
            date=b'%d %d' % (rev.timestamp, -rev.timezone),
            author=self.recode(rev.committer),
            desc=self.recode(rev.message),
            branch=branch.encode('utf8'),
            rev=version,
            saverev=self._saverev,
        )
###END###
def gettags(self):
        bytetags = {}
        for branch in self._bzrbranches():
            if not branch.supports_tags():
                return {}
            tagdict = branch.tags.get_tag_dict()
            for name, rev in pycompat.iteritems(tagdict):
                bytetags[self.recode(name)] = rev
        return bytetags
###END###
def getchangedfiles(self, rev, i):
        self._modecache = {}
        curtree = self.sourcerepo.revision_tree(rev)
        if i is not None:
            parentid = self._parentids[rev][i]
        else:
            # no parent id, get the empty revision
            parentid = revision.NULL_REVISION

        prevtree = self.sourcerepo.revision_tree(parentid)
        changes = [e[0] for e in self._gettreechanges(curtree, prevtree)[0]]
        return changes
###END###
def _gettreechanges(self, current, origin):
        revid = current._revision_id
        changes = []
        renames = {}
        seen = set()

        # Fall back to the deprecated attribute for legacy installations.
        try:
            inventory = origin.root_inventory
        except AttributeError:
            inventory = origin.inventory

        # Process the entries by reverse lexicographic name order to
        # handle nested renames correctly, most specific first.

        def key(c):
            return c.path[0] or c.path[1] or ""

        curchanges = sorted(
            current.iter_changes(origin),
            key=key,
            reverse=True,
        )
        for change in curchanges:
            paths = change.path
            kind = change.kind
            executable = change.executable
            if paths[0] == u'' or paths[1] == u'':
                # ignore changes to tree root
                continue

            # bazaar tracks directories, mercurial does not, so
            # we have to rename the directory contents
            if kind[1] == 'directory':
                if kind[0] not in (None, 'directory'):
                    # Replacing 'something' with a directory, record it
                    # so it can be removed.
                    changes.append((self.recode(paths[0]), revid))

                if kind[0] == 'directory' and None not in paths:
                    renaming = paths[0] != paths[1]
                    # neither an add nor an delete - a move
                    # rename all directory contents manually
                    subdir = inventory.path2id(paths[0])
                    # get all child-entries of the directory
                    for name, entry in inventory.iter_entries(subdir):
                        # hg does not track directory renames
                        if entry.kind == 'directory':
                            continue
                        frompath = self.recode(paths[0] + '/' + name)
                        if frompath in seen:
                            # Already handled by a more specific change entry
                            # This is important when you have:
                            # a => b
                            # a/c => a/c
                            # Here a/c must not be renamed into b/c
                            continue
                        seen.add(frompath)
                        if not renaming:
                            continue
                        topath = self.recode(paths[1] + '/' + name)
                        # register the files as changed
                        changes.append((frompath, revid))
                        changes.append((topath, revid))
                        # add to mode cache
                        mode = (
                            (entry.executable and b'x')
                            or (entry.kind == 'symlink' and b's')
                            or b''
                        )
                        self._modecache[(topath, revid)] = mode
                        # register the change as move
                        renames[topath] = frompath

                # no further changes, go to the next change
                continue

            # we got unicode paths, need to convert them
            path, topath = paths
            if path is not None:
                path = self.recode(path)
            if topath is not None:
                topath = self.recode(topath)
            seen.add(path or topath)

            if topath is None:
                # file deleted
                changes.append((path, revid))
                continue

            # renamed
            if path and path != topath:
                renames[topath] = path
                changes.append((path, revid))

            # populate the mode cache
            kind, executable = [e[1] for e in (kind, executable)]
            mode = (executable and b'x') or (kind == 'symlink' and b'l') or b''
            self._modecache[(topath, revid)] = mode
            changes.append((topath, revid))

        return changes, renames
###END###
def _filterghosts(self, ids):
        """Filters out ghost revisions which hg does not support, see
        <http://bazaar-vcs.org/GhostRevision>
        """
        parentmap = self.sourcerepo.get_parent_map(ids)
        parents = tuple([parent for parent in ids if parent in parentmap])
        return parents
###END###
def __init__(self, ui, repotype, path=None, revs=None):
        common.converter_source.__init__(self, ui, repotype, path, revs)
        if revs and len(revs) > 1:
            raise error.Abort(
                _(
                    b'monotone source does not support specifying '
                    b'multiple revs'
                )
            )
        common.commandline.__init__(self, ui, b'mtn')

        self.ui = ui
        self.path = path
        self.automatestdio = False
        self.revs = revs

        norepo = common.NoRepo(
            _(b"%s does not look like a monotone repository") % path
        )
        if not os.path.exists(os.path.join(path, b'_MTN')):
            # Could be a monotone repository (SQLite db file)
            try:
                f = open(path, b'rb')
                header = f.read(16)
                f.close()
            except IOError:
                header = b''
            if header != b'SQLite format 3\x00':
                raise norepo

        # regular expressions for parsing monotone output
        space = br'\s*'
        name = br'\s+"((?:\\"|[^"])*)"\s*'
        value = name
        revision = br'\s+\[(\w+)\]\s*'
        lines = br'(?:.|\n)+'

        self.dir_re = re.compile(space + b"dir" + name)
        self.file_re = re.compile(
            space + b"file" + name + b"content" + revision
        )
        self.add_file_re = re.compile(
            space + b"add_file" + name + b"content" + revision
        )
        self.patch_re = re.compile(
            space + b"patch" + name + b"from" + revision + b"to" + revision
        )
        self.rename_re = re.compile(space + b"rename" + name + b"to" + name)
        self.delete_re = re.compile(space + b"delete" + name)
        self.tag_re = re.compile(space + b"tag" + name + b"revision" + revision)
        self.cert_re = re.compile(
            lines + space + b"name" + name + b"value" + value
        )

        attr = space + b"file" + lines + space + b"attr" + space
        self.attr_execute_re = re.compile(
            attr + b'"mtn:execute"' + space + b'"true"'
        )

        # cached data
        self.manifest_rev = None
        self.manifest = None
        self.files = None
        self.dirs = None

        common.checktool(b'mtn', abort=False)
###END###
def mtnrun(self, *args, **kwargs):
        if self.automatestdio:
            return self.mtnrunstdio(*args, **kwargs)
        else:
            return self.mtnrunsingle(*args, **kwargs)
###END###
def mtnrunsingle(self, *args, **kwargs):
        kwargs['d'] = self.path
        return self.run0(b'automate', *args, **kwargs)
###END###
def mtnrunstdio(self, *args, **kwargs):
        # Prepare the command in automate stdio format
        kwargs = pycompat.byteskwargs(kwargs)
        command = []
        for k, v in pycompat.iteritems(kwargs):
            command.append(b"%d:%s" % (len(k), k))
            if v:
                command.append(b"%d:%s" % (len(v), v))
        if command:
            command.insert(0, b'o')
            command.append(b'e')

        command.append(b'l')
        for arg in args:
            command.append(b"%d:%s" % (len(arg), arg))
        command.append(b'e')
        command = b''.join(command)

        self.ui.debug(b"mtn: sending '%s'\n" % command)
        self.mtnwritefp.write(command)
        self.mtnwritefp.flush()

        return self.mtnstdioreadcommandoutput(command)
###END###
def mtnstdioreadpacket(self):
        read = None
        commandnbr = b''
        while read != b':':
            read = self.mtnreadfp.read(1)
            if not read:
                raise error.Abort(_(b'bad mtn packet - no end of commandnbr'))
            commandnbr += read
        commandnbr = commandnbr[:-1]

        stream = self.mtnreadfp.read(1)
        if stream not in b'mewptl':
            raise error.Abort(
                _(b'bad mtn packet - bad stream type %s') % stream
            )

        read = self.mtnreadfp.read(1)
        if read != b':':
            raise error.Abort(_(b'bad mtn packet - no divider before size'))

        read = None
        lengthstr = b''
        while read != b':':
            read = self.mtnreadfp.read(1)
            if not read:
                raise error.Abort(_(b'bad mtn packet - no end of packet size'))
            lengthstr += read
        try:
            length = pycompat.long(lengthstr[:-1])
        except TypeError:
            raise error.Abort(
                _(b'bad mtn packet - bad packet size %s') % lengthstr
            )

        read = self.mtnreadfp.read(length)
        if len(read) != length:
            raise error.Abort(
                _(
                    b"bad mtn packet - unable to read full packet "
                    b"read %s of %s"
                )
                % (len(read), length)
            )

        return (commandnbr, stream, length, read)
###END###
def mtnstdioreadcommandoutput(self, command):
        retval = []
        while True:
            commandnbr, stream, length, output = self.mtnstdioreadpacket()
            self.ui.debug(
                b'mtn: read packet %s:%s:%d\n' % (commandnbr, stream, length)
            )

            if stream == b'l':
                # End of command
                if output != b'0':
                    raise error.Abort(
                        _(b"mtn command '%s' returned %s") % (command, output)
                    )
                break
            elif stream in b'ew':
                # Error, warning output
                self.ui.warn(_(b'%s error:\n') % self.command)
                self.ui.warn(output)
            elif stream == b'p':
                # Progress messages
                self.ui.debug(b'mtn: ' + output)
            elif stream == b'm':
                # Main stream - command output
                retval.append(output)

        return b''.join(retval)
###END###
def mtnloadmanifest(self, rev):
        if self.manifest_rev == rev:
            return
        self.manifest = self.mtnrun(b"get_manifest_of", rev).split(b"\n\n")
        self.manifest_rev = rev
        self.files = {}
        self.dirs = {}

        for e in self.manifest:
            m = self.file_re.match(e)
            if m:
                attr = b""
                name = m.group(1)
                node = m.group(2)
                if self.attr_execute_re.match(e):
                    attr += b"x"
                self.files[name] = (node, attr)
            m = self.dir_re.match(e)
            if m:
                self.dirs[m.group(1)] = True
###END###
def mtnisfile(self, name, rev):
        # a non-file could be a directory or a deleted or renamed file
        self.mtnloadmanifest(rev)
        return name in self.files
###END###
def mtnisdir(self, name, rev):
        self.mtnloadmanifest(rev)
        return name in self.dirs
###END###
def mtngetcerts(self, rev):
        certs = {
            b"author": b"<missing>",
            b"date": b"<missing>",
            b"changelog": b"<missing>",
            b"branch": b"<missing>",
        }
        certlist = self.mtnrun(b"certs", rev)
        # mtn < 0.45:
        #   key "test@selenic.com"
        # mtn >= 0.45:
        #   key [ff58a7ffb771907c4ff68995eada1c4da068d328]
        certlist = re.split(br'\n\n {6}key ["\[]', certlist)
        for e in certlist:
            m = self.cert_re.match(e)
            if m:
                name, value = m.groups()
                value = value.replace(br'\"', b'"')
                value = value.replace(br'\\', b'\\')
                certs[name] = value
        # Monotone may have subsecond dates: 2005-02-05T09:39:12.364306
        # and all times are stored in UTC
        certs[b"date"] = certs[b"date"].split(b'.')[0] + b" UTC"
        return certs
###END###
def getheads(self):
        if not self.revs:
            return self.mtnrun(b"leaves").splitlines()
        else:
            return self.revs
###END###
def getchanges(self, rev, full):
        if full:
            raise error.Abort(
                _(b"convert from monotone does not support --full")
            )
        revision = self.mtnrun(b"get_revision", rev).split(b"\n\n")
        files = {}
        ignoremove = {}
        renameddirs = []
        copies = {}
        for e in revision:
            m = self.add_file_re.match(e)
            if m:
                files[m.group(1)] = rev
                ignoremove[m.group(1)] = rev
            m = self.patch_re.match(e)
            if m:
                files[m.group(1)] = rev
            # Delete/rename is handled later when the convert engine
            # discovers an IOError exception from getfile,
            # but only if we add the "from" file to the list of changes.
            m = self.delete_re.match(e)
            if m:
                files[m.group(1)] = rev
            m = self.rename_re.match(e)
            if m:
                toname = m.group(2)
                fromname = m.group(1)
                if self.mtnisfile(toname, rev):
                    ignoremove[toname] = 1
                    copies[toname] = fromname
                    files[toname] = rev
                    files[fromname] = rev
                elif self.mtnisdir(toname, rev):
                    renameddirs.append((fromname, toname))

        # Directory renames can be handled only once we have recorded
        # all new files
        for fromdir, todir in renameddirs:
            renamed = {}
            for tofile in self.files:
                if tofile in ignoremove:
                    continue
                if tofile.startswith(todir + b'/'):
                    renamed[tofile] = fromdir + tofile[len(todir) :]
                    # Avoid chained moves like:
                    # d1(/a) => d3/d1(/a)
                    # d2 => d3
                    ignoremove[tofile] = 1
            for tofile, fromfile in renamed.items():
                self.ui.debug(
                    b"copying file in renamed directory from '%s' to '%s'"
                    % (fromfile, tofile),
                    b'\n',
                )
                files[tofile] = rev
                copies[tofile] = fromfile
            for fromfile in renamed.values():
                files[fromfile] = rev

        return (files.items(), copies, set())
###END###
def getfile(self, name, rev):
        if not self.mtnisfile(name, rev):
            return None, None
        try:
            data = self.mtnrun(b"get_file_of", name, r=rev)
        except Exception:
            return None, None
        self.mtnloadmanifest(rev)
        node, attr = self.files.get(name, (None, b""))
        return data, attr
###END###
def getcommit(self, rev):
        extra = {}
        certs = self.mtngetcerts(rev)
        if certs.get(b'suspend') == certs[b"branch"]:
            extra[b'close'] = b'1'
        dateformat = b"%Y-%m-%dT%H:%M:%S"
        return common.commit(
            author=certs[b"author"],
            date=dateutil.datestr(dateutil.strdate(certs[b"date"], dateformat)),
            desc=certs[b"changelog"],
            rev=rev,
            parents=self.mtnrun(b"parents", rev).splitlines(),
            branch=certs[b"branch"],
            extra=extra,
        )
###END###
def gettags(self):
        tags = {}
        for e in self.mtnrun(b"tags").split(b"\n\n"):
            m = self.tag_re.match(e)
            if m:
                tags[m.group(1)] = m.group(2)
        return tags
###END###
def getchangedfiles(self, rev, i):
        # This function is only needed to support --filemap
        # ... and we don't support that
        raise NotImplementedError
###END###
def before(self):
        # Check if we have a new enough version to use automate stdio
        try:
            versionstr = self.mtnrunsingle(b"interface_version")
            version = float(versionstr)
        except Exception:
            raise error.Abort(
                _(b"unable to determine mtn automate interface version")
            )

        if version >= 12.0:
            self.automatestdio = True
            self.ui.debug(
                b"mtn automate version %f - using automate stdio\n" % version
            )

            # launch the long-running automate stdio process
            self.mtnwritefp, self.mtnreadfp = self._run2(
                b'automate', b'stdio', b'-d', self.path
            )
            # read the headers
            read = self.mtnreadfp.readline()
            if read != b'format-version: 2\n':
                raise error.Abort(
                    _(b'mtn automate stdio header unexpected: %s') % read
                )
            while read != b'\n':
                read = self.mtnreadfp.readline()
                if not read:
                    raise error.Abort(
                        _(
                            b"failed to reach end of mtn automate "
                            b"stdio headers"
                        )
                    )
        else:
            self.ui.debug(
                b"mtn automate version %s - not using automate stdio "
                b"(automate >= 12.0 - mtn >= 0.46 is needed)\n" % version
            )
###END###
def after(self):
        if self.automatestdio:
            self.mtnwritefp.close()
            self.mtnwritefp = None
            self.mtnreadfp.close()
            self.mtnreadfp = None
###END###
def __init__(self, p):
        self.copyfrom_path = p.copyfrom_path
        self.copyfrom_rev = p.copyfrom_rev
        self.action = p.action
###END###
def __init__(self, stdout):
        self._stdout = stdout
###END###
def __iter__(self):
        while True:
            try:
                entry = pickle.load(self._stdout)
            except EOFError:
                raise error.Abort(
                    _(
                        b'Mercurial failed to run itself, check'
                        b' hg executable is in PATH'
                    )
                )
            try:
                orig_paths, revnum, author, date, message = entry
            except (TypeError, ValueError):
                if entry is None:
                    break
                raise error.Abort(_(b"log stream exception '%s'") % entry)
            yield entry
###END###
def close(self):
        if self._stdout:
            self._stdout.close()
            self._stdout = None
###END###
def __init__(
        self,
        url,
        paths,
        start,
        end,
        limit=0,
        discover_changed_paths=True,
        strict_node_history=False,
    ):
        def receiver(orig_paths, revnum, author, date, message, pool):
            paths = {}
            if orig_paths is not None:
                for k, v in pycompat.iteritems(orig_paths):
                    paths[k] = changedpath(v)
            self.append((paths, revnum, author, date, message))

        # Use an ra of our own so that our parent can consume
        # our results without confusing the server.
        t = transport.SvnRaTransport(url=url)
        svn.ra.get_log(
            t.ra,
            paths,
            start,
            end,
            limit,
            discover_changed_paths,
            strict_node_history,
            receiver,
        )
###END###
def close(self):
        pass
###END###
def __init__(self, ui, repotype, url, revs=None):
        super(svn_source, self).__init__(ui, repotype, url, revs=revs)

        init_fsencoding()
        if not (
            url.startswith(b'svn://')
            or url.startswith(b'svn+ssh://')
            or (
                os.path.exists(url)
                and os.path.exists(os.path.join(url, b'.svn'))
            )
            or issvnurl(ui, url)
        ):
            raise NoRepo(
                _(b"%s does not look like a Subversion repository") % url
            )
        if svn is None:
            raise MissingTool(_(b'could not load Subversion python bindings'))

        try:
            version = svn.core.SVN_VER_MAJOR, svn.core.SVN_VER_MINOR
            if version < (1, 4):
                raise MissingTool(
                    _(
                        b'Subversion python bindings %d.%d found, '
                        b'1.4 or later required'
                    )
                    % version
                )
        except AttributeError:
            raise MissingTool(
                _(
                    b'Subversion python bindings are too old, 1.4 '
                    b'or later required'
                )
            )

        self.lastrevs = {}

        latest = None
        try:
            # Support file://path@rev syntax. Useful e.g. to convert
            # deleted branches.
            at = url.rfind(b'@')
            if at >= 0:
                latest = int(url[at + 1 :])
                url = url[:at]
        except ValueError:
            pass
        self.url = geturl(url)
        self.encoding = b'UTF-8'  # Subversion is always nominal UTF-8
        try:
            with util.with_lc_ctype():
                self.transport = transport.SvnRaTransport(url=self.url)
                self.ra = self.transport.ra
                self.ctx = self.transport.client
                self.baseurl = svn.ra.get_repos_root(self.ra)
                # Module is either empty or a repository path starting with
                # a slash and not ending with a slash.
                self.module = urlreq.unquote(self.url[len(self.baseurl) :])
                self.prevmodule = None
                self.rootmodule = self.module
                self.commits = {}
                self.paths = {}
                self.uuid = svn.ra.get_uuid(self.ra)
        except svn.core.SubversionException:
            ui.traceback()
            svnversion = b'%d.%d.%d' % (
                svn.core.SVN_VER_MAJOR,
                svn.core.SVN_VER_MINOR,
                svn.core.SVN_VER_MICRO,
            )
            raise NoRepo(
                _(
                    b"%s does not look like a Subversion repository "
                    b"to libsvn version %s"
                )
                % (self.url, svnversion)
            )

        if revs:
            if len(revs) > 1:
                raise error.Abort(
                    _(
                        b'subversion source does not support '
                        b'specifying multiple revisions'
                    )
                )
            try:
                latest = int(revs[0])
            except ValueError:
                raise error.Abort(
                    _(b'svn: revision %s is not an integer') % revs[0]
                )

        trunkcfg = self.ui.config(b'convert', b'svn.trunk')
        if trunkcfg is None:
            trunkcfg = b'trunk'
        self.trunkname = trunkcfg.strip(b'/')
        self.startrev = self.ui.config(b'convert', b'svn.startrev')
        try:
            self.startrev = int(self.startrev)
            if self.startrev < 0:
                self.startrev = 0
        except ValueError:
            raise error.Abort(
                _(b'svn: start revision %s is not an integer') % self.startrev
            )

        try:
            with util.with_lc_ctype():
                self.head = self.latest(self.module, latest)
        except SvnPathNotFound:
            self.head = None
        if not self.head:
            raise error.Abort(
                _(b'no revision found in module %s') % self.module
            )
        self.last_changed = self.revnum(self.head)

        self._changescache = (None, None)

        if os.path.exists(os.path.join(url, b'.svn/entries')):
            self.wc = url
        else:
            self.wc = None
        self.convertfp = None
###END###
def before(self):
        self.with_lc_ctype = util.with_lc_ctype()
        self.with_lc_ctype.__enter__()
###END###
def after(self):
        self.with_lc_ctype.__exit__(None, None, None)
###END###
def setrevmap(self, revmap):
        lastrevs = {}
        for revid in revmap:
            uuid, module, revnum = revsplit(revid)
            lastrevnum = lastrevs.setdefault(module, revnum)
            if revnum > lastrevnum:
                lastrevs[module] = revnum
        self.lastrevs = lastrevs
###END###
def exists(self, path, optrev):
        try:
            svn.client.ls(
                self.url.rstrip(b'/') + b'/' + quote(path),
                optrev,
                False,
                self.ctx,
            )
            return True
        except svn.core.SubversionException:
            return False
###END###
def getheads(self):
        def isdir(path, revnum):
            kind = self._checkpath(path, revnum)
            return kind == svn.core.svn_node_dir

        def getcfgpath(name, rev):
            cfgpath = self.ui.config(b'convert', b'svn.' + name)
            if cfgpath is not None and cfgpath.strip() == b'':
                return None
            path = (cfgpath or name).strip(b'/')
            if not self.exists(path, rev):
                if self.module.endswith(path) and name == b'trunk':
                    # we are converting from inside this directory
                    return None
                if cfgpath:
                    raise error.Abort(
                        _(b'expected %s to be at %r, but not found')
                        % (name, path)
                    )
                return None
            self.ui.note(
                _(b'found %s at %r\n') % (name, pycompat.bytestr(path))
            )
            return path

        rev = optrev(self.last_changed)
        oldmodule = b''
        trunk = getcfgpath(b'trunk', rev)
        self.tags = getcfgpath(b'tags', rev)
        branches = getcfgpath(b'branches', rev)

        # If the project has a trunk or branches, we will extract heads
        # from them. We keep the project root otherwise.
        if trunk:
            oldmodule = self.module or b''
            self.module += b'/' + trunk
            self.head = self.latest(self.module, self.last_changed)
            if not self.head:
                raise error.Abort(
                    _(b'no revision found in module %s') % self.module
                )

        # First head in the list is the module's head
        self.heads = [self.head]
        if self.tags is not None:
            self.tags = b'%s/%s' % (oldmodule, (self.tags or b'tags'))

        # Check if branches bring a few more heads to the list
        if branches:
            rpath = self.url.strip(b'/')
            branchnames = svn.client.ls(
                rpath + b'/' + quote(branches), rev, False, self.ctx
            )
            for branch in sorted(branchnames):
                module = b'%s/%s/%s' % (oldmodule, branches, branch)
                if not isdir(module, self.last_changed):
                    continue
                brevid = self.latest(module, self.last_changed)
                if not brevid:
                    self.ui.note(_(b'ignoring empty branch %s\n') % branch)
                    continue
                self.ui.note(
                    _(b'found branch %s at %d\n')
                    % (branch, self.revnum(brevid))
                )
                self.heads.append(brevid)

        if self.startrev and self.heads:
            if len(self.heads) > 1:
                raise error.Abort(
                    _(
                        b'svn: start revision is not supported '
                        b'with more than one branch'
                    )
                )
            revnum = self.revnum(self.heads[0])
            if revnum < self.startrev:
                raise error.Abort(
                    _(b'svn: no revision found after start revision %d')
                    % self.startrev
                )

        return self.heads
###END###
def _getchanges(self, rev, full):
        (paths, parents) = self.paths[rev]
        copies = {}
        if parents:
            files, self.removed, copies = self.expandpaths(rev, paths, parents)
        if full or not parents:
            # Perform a full checkout on roots
            uuid, module, revnum = revsplit(rev)
            entries = svn.client.ls(
                self.baseurl + quote(module), optrev(revnum), True, self.ctx
            )
            files = [
                n
                for n, e in pycompat.iteritems(entries)
                if e.kind == svn.core.svn_node_file
            ]
            self.removed = set()

        files.sort()
        files = pycompat.ziplist(files, [rev] * len(files))
        return (files, copies)
###END###
def getchanges(self, rev, full):
        # reuse cache from getchangedfiles
        if self._changescache[0] == rev and not full:
            (files, copies) = self._changescache[1]
        else:
            (files, copies) = self._getchanges(rev, full)
            # caller caches the result, so free it here to release memory
            del self.paths[rev]
        return (files, copies, set())
###END###
def getchangedfiles(self, rev, i):
        # called from filemap - cache computed values for reuse in getchanges
        (files, copies) = self._getchanges(rev, False)
        self._changescache = (rev, (files, copies))
        return [f[0] for f in files]
###END###
def getcommit(self, rev):
        if rev not in self.commits:
            uuid, module, revnum = revsplit(rev)
            self.module = module
            self.reparent(module)
            # We assume that:
            # - requests for revisions after "stop" come from the
            # revision graph backward traversal. Cache all of them
            # down to stop, they will be used eventually.
            # - requests for revisions before "stop" come to get
            # isolated branches parents. Just fetch what is needed.
            stop = self.lastrevs.get(module, 0)
            if revnum < stop:
                stop = revnum + 1
            self._fetch_revisions(revnum, stop)
            if rev not in self.commits:
                raise error.Abort(_(b'svn: revision %s not found') % revnum)
        revcommit = self.commits[rev]
        # caller caches the result, so free it here to release memory
        del self.commits[rev]
        return revcommit
###END###
def checkrevformat(self, revstr, mapname=b'splicemap'):
        """fails if revision format does not match the correct format"""
        if not re.match(
            br'svn:[0-9a-f]{8,8}-[0-9a-f]{4,4}-'
            br'[0-9a-f]{4,4}-[0-9a-f]{4,4}-[0-9a-f]'
            br'{12,12}(.*)@[0-9]+$',
            revstr,
        ):
            raise error.Abort(
                _(b'%s entry %s is not a valid revision identifier')
                % (mapname, revstr)
            )
###END###
def numcommits(self):
        return int(self.head.rsplit(b'@', 1)[1]) - self.startrev
###END###
def gettags(self):
        tags = {}
        if self.tags is None:
            return tags

        # svn tags are just a convention, project branches left in a
        # 'tags' directory. There is no other relationship than
        # ancestry, which is expensive to discover and makes them hard
        # to update incrementally.  Worse, past revisions may be
        # referenced by tags far away in the future, requiring a deep
        # history traversal on every calculation.  Current code
        # performs a single backward traversal, tracking moves within
        # the tags directory (tag renaming) and recording a new tag
        # everytime a project is copied from outside the tags
        # directory. It also lists deleted tags, this behaviour may
        # change in the future.
        pendings = []
        tagspath = self.tags
        start = svn.ra.get_latest_revnum(self.ra)
        stream = self._getlog([self.tags], start, self.startrev)
        try:
            for entry in stream:
                origpaths, revnum, author, date, message = entry
                if not origpaths:
                    origpaths = []
                copies = [
                    (e.copyfrom_path, e.copyfrom_rev, p)
                    for p, e in pycompat.iteritems(origpaths)
                    if e.copyfrom_path
                ]
                # Apply moves/copies from more specific to general
                copies.sort(reverse=True)

                srctagspath = tagspath
                if copies and copies[-1][2] == tagspath:
                    # Track tags directory moves
                    srctagspath = copies.pop()[0]

                for source, sourcerev, dest in copies:
                    if not dest.startswith(tagspath + b'/'):
                        continue
                    for tag in pendings:
                        if tag[0].startswith(dest):
                            tagpath = source + tag[0][len(dest) :]
                            tag[:2] = [tagpath, sourcerev]
                            break
                    else:
                        pendings.append([source, sourcerev, dest])

                # Filter out tags with children coming from different
                # parts of the repository like:
                # /tags/tag.1 (from /trunk:10)
                # /tags/tag.1/foo (from /branches/foo:12)
                # Here/tags/tag.1 discarded as well as its children.
                # It happens with tools like cvs2svn. Such tags cannot
                # be represented in mercurial.
                addeds = {
                    p: e.copyfrom_path
                    for p, e in pycompat.iteritems(origpaths)
                    if e.action == b'A' and e.copyfrom_path
                }
                badroots = set()
                for destroot in addeds:
                    for source, sourcerev, dest in pendings:
                        if not dest.startswith(
                            destroot + b'/'
                        ) or source.startswith(addeds[destroot] + b'/'):
                            continue
                        badroots.add(destroot)
                        break

                for badroot in badroots:
                    pendings = [
                        p
                        for p in pendings
                        if p[2] != badroot
                        and not p[2].startswith(badroot + b'/')
                    ]

                # Tell tag renamings from tag creations
                renamings = []
                for source, sourcerev, dest in pendings:
                    tagname = dest.split(b'/')[-1]
                    if source.startswith(srctagspath):
                        renamings.append([source, sourcerev, tagname])
                        continue
                    if tagname in tags:
                        # Keep the latest tag value
                        continue
                    # From revision may be fake, get one with changes
                    try:
                        tagid = self.latest(source, sourcerev)
                        if tagid and tagname not in tags:
                            tags[tagname] = tagid
                    except SvnPathNotFound:
                        # It happens when we are following directories
                        # we assumed were copied with their parents
                        # but were really created in the tag
                        # directory.
                        pass
                pendings = renamings
                tagspath = srctagspath
        finally:
            stream.close()
        return tags
###END###
def converted(self, rev, destrev):
        if not self.wc:
            return
        if self.convertfp is None:
            self.convertfp = open(
                os.path.join(self.wc, b'.svn', b'hg-shamap'), b'ab'
            )
        self.convertfp.write(
            util.tonativeeol(b'%s %d\n' % (destrev, self.revnum(rev)))
        )
        self.convertfp.flush()
###END###
def revid(self, revnum, module=None):
        return b'svn:%s%s@%d' % (self.uuid, module or self.module, revnum)
###END###
def revnum(self, rev):
        return int(rev.split(b'@')[-1])
###END###
def latest(self, path, stop=None):
        """Find the latest revid affecting path, up to stop revision
        number. If stop is None, default to repository latest
        revision. It may return a revision in a different module,
        since a branch may be moved without a change being
        reported. Return None if computed module does not belong to
        rootmodule subtree.
        """

        def findchanges(path, start, stop=None):
            stream = self._getlog([path], start, stop or 1)
            try:
                for entry in stream:
                    paths, revnum, author, date, message = entry
                    if stop is None and paths:
                        # We do not know the latest changed revision,
                        # keep the first one with changed paths.
                        break
                    if stop is not None and revnum <= stop:
                        break

                    for p in paths:
                        if not path.startswith(p) or not paths[p].copyfrom_path:
                            continue
                        newpath = paths[p].copyfrom_path + path[len(p) :]
                        self.ui.debug(
                            b"branch renamed from %s to %s at %d\n"
                            % (path, newpath, revnum)
                        )
                        path = newpath
                        break
                if not paths:
                    revnum = None
                return revnum, path
            finally:
                stream.close()

        if not path.startswith(self.rootmodule):
            # Requests on foreign branches may be forbidden at server level
            self.ui.debug(b'ignoring foreign branch %r\n' % path)
            return None

        if stop is None:
            stop = svn.ra.get_latest_revnum(self.ra)
        try:
            prevmodule = self.reparent(b'')
            dirent = svn.ra.stat(self.ra, path.strip(b'/'), stop)
            self.reparent(prevmodule)
        except svn.core.SubversionException:
            dirent = None
        if not dirent:
            raise SvnPathNotFound(
                _(b'%s not found up to revision %d') % (path, stop)
            )

        # stat() gives us the previous revision on this line of
        # development, but it might be in *another module*. Fetch the
        # log and detect renames down to the latest revision.
        revnum, realpath = findchanges(path, stop, dirent.created_rev)
        if revnum is None:
            # Tools like svnsync can create empty revision, when
            # synchronizing only a subtree for instance. These empty
            # revisions created_rev still have their original values
            # despite all changes having disappeared and can be
            # returned by ra.stat(), at least when stating the root
            # module. In that case, do not trust created_rev and scan
            # the whole history.
            revnum, realpath = findchanges(path, stop)
            if revnum is None:
                self.ui.debug(b'ignoring empty branch %r\n' % realpath)
                return None

        if not realpath.startswith(self.rootmodule):
            self.ui.debug(b'ignoring foreign branch %r\n' % realpath)
            return None
        return self.revid(revnum, realpath)
###END###
def reparent(self, module):
        """Reparent the svn transport and return the previous parent."""
        if self.prevmodule == module:
            return module
        svnurl = self.baseurl + quote(module)
        prevmodule = self.prevmodule
        if prevmodule is None:
            prevmodule = b''
        self.ui.debug(b"reparent to %s\n" % svnurl)
        svn.ra.reparent(self.ra, svnurl)
        self.prevmodule = module
        return prevmodule
###END###
def expandpaths(self, rev, paths, parents):
        changed, removed = set(), set()
        copies = {}

        new_module, revnum = revsplit(rev)[1:]
        if new_module != self.module:
            self.module = new_module
            self.reparent(self.module)

        progress = self.ui.makeprogress(
            _(b'scanning paths'), unit=_(b'paths'), total=len(paths)
        )
        for i, (path, ent) in enumerate(paths):
            progress.update(i, item=path)
            entrypath = self.getrelpath(path)

            kind = self._checkpath(entrypath, revnum)
            if kind == svn.core.svn_node_file:
                changed.add(self.recode(entrypath))
                if not ent.copyfrom_path or not parents:
                    continue
                # Copy sources not in parent revisions cannot be
                # represented, ignore their origin for now
                pmodule, prevnum = revsplit(parents[0])[1:]
                if ent.copyfrom_rev < prevnum:
                    continue
                copyfrom_path = self.getrelpath(ent.copyfrom_path, pmodule)
                if not copyfrom_path:
                    continue
                self.ui.debug(
                    b"copied to %s from %s@%d\n"
                    % (entrypath, copyfrom_path, ent.copyfrom_rev)
                )
                copies[self.recode(entrypath)] = self.recode(copyfrom_path)
            elif kind == 0:  # gone, but had better be a deleted *file*
                self.ui.debug(b"gone from %d\n" % ent.copyfrom_rev)
                pmodule, prevnum = revsplit(parents[0])[1:]
                parentpath = pmodule + b"/" + entrypath
                fromkind = self._checkpath(entrypath, prevnum, pmodule)

                if fromkind == svn.core.svn_node_file:
                    removed.add(self.recode(entrypath))
                elif fromkind == svn.core.svn_node_dir:
                    oroot = parentpath.strip(b'/')
                    nroot = path.strip(b'/')
                    children = self._iterfiles(oroot, prevnum)
                    for childpath in children:
                        childpath = childpath.replace(oroot, nroot)
                        childpath = self.getrelpath(b"/" + childpath, pmodule)
                        if childpath:
                            removed.add(self.recode(childpath))
                else:
                    self.ui.debug(
                        b'unknown path in revision %d: %s\n' % (revnum, path)
                    )
            elif kind == svn.core.svn_node_dir:
                if ent.action == b'M':
                    # If the directory just had a prop change,
                    # then we shouldn't need to look for its children.
                    continue
                if ent.action == b'R' and parents:
                    # If a directory is replacing a file, mark the previous
                    # file as deleted
                    pmodule, prevnum = revsplit(parents[0])[1:]
                    pkind = self._checkpath(entrypath, prevnum, pmodule)
                    if pkind == svn.core.svn_node_file:
                        removed.add(self.recode(entrypath))
                    elif pkind == svn.core.svn_node_dir:
                        # We do not know what files were kept or removed,
                        # mark them all as changed.
                        for childpath in self._iterfiles(pmodule, prevnum):
                            childpath = self.getrelpath(b"/" + childpath)
                            if childpath:
                                changed.add(self.recode(childpath))

                for childpath in self._iterfiles(path, revnum):
                    childpath = self.getrelpath(b"/" + childpath)
                    if childpath:
                        changed.add(self.recode(childpath))

                # Handle directory copies
                if not ent.copyfrom_path or not parents:
                    continue
                # Copy sources not in parent revisions cannot be
                # represented, ignore their origin for now
                pmodule, prevnum = revsplit(parents[0])[1:]
                if ent.copyfrom_rev < prevnum:
                    continue
                copyfrompath = self.getrelpath(ent.copyfrom_path, pmodule)
                if not copyfrompath:
                    continue
                self.ui.debug(
                    b"mark %s came from %s:%d\n"
                    % (path, copyfrompath, ent.copyfrom_rev)
                )
                children = self._iterfiles(ent.copyfrom_path, ent.copyfrom_rev)
                for childpath in children:
                    childpath = self.getrelpath(b"/" + childpath, pmodule)
                    if not childpath:
                        continue
                    copytopath = path + childpath[len(copyfrompath) :]
                    copytopath = self.getrelpath(copytopath)
                    copies[self.recode(copytopath)] = self.recode(childpath)

        progress.complete()
        changed.update(removed)
        return (list(changed), removed, copies)
###END###
def _fetch_revisions(self, from_revnum, to_revnum):
        if from_revnum < to_revnum:
            from_revnum, to_revnum = to_revnum, from_revnum

        self.child_cset = None

        def parselogentry(orig_paths, revnum, author, date, message):
            """Return the parsed commit object or None, and True if
            the revision is a branch root.
            """
            self.ui.debug(
                b"parsing revision %d (%d changes)\n"
                % (revnum, len(orig_paths))
            )

            branched = False
            rev = self.revid(revnum)
            # branch log might return entries for a parent we already have

            if rev in self.commits or revnum < to_revnum:
                return None, branched

            parents = []
            # check whether this revision is the start of a branch or part
            # of a branch renaming
            orig_paths = sorted(pycompat.iteritems(orig_paths))
            root_paths = [
                (p, e) for p, e in orig_paths if self.module.startswith(p)
            ]
            if root_paths:
                path, ent = root_paths[-1]
                if ent.copyfrom_path:
                    branched = True
                    newpath = ent.copyfrom_path + self.module[len(path) :]
                    # ent.copyfrom_rev may not be the actual last revision
                    previd = self.latest(newpath, ent.copyfrom_rev)
                    if previd is not None:
                        prevmodule, prevnum = revsplit(previd)[1:]
                        if prevnum >= self.startrev:
                            parents = [previd]
                            self.ui.note(
                                _(b'found parent of branch %s at %d: %s\n')
                                % (self.module, prevnum, prevmodule)
                            )
                else:
                    self.ui.debug(b"no copyfrom path, don't know what to do.\n")

            paths = []
            # filter out unrelated paths
            for path, ent in orig_paths:
                if self.getrelpath(path) is None:
                    continue
                paths.append((path, ent))

            date = parsesvndate(date)
            if self.ui.configbool(b'convert', b'localtimezone'):
                date = makedatetimestamp(date[0])

            if message:
                log = self.recode(message)
            else:
                log = b''

            if author:
                author = self.recode(author)
            else:
                author = b''

            try:
                branch = self.module.split(b"/")[-1]
                if branch == self.trunkname:
                    branch = None
            except IndexError:
                branch = None

            cset = commit(
                author=author,
                date=dateutil.datestr(date, b'%Y-%m-%d %H:%M:%S %1%2'),
                desc=log,
                parents=parents,
                branch=branch,
                rev=rev,
            )

            self.commits[rev] = cset
            # The parents list is *shared* among self.paths and the
            # commit object. Both will be updated below.
            self.paths[rev] = (paths, cset.parents)
            if self.child_cset and not self.child_cset.parents:
                self.child_cset.parents[:] = [rev]
            self.child_cset = cset
            return cset, branched

        self.ui.note(
            _(b'fetching revision log for "%s" from %d to %d\n')
            % (self.module, from_revnum, to_revnum)
        )

        try:
            firstcset = None
            lastonbranch = False
            stream = self._getlog([self.module], from_revnum, to_revnum)
            try:
                for entry in stream:
                    paths, revnum, author, date, message = entry
                    if revnum < self.startrev:
                        lastonbranch = True
                        break
                    if not paths:
                        self.ui.debug(b'revision %d has no entries\n' % revnum)
                        # If we ever leave the loop on an empty
                        # revision, do not try to get a parent branch
                        lastonbranch = lastonbranch or revnum == 0
                        continue
                    cset, lastonbranch = parselogentry(
                        paths, revnum, author, date, message
                    )
                    if cset:
                        firstcset = cset
                    if lastonbranch:
                        break
            finally:
                stream.close()

            if not lastonbranch and firstcset and not firstcset.parents:
                # The first revision of the sequence (the last fetched one)
                # has invalid parents if not a branch root. Find the parent
                # revision now, if any.
                try:
                    firstrevnum = self.revnum(firstcset.rev)
                    if firstrevnum > 1:
                        latest = self.latest(self.module, firstrevnum - 1)
                        if latest:
                            firstcset.parents.append(latest)
                except SvnPathNotFound:
                    pass
        except svn.core.SubversionException as xxx_todo_changeme:
            (inst, num) = xxx_todo_changeme.args
            if num == svn.core.SVN_ERR_FS_NO_SUCH_REVISION:
                raise error.Abort(
                    _(b'svn: branch has no revision %s') % to_revnum
                )
            raise
###END###
def getfile(self, file, rev):
        # TODO: ra.get_file transmits the whole file instead of diffs.
        if file in self.removed:
            return None, None
        try:
            new_module, revnum = revsplit(rev)[1:]
            if self.module != new_module:
                self.module = new_module
                self.reparent(self.module)
            io = stringio()
            info = svn.ra.get_file(self.ra, file, revnum, io)
            data = io.getvalue()
            # ra.get_file() seems to keep a reference on the input buffer
            # preventing collection. Release it explicitly.
            io.close()
            if isinstance(info, list):
                info = info[-1]
            mode = (b"svn:executable" in info) and b'x' or b''
            mode = (b"svn:special" in info) and b'l' or mode
        except svn.core.SubversionException as e:
            notfound = (
                svn.core.SVN_ERR_FS_NOT_FOUND,
                svn.core.SVN_ERR_RA_DAV_PATH_NOT_FOUND,
            )
            if e.apr_err in notfound:  # File not found
                return None, None
            raise
        if mode == b'l':
            link_prefix = b"link "
            if data.startswith(link_prefix):
                data = data[len(link_prefix) :]
        return data, mode
###END###
def _iterfiles(self, path, revnum):
        """Enumerate all files in path at revnum, recursively."""
        path = path.strip(b'/')
        pool = svn.core.Pool()
        rpath = b'/'.join([self.baseurl, quote(path)]).strip(b'/')
        entries = svn.client.ls(rpath, optrev(revnum), True, self.ctx, pool)
        if path:
            path += b'/'
        return (
            (path + p)
            for p, e in pycompat.iteritems(entries)
            if e.kind == svn.core.svn_node_file
        )
###END###
def getrelpath(self, path, module=None):
        if module is None:
            module = self.module
        # Given the repository url of this wc, say
        #   "http://server/plone/CMFPlone/branches/Plone-2_0-branch"
        # extract the "entry" portion (a relative path) from what
        # svn log --xml says, i.e.
        #   "/CMFPlone/branches/Plone-2_0-branch/tests/PloneTestCase.py"
        # that is to say "tests/PloneTestCase.py"
        if path.startswith(module):
            relative = path.rstrip(b'/')[len(module) :]
            if relative.startswith(b'/'):
                return relative[1:]
            elif relative == b'':
                return relative

        # The path is outside our tracked tree...
        self.ui.debug(
            b'%r is not under %r, ignoring\n'
            % (pycompat.bytestr(path), pycompat.bytestr(module))
        )
        return None
###END###
def _checkpath(self, path, revnum, module=None):
        if module is not None:
            prevmodule = self.reparent(b'')
            path = module + b'/' + path
        try:
            # ra.check_path does not like leading slashes very much, it leads
            # to PROPFIND subversion errors
            return svn.ra.check_path(self.ra, path.strip(b'/'), revnum)
        finally:
            if module is not None:
                self.reparent(prevmodule)
###END###
def _getlog(
        self,
        paths,
        start,
        end,
        limit=0,
        discover_changed_paths=True,
        strict_node_history=False,
    ):
        # Normalize path names, svn >= 1.5 only wants paths relative to
        # supplied URL
        relpaths = []
        for p in paths:
            if not p.startswith(b'/'):
                p = self.module + b'/' + p
            relpaths.append(p.strip(b'/'))
        args = [
            self.baseurl,
            relpaths,
            start,
            end,
            limit,
            discover_changed_paths,
            strict_node_history,
        ]
        # developer config: convert.svn.debugsvnlog
        if not self.ui.configbool(b'convert', b'svn.debugsvnlog'):
            return directlogstream(*args)
        arg = encodeargs(args)
        hgexe = procutil.hgexecutable()
        cmd = b'%s debugsvnlog' % procutil.shellquote(hgexe)
        stdin, stdout = procutil.popen2(cmd)
        stdin.write(arg)
        try:
            stdin.close()
        except IOError:
            raise error.Abort(
                _(
                    b'Mercurial failed to run itself, check'
                    b' hg executable is in PATH'
                )
            )
        return logstream(stdout)
###END###
def prerun(self):
        if self.wc:
            os.chdir(self.wc)
###END###
def postrun(self):
        if self.wc:
            os.chdir(self.cwd)
###END###
def join(self, name):
        return os.path.join(self.wc, b'.svn', name)
###END###
def revmapfile(self):
        return self.join(b'hg-shamap')
###END###
def authorfile(self):
        return self.join(b'hg-authormap')
###END###
def __init__(self, ui, repotype, path):

        converter_sink.__init__(self, ui, repotype, path)
        commandline.__init__(self, ui, b'svn')
        self.delete = []
        self.setexec = []
        self.delexec = []
        self.copies = []
        self.wc = None
        self.cwd = encoding.getcwd()

        created = False
        if os.path.isfile(os.path.join(path, b'.svn', b'entries')):
            self.wc = os.path.realpath(path)
            self.run0(b'update')
        else:
            if not re.search(br'^(file|http|https|svn|svn\+ssh)://', path):
                path = os.path.realpath(path)
                if os.path.isdir(os.path.dirname(path)):
                    if not os.path.exists(
                        os.path.join(path, b'db', b'fs-type')
                    ):
                        ui.status(
                            _(b"initializing svn repository '%s'\n")
                            % os.path.basename(path)
                        )
                        commandline(ui, b'svnadmin').run0(b'create', path)
                        created = path
                    path = util.normpath(path)
                    if not path.startswith(b'/'):
                        path = b'/' + path
                    path = b'file://' + path

            wcpath = os.path.join(
                encoding.getcwd(), os.path.basename(path) + b'-wc'
            )
            ui.status(
                _(b"initializing svn working copy '%s'\n")
                % os.path.basename(wcpath)
            )
            self.run0(b'checkout', path, wcpath)

            self.wc = wcpath
        self.opener = vfsmod.vfs(self.wc)
        self.wopener = vfsmod.vfs(self.wc)
        self.childmap = mapfile(ui, self.join(b'hg-childmap'))
        if util.checkexec(self.wc):
            self.is_exec = util.isexec
        else:
            self.is_exec = None

        if created:
            prop_actions_allowed = [
                (b'M', b'svn:log'),
                (b'A', b'hg:convert-branch'),
                (b'A', b'hg:convert-rev'),
            ]

            if self.ui.configbool(
                b'convert', b'svn.dangerous-set-commit-dates'
            ):
                prop_actions_allowed.append((b'M', b'svn:date'))

            hook = os.path.join(created, b'hooks', b'pre-revprop-change')
            fp = open(hook, b'wb')
            fp.write(gen_pre_revprop_change_hook(prop_actions_allowed))
            fp.close()
            util.setflags(hook, False, True)

        output = self.run0(b'info')
        self.uuid = self.uuid_re.search(output).group(1).strip()
###END###
def wjoin(self, *names):
        return os.path.join(self.wc, *names)
###END###
def manifest(self):
        # As of svn 1.7, the "add" command fails when receiving
        # already tracked entries, so we have to track and filter them
        # ourselves.
        m = set()
        output = self.run0(b'ls', recursive=True, xml=True)
        doc = xml.dom.minidom.parseString(output)
        for e in doc.getElementsByTagName('entry'):
            for n in e.childNodes:
                if n.nodeType != n.ELEMENT_NODE or n.tagName != 'name':
                    continue
                name = ''.join(
                    c.data for c in n.childNodes if c.nodeType == c.TEXT_NODE
                )
                # Entries are compared with names coming from
                # mercurial, so bytes with undefined encoding. Our
                # best bet is to assume they are in local
                # encoding. They will be passed to command line calls
                # later anyway, so they better be.
                m.add(encoding.unitolocal(name))
                break
        return m
###END###
def putfile(self, filename, flags, data):
        if b'l' in flags:
            self.wopener.symlink(data, filename)
        else:
            try:
                if os.path.islink(self.wjoin(filename)):
                    os.unlink(filename)
            except OSError:
                pass

            if self.is_exec:
                # We need to check executability of the file before the change,
                # because `vfs.write` is able to reset exec bit.
                wasexec = False
                if os.path.exists(self.wjoin(filename)):
                    wasexec = self.is_exec(self.wjoin(filename))

            self.wopener.write(filename, data)

            if self.is_exec:
                if wasexec:
                    if b'x' not in flags:
                        self.delexec.append(filename)
                else:
                    if b'x' in flags:
                        self.setexec.append(filename)
                util.setflags(self.wjoin(filename), False, b'x' in flags)
###END###
def _copyfile(self, source, dest):
        # SVN's copy command pukes if the destination file exists, but
        # our copyfile method expects to record a copy that has
        # already occurred.  Cross the semantic gap.
        wdest = self.wjoin(dest)
        exists = os.path.lexists(wdest)
        if exists:
            fd, tempname = pycompat.mkstemp(
                prefix=b'hg-copy-', dir=os.path.dirname(wdest)
            )
            os.close(fd)
            os.unlink(tempname)
            os.rename(wdest, tempname)
        try:
            self.run0(b'copy', source, dest)
        finally:
            self.manifest.add(dest)
            if exists:
                try:
                    os.unlink(wdest)
                except OSError:
                    pass
                os.rename(tempname, wdest)
###END###
def dirs_of(self, files):
        dirs = set()
        for f in files:
            if os.path.isdir(self.wjoin(f)):
                dirs.add(f)
            i = len(f)
            for i in iter(lambda: f.rfind(b'/', 0, i), -1):
                dirs.add(f[:i])
        return dirs
###END###
def add_dirs(self, files):
        add_dirs = [
            d for d in sorted(self.dirs_of(files)) if d not in self.manifest
        ]
        if add_dirs:
            self.manifest.update(add_dirs)
            self.xargs(add_dirs, b'add', non_recursive=True, quiet=True)
        return add_dirs
###END###
def add_files(self, files):
        files = [f for f in files if f not in self.manifest]
        if files:
            self.manifest.update(files)
            self.xargs(files, b'add', quiet=True)
        return files
###END###
def addchild(self, parent, child):
        self.childmap[parent] = child
###END###
def revid(self, rev):
        return b"svn:%s@%s" % (self.uuid, rev)
###END###
def putcommit(
        self, files, copies, parents, commit, source, revmap, full, cleanp2
    ):
        for parent in parents:
            try:
                return self.revid(self.childmap[parent])
            except KeyError:
                pass

        # Apply changes to working copy
        for f, v in files:
            data, mode = source.getfile(f, v)
            if data is None:
                self.delete.append(f)
            else:
                self.putfile(f, mode, data)
                if f in copies:
                    self.copies.append([copies[f], f])
        if full:
            self.delete.extend(sorted(self.manifest.difference(files)))
        files = [f[0] for f in files]

        entries = set(self.delete)
        files = frozenset(files)
        entries.update(self.add_dirs(files.difference(entries)))
        if self.copies:
            for s, d in self.copies:
                self._copyfile(s, d)
            self.copies = []
        if self.delete:
            self.xargs(self.delete, b'delete')
            for f in self.delete:
                self.manifest.remove(f)
            self.delete = []
        entries.update(self.add_files(files.difference(entries)))
        if self.delexec:
            self.xargs(self.delexec, b'propdel', b'svn:executable')
            self.delexec = []
        if self.setexec:
            self.xargs(self.setexec, b'propset', b'svn:executable', b'*')
            self.setexec = []

        fd, messagefile = pycompat.mkstemp(prefix=b'hg-convert-')
        fp = os.fdopen(fd, 'wb')
        fp.write(util.tonativeeol(commit.desc))
        fp.close()
        try:
            output = self.run0(
                b'commit',
                username=stringutil.shortuser(commit.author),
                file=messagefile,
                encoding=b'utf-8',
            )
            try:
                rev = self.commit_re.search(output).group(1)
            except AttributeError:
                if not files:
                    return parents[0] if parents else b'None'
                self.ui.warn(_(b'unexpected svn output:\n'))
                self.ui.warn(output)
                raise error.Abort(_(b'unable to cope with svn output'))
            if commit.rev:
                self.run(
                    b'propset',
                    b'hg:convert-rev',
                    commit.rev,
                    revprop=True,
                    revision=rev,
                )
            if commit.branch and commit.branch != b'default':
                self.run(
                    b'propset',
                    b'hg:convert-branch',
                    commit.branch,
                    revprop=True,
                    revision=rev,
                )

            if self.ui.configbool(
                b'convert', b'svn.dangerous-set-commit-dates'
            ):
                # Subverson always uses UTC to represent date and time
                date = dateutil.parsedate(commit.date)
                date = (date[0], 0)

                # The only way to set date and time for svn commit is to use propset after commit is done
                self.run(
                    b'propset',
                    b'svn:date',
                    formatsvndate(date),
                    revprop=True,
                    revision=rev,
                )

            for parent in parents:
                self.addchild(parent, rev)
            return self.revid(rev)
        finally:
            os.unlink(messagefile)
###END###
def puttags(self, tags):
        self.ui.warn(_(b'writing Subversion tags is not yet implemented\n'))
        return None, None
###END###
def hascommitfrommap(self, rev):
        # We trust that revisions referenced in a map still is present
        # TODO: implement something better if necessary and feasible
        return True
###END###
def hascommitforsplicemap(self, rev):
        # This is not correct as one can convert to an existing subversion
        # repository and childmap would not list all revisions. Too bad.
        if rev in self.childmap:
            return True
        raise error.Abort(
            _(
                b'splice map revision %s not found in subversion '
                b'child map (revision lookups are not implemented)'
            )
            % rev
        )
###END###
def __init__(self, ui, source, filecount):
        self.ui = ui
        self.source = source
        self.progress = ui.makeprogress(
            _(b'getting files'), unit=_(b'files'), total=filecount
        )
###END###
def getfile(self, file, rev):
        self.progress.increment(item=file)
        return self.source.getfile(file, rev)
###END###
def targetfilebelongstosource(self, targetfilename):
        return self.source.targetfilebelongstosource(targetfilename)
###END###
def lookuprev(self, rev):
        return self.source.lookuprev(rev)
###END###
def close(self):
        self.progress.complete()
###END###
def __init__(self, ui, source, dest, revmapfile, opts):

        self.source = source
        self.dest = dest
        self.ui = ui
        self.opts = opts
        self.commitcache = {}
        self.authors = {}
        self.authorfile = None

        # Record converted revisions persistently: maps source revision
        # ID to target revision ID (both strings).  (This is how
        # incremental conversions work.)
        self.map = mapfile(ui, revmapfile)

        # Read first the dst author map if any
        authorfile = self.dest.authorfile()
        if authorfile and os.path.exists(authorfile):
            self.readauthormap(authorfile)
        # Extend/Override with new author map if necessary
        if opts.get(b'authormap'):
            self.readauthormap(opts.get(b'authormap'))
            self.authorfile = self.dest.authorfile()

        self.splicemap = self.parsesplicemap(opts.get(b'splicemap'))
        self.branchmap = mapfile(ui, opts.get(b'branchmap'))
###END###
def parsesplicemap(self, path):
        """check and validate the splicemap format and
        return a child/parents dictionary.
        Format checking has two parts.
        1. generic format which is same across all source types
        2. specific format checking which may be different for
           different source type.  This logic is implemented in
           checkrevformat function in source files like
           hg.py, subversion.py etc.
        """

        if not path:
            return {}
        m = {}
        try:
            fp = open(path, b'rb')
            for i, line in enumerate(util.iterfile(fp)):
                line = line.splitlines()[0].rstrip()
                if not line:
                    # Ignore blank lines
                    continue
                # split line
                lex = common.shlexer(data=line, whitespace=b',')
                line = list(lex)
                # check number of parents
                if not (2 <= len(line) <= 3):
                    raise error.Abort(
                        _(
                            b'syntax error in %s(%d): child parent1'
                            b'[,parent2] expected'
                        )
                        % (path, i + 1)
                    )
                for part in line:
                    self.source.checkrevformat(part)
                child, p1, p2 = line[0], line[1:2], line[2:]
                if p1 == p2:
                    m[child] = p1
                else:
                    m[child] = p1 + p2
        # if file does not exist or error reading, exit
        except IOError:
            raise error.Abort(
                _(b'splicemap file not found or error reading %s:') % path
            )
        return m
###END###
def walktree(self, heads):
        """Return a mapping that identifies the uncommitted parents of every
        uncommitted changeset."""
        visit = list(heads)
        known = set()
        parents = {}
        numcommits = self.source.numcommits()
        progress = self.ui.makeprogress(
            _(b'scanning'), unit=_(b'revisions'), total=numcommits
        )
        while visit:
            n = visit.pop(0)
            if n in known:
                continue
            if n in self.map:
                m = self.map[n]
                if m == SKIPREV or self.dest.hascommitfrommap(m):
                    continue
            known.add(n)
            progress.update(len(known))
            commit = self.cachecommit(n)
            parents[n] = []
            for p in commit.parents:
                parents[n].append(p)
                visit.append(p)
        progress.complete()

        return parents
###END###
def mergesplicemap(self, parents, splicemap):
        """A splicemap redefines child/parent relationships. Check the
        map contains valid revision identifiers and merge the new
        links in the source graph.
        """
        for c in sorted(splicemap):
            if c not in parents:
                if not self.dest.hascommitforsplicemap(self.map.get(c, c)):
                    # Could be in source but not converted during this run
                    self.ui.warn(
                        _(
                            b'splice map revision %s is not being '
                            b'converted, ignoring\n'
                        )
                        % c
                    )
                continue
            pc = []
            for p in splicemap[c]:
                # We do not have to wait for nodes already in dest.
                if self.dest.hascommitforsplicemap(self.map.get(p, p)):
                    continue
                # Parent is not in dest and not being converted, not good
                if p not in parents:
                    raise error.Abort(_(b'unknown splice map parent: %s') % p)
                pc.append(p)
            parents[c] = pc
###END###
def toposort(self, parents, sortmode):
        """Return an ordering such that every uncommitted changeset is
        preceded by all its uncommitted ancestors."""

        def mapchildren(parents):
            """Return a (children, roots) tuple where 'children' maps parent
            revision identifiers to children ones, and 'roots' is the list of
            revisions without parents. 'parents' must be a mapping of revision
            identifier to its parents ones.
            """
            visit = collections.deque(sorted(parents))
            seen = set()
            children = {}
            roots = []

            while visit:
                n = visit.popleft()
                if n in seen:
                    continue
                seen.add(n)
                # Ensure that nodes without parents are present in the
                # 'children' mapping.
                children.setdefault(n, [])
                hasparent = False
                for p in parents[n]:
                    if p not in self.map:
                        visit.append(p)
                        hasparent = True
                    children.setdefault(p, []).append(n)
                if not hasparent:
                    roots.append(n)

            return children, roots

        # Sort functions are supposed to take a list of revisions which
        # can be converted immediately and pick one

        def makebranchsorter():
            """If the previously converted revision has a child in the
            eligible revisions list, pick it. Return the list head
            otherwise. Branch sort attempts to minimize branch
            switching, which is harmful for Mercurial backend
            compression.
            """
            prev = [None]

            def picknext(nodes):
                next = nodes[0]
                for n in nodes:
                    if prev[0] in parents[n]:
                        next = n
                        break
                prev[0] = next
                return next

            return picknext

        def makesourcesorter():
            """Source specific sort."""
            keyfn = lambda n: self.commitcache[n].sortkey

            def picknext(nodes):
                return sorted(nodes, key=keyfn)[0]

            return picknext

        def makeclosesorter():
            """Close order sort."""
            keyfn = lambda n: (
                b'close' not in self.commitcache[n].extra,
                self.commitcache[n].sortkey,
            )

            def picknext(nodes):
                return sorted(nodes, key=keyfn)[0]

            return picknext

        def makedatesorter():
            """Sort revisions by date."""
            dates = {}

            def getdate(n):
                if n not in dates:
                    dates[n] = dateutil.parsedate(self.commitcache[n].date)
                return dates[n]

            def picknext(nodes):
                return min([(getdate(n), n) for n in nodes])[1]

            return picknext

        if sortmode == b'branchsort':
            picknext = makebranchsorter()
        elif sortmode == b'datesort':
            picknext = makedatesorter()
        elif sortmode == b'sourcesort':
            picknext = makesourcesorter()
        elif sortmode == b'closesort':
            picknext = makeclosesorter()
        else:
            raise error.Abort(_(b'unknown sort mode: %s') % sortmode)

        children, actives = mapchildren(parents)

        s = []
        pendings = {}
        while actives:
            n = picknext(actives)
            actives.remove(n)
            s.append(n)

            # Update dependents list
            for c in children.get(n, []):
                if c not in pendings:
                    pendings[c] = [p for p in parents[c] if p not in self.map]
                try:
                    pendings[c].remove(n)
                except ValueError:
                    raise error.Abort(
                        _(b'cycle detected between %s and %s')
                        % (recode(c), recode(n))
                    )
                if not pendings[c]:
                    # Parents are converted, node is eligible
                    actives.insert(0, c)
                    pendings[c] = None

        if len(s) != len(parents):
            raise error.Abort(_(b"not all revisions were sorted"))

        return s
###END###
def writeauthormap(self):
        authorfile = self.authorfile
        if authorfile:
            self.ui.status(_(b'writing author map file %s\n') % authorfile)
            ofile = open(authorfile, b'wb+')
            for author in self.authors:
                ofile.write(
                    util.tonativeeol(
                        b"%s=%s\n" % (author, self.authors[author])
                    )
                )
            ofile.close()
###END###
def readauthormap(self, authorfile):
        self.authors = readauthormap(self.ui, authorfile, self.authors)
###END###
def cachecommit(self, rev):
        commit = self.source.getcommit(rev)
        commit.author = self.authors.get(commit.author, commit.author)
        commit.branch = mapbranch(commit.branch, self.branchmap)
        self.commitcache[rev] = commit
        return commit
###END###
def copy(self, rev):
        commit = self.commitcache[rev]
        full = self.opts.get(b'full')
        changes = self.source.getchanges(rev, full)
        if isinstance(changes, bytes):
            if changes == SKIPREV:
                dest = SKIPREV
            else:
                dest = self.map[changes]
            self.map[rev] = dest
            return
        files, copies, cleanp2 = changes
        pbranches = []
        if commit.parents:
            for prev in commit.parents:
                if prev not in self.commitcache:
                    self.cachecommit(prev)
                pbranches.append(
                    (self.map[prev], self.commitcache[prev].branch)
                )
        self.dest.setbranch(commit.branch, pbranches)
        try:
            parents = self.splicemap[rev]
            self.ui.status(
                _(b'spliced in %s as parents of %s\n')
                % (_(b' and ').join(parents), rev)
            )
            parents = [self.map.get(p, p) for p in parents]
        except KeyError:
            parents = [b[0] for b in pbranches]
            parents.extend(
                self.map[x] for x in commit.optparents if x in self.map
            )
        if len(pbranches) != 2:
            cleanp2 = set()
        if len(parents) < 3:
            source = progresssource(self.ui, self.source, len(files))
        else:
            # For an octopus merge, we end up traversing the list of
            # changed files N-1 times. This tweak to the number of
            # files makes it so the progress bar doesn't overflow
            # itself.
            source = progresssource(
                self.ui, self.source, len(files) * (len(parents) - 1)
            )
        newnode = self.dest.putcommit(
            files, copies, parents, commit, source, self.map, full, cleanp2
        )
        source.close()
        self.source.converted(rev, newnode)
        self.map[rev] = newnode
###END###
def convert(self, sortmode):
        try:
            self.source.before()
            self.dest.before()
            self.source.setrevmap(self.map)
            self.ui.status(_(b"scanning source...\n"))
            heads = self.source.getheads()
            parents = self.walktree(heads)
            self.mergesplicemap(parents, self.splicemap)
            self.ui.status(_(b"sorting...\n"))
            t = self.toposort(parents, sortmode)
            num = len(t)
            c = None

            self.ui.status(_(b"converting...\n"))
            progress = self.ui.makeprogress(
                _(b'converting'), unit=_(b'revisions'), total=len(t)
            )
            for i, c in enumerate(t):
                num -= 1
                desc = self.commitcache[c].desc
                if b"\n" in desc:
                    desc = desc.splitlines()[0]
                # convert log message to local encoding without using
                # tolocal() because the encoding.encoding convert()
                # uses is 'utf-8'
                self.ui.status(b"%d %s\n" % (num, recode(desc)))
                self.ui.note(_(b"source: %s\n") % recode(c))
                progress.update(i)
                self.copy(c)
            progress.complete()

            if not self.ui.configbool(b'convert', b'skiptags'):
                tags = self.source.gettags()
                ctags = {}
                for k in tags:
                    v = tags[k]
                    if self.map.get(v, SKIPREV) != SKIPREV:
                        ctags[k] = self.map[v]

                if c and ctags:
                    nrev, tagsparent = self.dest.puttags(ctags)
                    if nrev and tagsparent:
                        # write another hash correspondence to override the
                        # previous one so we don't end up with extra tag heads
                        tagsparents = [
                            e
                            for e in pycompat.iteritems(self.map)
                            if e[1] == tagsparent
                        ]
                        if tagsparents:
                            self.map[tagsparents[0][0]] = nrev

            bookmarks = self.source.getbookmarks()
            cbookmarks = {}
            for k in bookmarks:
                v = bookmarks[k]
                if self.map.get(v, SKIPREV) != SKIPREV:
                    cbookmarks[k] = self.map[v]

            if c and cbookmarks:
                self.dest.putbookmarks(cbookmarks)

            self.writeauthormap()
        finally:
            self.cleanup()
###END###
def cleanup(self):
        try:
            self.dest.after()
        finally:
            self.source.after()
        self.map.close()
###END###
def __init__(self, ui, path=None):
        self.ui = ui
        self.include = {}
        self.exclude = {}
        self.rename = {}
        self.targetprefixes = None
        if path:
            if self.parse(path):
                raise error.Abort(_(b'errors in filemap'))
###END###
def parse(self, path):
        errs = 0

        def check(name, mapping, listname):
            if not name:
                self.ui.warn(
                    _(b'%s:%d: path to %s is missing\n')
                    % (lex.infile, lex.lineno, listname)
                )
                return 1
            if name in mapping:
                self.ui.warn(
                    _(b'%s:%d: %r already in %s list\n')
                    % (lex.infile, lex.lineno, name, listname)
                )
                return 1
            if name.startswith(b'/') or name.endswith(b'/') or b'//' in name:
                self.ui.warn(
                    _(b'%s:%d: superfluous / in %s %r\n')
                    % (lex.infile, lex.lineno, listname, pycompat.bytestr(name))
                )
                return 1
            return 0

        lex = common.shlexer(
            filepath=path, wordchars=b'!@#$%^&*()-=+[]{}|;:,./<>?'
        )
        cmd = lex.get_token()
        while cmd:
            if cmd == b'include':
                name = normalize(lex.get_token())
                errs += check(name, self.exclude, b'exclude')
                self.include[name] = name
            elif cmd == b'exclude':
                name = normalize(lex.get_token())
                errs += check(name, self.include, b'include')
                errs += check(name, self.rename, b'rename')
                self.exclude[name] = name
            elif cmd == b'rename':
                src = normalize(lex.get_token())
                dest = normalize(lex.get_token())
                errs += check(src, self.exclude, b'exclude')
                self.rename[src] = dest
            elif cmd == b'source':
                errs += self.parse(normalize(lex.get_token()))
            else:
                self.ui.warn(
                    _(b'%s:%d: unknown directive %r\n')
                    % (lex.infile, lex.lineno, pycompat.bytestr(cmd))
                )
                errs += 1
            cmd = lex.get_token()
        return errs
###END###
def lookup(self, name, mapping):
        name = normalize(name)
        for pre, suf in rpairs(name):
            try:
                return mapping[pre], pre, suf
            except KeyError:
                pass
        return b'', name, b''
###END###
def istargetfile(self, filename):
        """Return true if the given target filename is covered as a destination
        of the filemap. This is useful for identifying what parts of the target
        repo belong to the source repo and what parts don't."""
        if self.targetprefixes is None:
            self.targetprefixes = set()
            for before, after in pycompat.iteritems(self.rename):
                self.targetprefixes.add(after)

        # If "." is a target, then all target files are considered from the
        # source.
        if not self.targetprefixes or b'.' in self.targetprefixes:
            return True

        filename = normalize(filename)
        for pre, suf in rpairs(filename):
            # This check is imperfect since it doesn't account for the
            # include/exclude list, but it should work in filemaps that don't
            # apply include/exclude to the same source directories they are
            # renaming.
            if pre in self.targetprefixes:
                return True
        return False
###END###
def __call__(self, name):
        if self.include:
            inc = self.lookup(name, self.include)[0]
        else:
            inc = name
        if self.exclude:
            exc = self.lookup(name, self.exclude)[0]
        else:
            exc = b''
        if (not self.include and exc) or (len(inc) <= len(exc)):
            return None
        newpre, pre, suf = self.lookup(name, self.rename)
        if newpre:
            if newpre == b'.':
                return suf
            if suf:
                if newpre.endswith(b'/'):
                    return newpre + suf
                return newpre + b'/' + suf
            return newpre
        return name
###END###
def active(self):
        return bool(self.include or self.exclude or self.rename)
###END###
def __init__(self, ui, baseconverter, filemap):
        super(filemap_source, self).__init__(ui, baseconverter.repotype)
        self.base = baseconverter
        self.filemapper = filemapper(ui, filemap)
        self.commits = {}
        # if a revision rev has parent p in the original revision graph, then
        # rev will have parent self.parentmap[p] in the restricted graph.
        self.parentmap = {}
        # self.wantedancestors[rev] is the set of all ancestors of rev that
        # are in the restricted graph.
        self.wantedancestors = {}
        self.convertedorder = None
        self._rebuilt = False
        self.origparents = {}
        self.children = {}
        self.seenchildren = {}
        # experimental config: convert.ignoreancestorcheck
        self.ignoreancestorcheck = self.ui.configbool(
            b'convert', b'ignoreancestorcheck'
        )
###END###
def before(self):
        self.base.before()
###END###
def after(self):
        self.base.after()
###END###
def setrevmap(self, revmap):
        # rebuild our state to make things restartable
        #
        # To avoid calling getcommit for every revision that has already
        # been converted, we rebuild only the parentmap, delaying the
        # rebuild of wantedancestors until we need it (i.e. until a
        # merge).
        #
        # We assume the order argument lists the revisions in
        # topological order, so that we can infer which revisions were
        # wanted by previous runs.
        self._rebuilt = not revmap
        seen = {SKIPREV: SKIPREV}
        dummyset = set()
        converted = []
        for rev in revmap.order:
            mapped = revmap[rev]
            wanted = mapped not in seen
            if wanted:
                seen[mapped] = rev
                self.parentmap[rev] = rev
            else:
                self.parentmap[rev] = seen[mapped]
            self.wantedancestors[rev] = dummyset
            arg = seen[mapped]
            if arg == SKIPREV:
                arg = None
            converted.append((rev, wanted, arg))
        self.convertedorder = converted
        return self.base.setrevmap(revmap)
###END###
def rebuild(self):
        if self._rebuilt:
            return True
        self._rebuilt = True
        self.parentmap.clear()
        self.wantedancestors.clear()
        self.seenchildren.clear()
        for rev, wanted, arg in self.convertedorder:
            if rev not in self.origparents:
                try:
                    self.origparents[rev] = self.getcommit(rev).parents
                except error.RepoLookupError:
                    self.ui.debug(b"unknown revmap source: %s\n" % rev)
                    continue
            if arg is not None:
                self.children[arg] = self.children.get(arg, 0) + 1

        for rev, wanted, arg in self.convertedorder:
            try:
                parents = self.origparents[rev]
            except KeyError:
                continue  # unknown revmap source
            if wanted:
                self.mark_wanted(rev, parents)
            else:
                self.mark_not_wanted(rev, arg)
            self._discard(arg, *parents)

        return True
###END###
def getheads(self):
        return self.base.getheads()
###END###
def getcommit(self, rev):
        # We want to save a reference to the commit objects to be able
        # to rewrite their parents later on.
        c = self.commits[rev] = self.base.getcommit(rev)
        for p in c.parents:
            self.children[p] = self.children.get(p, 0) + 1
        return c
###END###
def numcommits(self):
        return self.base.numcommits()
###END###
def _cachedcommit(self, rev):
        if rev in self.commits:
            return self.commits[rev]
        return self.base.getcommit(rev)
###END###
def _discard(self, *revs):
        for r in revs:
            if r is None:
                continue
            self.seenchildren[r] = self.seenchildren.get(r, 0) + 1
            if self.seenchildren[r] == self.children[r]:
                self.wantedancestors.pop(r, None)
                self.parentmap.pop(r, None)
                del self.seenchildren[r]
                if self._rebuilt:
                    del self.children[r]
###END###
def wanted(self, rev, i):
        # Return True if we're directly interested in rev.
        #
        # i is an index selecting one of the parents of rev (if rev
        # has no parents, i is None).  getchangedfiles will give us
        # the list of files that are different in rev and in the parent
        # indicated by i.  If we're interested in any of these files,
        # we're interested in rev.
        try:
            files = self.base.getchangedfiles(rev, i)
        except NotImplementedError:
            raise error.Abort(_(b"source repository doesn't support --filemap"))
        for f in files:
            if self.filemapper(f):
                return True

        # The include directive is documented to include nothing else (though
        # valid branch closes are included).
        if self.filemapper.include:
            return False

        # Allow empty commits in the source revision through.  The getchanges()
        # method doesn't even bother calling this if it determines that the
        # close marker is significant (i.e. all of the branch ancestors weren't
        # eliminated).  Therefore if there *is* a close marker, getchanges()
        # doesn't consider it significant, and this revision should be dropped.
        return not files and b'close' not in self.commits[rev].extra
###END###
def mark_not_wanted(self, rev, p):
        # Mark rev as not interesting and update data structures.

        if p is None:
            # A root revision. Use SKIPREV to indicate that it doesn't
            # map to any revision in the restricted graph.  Put SKIPREV
            # in the set of wanted ancestors to simplify code elsewhere
            self.parentmap[rev] = SKIPREV
            self.wantedancestors[rev] = {SKIPREV}
            return

        # Reuse the data from our parent.
        self.parentmap[rev] = self.parentmap[p]
        self.wantedancestors[rev] = self.wantedancestors[p]
###END###
def mark_wanted(self, rev, parents):
        # Mark rev ss wanted and update data structures.

        # rev will be in the restricted graph, so children of rev in
        # the original graph should still have rev as a parent in the
        # restricted graph.
        self.parentmap[rev] = rev

        # The set of wanted ancestors of rev is the union of the sets
        # of wanted ancestors of its parents. Plus rev itself.
        wrev = set()
        for p in parents:
            if p in self.wantedancestors:
                wrev.update(self.wantedancestors[p])
            else:
                self.ui.warn(
                    _(b'warning: %s parent %s is missing\n') % (rev, p)
                )
        wrev.add(rev)
        self.wantedancestors[rev] = wrev
###END###
def getchanges(self, rev, full):
        parents = self.commits[rev].parents
        if len(parents) > 1 and not self.ignoreancestorcheck:
            self.rebuild()

        # To decide whether we're interested in rev we:
        #
        # - calculate what parents rev will have if it turns out we're
        #   interested in it.  If it's going to have more than 1 parent,
        #   we're interested in it.
        #
        # - otherwise, we'll compare it with the single parent we found.
        #   If any of the files we're interested in is different in the
        #   the two revisions, we're interested in rev.

        # A parent p is interesting if its mapped version (self.parentmap[p]):
        # - is not SKIPREV
        # - is still not in the list of parents (we don't want duplicates)
        # - is not an ancestor of the mapped versions of the other parents or
        #   there is no parent in the same branch than the current revision.
        mparents = []
        knownparents = set()
        branch = self.commits[rev].branch
        hasbranchparent = False
        for i, p1 in enumerate(parents):
            mp1 = self.parentmap[p1]
            if mp1 == SKIPREV or mp1 in knownparents:
                continue

            isancestor = not self.ignoreancestorcheck and any(
                p2
                for p2 in parents
                if p1 != p2
                and mp1 != self.parentmap[p2]
                and mp1 in self.wantedancestors[p2]
            )
            if not isancestor and not hasbranchparent and len(parents) > 1:
                # This could be expensive, avoid unnecessary calls.
                if self._cachedcommit(p1).branch == branch:
                    hasbranchparent = True
            mparents.append((p1, mp1, i, isancestor))
            knownparents.add(mp1)
        # Discard parents ancestors of other parents if there is a
        # non-ancestor one on the same branch than current revision.
        if hasbranchparent:
            mparents = [p for p in mparents if not p[3]]
        wp = None
        if mparents:
            wp = max(p[2] for p in mparents)
            mparents = [p[1] for p in mparents]
        elif parents:
            wp = 0

        self.origparents[rev] = parents

        closed = False
        if b'close' in self.commits[rev].extra:
            # A branch closing revision is only useful if one of its
            # parents belong to the branch being closed
            pbranches = [self._cachedcommit(p).branch for p in mparents]
            if branch in pbranches:
                closed = True

        if len(mparents) < 2 and not closed and not self.wanted(rev, wp):
            # We don't want this revision.
            # Update our state and tell the convert process to map this
            # revision to the same revision its parent as mapped to.
            p = None
            if parents:
                p = parents[wp]
            self.mark_not_wanted(rev, p)
            self.convertedorder.append((rev, False, p))
            self._discard(*parents)
            return self.parentmap[rev]

        # We want this revision.
        # Rewrite the parents of the commit object
        self.commits[rev].parents = mparents
        self.mark_wanted(rev, parents)
        self.convertedorder.append((rev, True, None))
        self._discard(*parents)

        # Get the real changes and do the filtering/mapping. To be
        # able to get the files later on in getfile, we hide the
        # original filename in the rev part of the return value.
        changes, copies, cleanp2 = self.base.getchanges(rev, full)
        files = {}
        ncleanp2 = set(cleanp2)
        for f, r in changes:
            newf = self.filemapper(f)
            if newf and (newf != f or newf not in files):
                files[newf] = (f, r)
                if newf != f:
                    ncleanp2.discard(f)
        files = sorted(files.items())

        ncopies = {}
        for c in copies:
            newc = self.filemapper(c)
            if newc:
                newsource = self.filemapper(copies[c])
                if newsource:
                    ncopies[newc] = newsource

        return files, ncopies, ncleanp2
###END###
def targetfilebelongstosource(self, targetfilename):
        return self.filemapper.istargetfile(targetfilename)
###END###
def getfile(self, name, rev):
        realname, realrev = rev
        return self.base.getfile(realname, realrev)
###END###
def gettags(self):
        return self.base.gettags()
###END###
def hasnativeorder(self):
        return self.base.hasnativeorder()
###END###
def lookuprev(self, rev):
        return self.base.lookuprev(rev)
###END###
def getbookmarks(self):
        return self.base.getbookmarks()
###END###
def converted(self, rev, sinkrev):
        self.base.converted(rev, sinkrev)
###END###
def __init__(self, l):
        self._l = l
###END###
def __iter__(self):
        return (_encodeornone(v) for v in self._l)
###END###
def get_token(self):
        return _encodeornone(self._l.get_token())
###END###
def infile(self):
        return self._l.infile or b'<unknown>'
###END###
def lineno(self):
        return self._l.lineno
###END###
def __init__(
        self,
        author,
        date,
        desc,
        parents,
        branch=None,
        rev=None,
        extra=None,
        sortkey=None,
        saverev=True,
        phase=phases.draft,
        optparents=None,
        ctx=None,
    ):
        self.author = author or b'unknown'
        self.date = date or b'0 0'
        self.desc = desc
        self.parents = parents  # will be converted and used as parents
        self.optparents = optparents or []  # will be used if already converted
        self.branch = branch
        self.rev = rev
        self.extra = extra or {}
        self.sortkey = sortkey
        self.saverev = saverev
        self.phase = phase
        self.ctx = ctx
###END###
def __init__(self, ui, repotype, path=None, revs=None):
        """Initialize conversion source (or raise NoRepo("message")
        exception if path is not a valid repository)"""
        self.ui = ui
        self.path = path
        self.revs = revs
        self.repotype = repotype

        self.encoding = b'utf-8'
###END###
def checkhexformat(self, revstr, mapname=b'splicemap'):
        """fails if revstr is not a 40 byte hex. mercurial and git both uses
        such format for their revision numbering
        """
        if not re.match(br'[0-9a-fA-F]{40,40}$', revstr):
            raise error.Abort(
                _(b'%s entry %s is not a valid revision identifier')
                % (mapname, revstr)
            )
###END###
def before(self):
        pass
###END###
def after(self):
        pass
###END###
def targetfilebelongstosource(self, targetfilename):
        """Returns true if the given targetfile belongs to the source repo. This
        is useful when only a subdirectory of the target belongs to the source
        repo."""
        # For normal full repo converts, this is always True.
        return True
###END###
def setrevmap(self, revmap):
        """set the map of already-converted revisions"""
###END###
def getheads(self):
        """Return a list of this repository's heads"""
        raise NotImplementedError
###END###
def getfile(self, name, rev):
        """Return a pair (data, mode) where data is the file content
        as a string and mode one of '', 'x' or 'l'. rev is the
        identifier returned by a previous call to getchanges().
        Data is None if file is missing/deleted in rev.
        """
        raise NotImplementedError
###END###
def getchanges(self, version, full):
        """Returns a tuple of (files, copies, cleanp2).

        files is a sorted list of (filename, id) tuples for all files
        changed between version and its first parent returned by
        getcommit(). If full, all files in that revision is returned.
        id is the source revision id of the file.

        copies is a dictionary of dest: source

        cleanp2 is the set of files filenames that are clean against p2.
        (Files that are clean against p1 are already not in files (unless
        full). This makes it possible to handle p2 clean files similarly.)
        """
        raise NotImplementedError
###END###
def getcommit(self, version):
        """Return the commit object for version"""
        raise NotImplementedError
###END###
def numcommits(self):
        """Return the number of commits in this source.

        If unknown, return None.
        """
        return None
###END###
def gettags(self):
        """Return the tags as a dictionary of name: revision

        Tag names must be UTF-8 strings.
        """
        raise NotImplementedError
###END###
def recode(self, s, encoding=None):
        if not encoding:
            encoding = self.encoding or b'utf-8'

        if isinstance(s, pycompat.unicode):
            return s.encode("utf-8")
        try:
            return s.decode(pycompat.sysstr(encoding)).encode("utf-8")
        except UnicodeError:
            try:
                return s.decode("latin-1").encode("utf-8")
            except UnicodeError:
                return s.decode(pycompat.sysstr(encoding), "replace").encode(
                    "utf-8"
                )
###END###
def getchangedfiles(self, rev, i):
        """Return the files changed by rev compared to parent[i].

        i is an index selecting one of the parents of rev.  The return
        value should be the list of files that are different in rev and
        this parent.

        If rev has no parents, i is None.

        This function is only needed to support --filemap
        """
        raise NotImplementedError
###END###
def converted(self, rev, sinkrev):
        '''Notify the source that a revision has been converted.'''
###END###
def hasnativeorder(self):
        """Return true if this source has a meaningful, native revision
        order. For instance, Mercurial revisions are store sequentially
        while there is no such global ordering with Darcs.
        """
        return False
###END###
def hasnativeclose(self):
        """Return true if this source has ability to close branch."""
        return False
###END###
def lookuprev(self, rev):
        """If rev is a meaningful revision reference in source, return
        the referenced identifier in the same format used by getcommit().
        return None otherwise.
        """
        return None
###END###
def getbookmarks(self):
        """Return the bookmarks as a dictionary of name: revision

        Bookmark names are to be UTF-8 strings.
        """
        return {}
###END###
def checkrevformat(self, revstr, mapname=b'splicemap'):
        """revstr is a string that describes a revision in the given
        source control system.  Return true if revstr has correct
        format.
        """
        return True
###END###
def __init__(self, ui, repotype, path):
        """Initialize conversion sink (or raise NoRepo("message")
        exception if path is not a valid repository)

        created is a list of paths to remove if a fatal error occurs
        later"""
        self.ui = ui
        self.path = path
        self.created = []
        self.repotype = repotype
###END###
def revmapfile(self):
        """Path to a file that will contain lines
        source_rev_id sink_rev_id
        mapping equivalent revision identifiers for each system."""
        raise NotImplementedError
###END###
def authorfile(self):
        """Path to a file that will contain lines
        srcauthor=dstauthor
        mapping equivalent authors identifiers for each system."""
        return None
###END###
def putcommit(
        self, files, copies, parents, commit, source, revmap, full, cleanp2
    ):
        """Create a revision with all changed files listed in 'files'
        and having listed parents. 'commit' is a commit object
        containing at a minimum the author, date, and message for this
        changeset.  'files' is a list of (path, version) tuples,
        'copies' is a dictionary mapping destinations to sources,
        'source' is the source repository, and 'revmap' is a mapfile
        of source revisions to converted revisions. Only getfile() and
        lookuprev() should be called on 'source'. 'full' means that 'files'
        is complete and all other files should be removed.
        'cleanp2' is a set of the filenames that are unchanged from p2
        (only in the common merge case where there two parents).

        Note that the sink repository is not told to update itself to
        a particular revision (or even what that revision would be)
        before it receives the file data.
        """
        raise NotImplementedError
###END###
def puttags(self, tags):
        """Put tags into sink.

        tags: {tagname: sink_rev_id, ...} where tagname is an UTF-8 string.
        Return a pair (tag_revision, tag_parent_revision), or (None, None)
        if nothing was changed.
        """
        raise NotImplementedError
###END###
def setbranch(self, branch, pbranches):
        """Set the current branch name. Called before the first putcommit
        on the branch.
        branch: branch name for subsequent commits
        pbranches: (converted parent revision, parent branch) tuples"""
###END###
def setfilemapmode(self, active):
        """Tell the destination that we're using a filemap

        Some converter_sources (svn in particular) can claim that a file
        was changed in a revision, even if there was no change.  This method
        tells the destination that we're using a filemap and that it should
        filter empty revisions.
        """
###END###
def before(self):
        pass
###END###
def after(self):
        pass
###END###
def putbookmarks(self, bookmarks):
        """Put bookmarks into sink.

        bookmarks: {bookmarkname: sink_rev_id, ...}
        where bookmarkname is an UTF-8 string.
        """
###END###
def hascommitfrommap(self, rev):
        """Return False if a rev mentioned in a filemap is known to not be
        present."""
        raise NotImplementedError
###END###
def hascommitforsplicemap(self, rev):
        """This method is for the special needs for splicemap handling and not
        for general use. Returns True if the sink contains rev, aborts on some
        special cases."""
        raise NotImplementedError
###END###
def __init__(self, ui, command):
        self.ui = ui
        self.command = command
###END###
def prerun(self):
        pass
###END###
def postrun(self):
        pass
###END###
def _cmdline(self, cmd, *args, **kwargs):
        kwargs = pycompat.byteskwargs(kwargs)
        cmdline = [self.command, cmd] + list(args)
        for k, v in pycompat.iteritems(kwargs):
            if len(k) == 1:
                cmdline.append(b'-' + k)
            else:
                cmdline.append(b'--' + k.replace(b'_', b'-'))
            try:
                if len(k) == 1:
                    cmdline.append(b'' + v)
                else:
                    cmdline[-1] += b'=' + v
            except TypeError:
                pass
        cmdline = [procutil.shellquote(arg) for arg in cmdline]
        if not self.ui.debugflag:
            cmdline += [b'2>', pycompat.bytestr(os.devnull)]
        cmdline = b' '.join(cmdline)
        return cmdline
###END###
def _run(self, cmd, *args, **kwargs):
        def popen(cmdline):
            p = subprocess.Popen(
                procutil.tonativestr(cmdline),
                shell=True,
                bufsize=-1,
                close_fds=procutil.closefds,
                stdout=subprocess.PIPE,
            )
            return p

        return self._dorun(popen, cmd, *args, **kwargs)
###END###
def _run2(self, cmd, *args, **kwargs):
        return self._dorun(procutil.popen2, cmd, *args, **kwargs)
###END###
def _run3(self, cmd, *args, **kwargs):
        return self._dorun(procutil.popen3, cmd, *args, **kwargs)
###END###
def _dorun(self, openfunc, cmd, *args, **kwargs):
        cmdline = self._cmdline(cmd, *args, **kwargs)
        self.ui.debug(b'running: %s\n' % (cmdline,))
        self.prerun()
        try:
            return openfunc(cmdline)
        finally:
            self.postrun()
###END###
def run(self, cmd, *args, **kwargs):
        p = self._run(cmd, *args, **kwargs)
        output = p.communicate()[0]
        self.ui.debug(output)
        return output, p.returncode
###END###
def runlines(self, cmd, *args, **kwargs):
        p = self._run(cmd, *args, **kwargs)
        output = p.stdout.readlines()
        p.wait()
        self.ui.debug(b''.join(output))
        return output, p.returncode
###END###
def checkexit(self, status, output=b''):
        if status:
            if output:
                self.ui.warn(_(b'%s error:\n') % self.command)
                self.ui.warn(output)
            msg = procutil.explainexit(status)
            raise error.Abort(b'%s %s' % (self.command, msg))
###END###
def run0(self, cmd, *args, **kwargs):
        output, status = self.run(cmd, *args, **kwargs)
        self.checkexit(status, output)
        return output
###END###
def runlines0(self, cmd, *args, **kwargs):
        output, status = self.runlines(cmd, *args, **kwargs)
        self.checkexit(status, b''.join(output))
        return output
###END###
def argmax(self):
        # POSIX requires at least 4096 bytes for ARG_MAX
        argmax = 4096
        try:
            argmax = os.sysconf("SC_ARG_MAX")
        except (AttributeError, ValueError):
            pass

        # Windows shells impose their own limits on command line length,
        # down to 2047 bytes for cmd.exe under Windows NT/2k and 2500 bytes
        # for older 4nt.exe. See http://support.microsoft.com/kb/830473 for
        # details about cmd.exe limitations.

        # Since ARG_MAX is for command line _and_ environment, lower our limit
        # (and make happy Windows shells while doing this).
        return argmax // 2 - 1
###END###
def _limit_arglist(self, arglist, cmd, *args, **kwargs):
        cmdlen = len(self._cmdline(cmd, *args, **kwargs))
        limit = self.argmax - cmdlen
        numbytes = 0
        fl = []
        for fn in arglist:
            b = len(fn) + 3
            if numbytes + b < limit or len(fl) == 0:
                fl.append(fn)
                numbytes += b
            else:
                yield fl
                fl = [fn]
                numbytes = b
        if fl:
            yield fl
###END###
def xargs(self, arglist, cmd, *args, **kwargs):
        for l in self._limit_arglist(arglist, cmd, *args, **kwargs):
            self.run0(cmd, *(list(args) + l), **kwargs)
###END###
def __init__(self, ui, path):
        super(mapfile, self).__init__()
        self.ui = ui
        self.path = path
        self.fp = None
        self.order = []
        self._read()
###END###
def _read(self):
        if not self.path:
            return
        try:
            fp = open(self.path, b'rb')
        except IOError as err:
            if err.errno != errno.ENOENT:
                raise
            return
        for i, line in enumerate(util.iterfile(fp)):
            line = line.splitlines()[0].rstrip()
            if not line:
                # Ignore blank lines
                continue
            try:
                key, value = line.rsplit(b' ', 1)
            except ValueError:
                raise error.Abort(
                    _(b'syntax error in %s(%d): key/value pair expected')
                    % (self.path, i + 1)
                )
            if key not in self:
                self.order.append(key)
            super(mapfile, self).__setitem__(key, value)
        fp.close()
###END###
def __setitem__(self, key, value):
        if self.fp is None:
            try:
                self.fp = open(self.path, b'ab')
            except IOError as err:
                raise error.Abort(
                    _(b'could not open map file %r: %s')
                    % (self.path, encoding.strtolocal(err.strerror))
                )
        self.fp.write(util.tonativeeol(b'%s %s\n' % (key, value)))
        self.fp.flush()
        super(mapfile, self).__setitem__(key, value)
###END###
def close(self):
        if self.fp:
            self.fp.close()
            self.fp = None
###END###
def __init__(self, url=b"", ra=None):
        self.pool = Pool()
        self.svn_url = url
        self.username = b''
        self.password = b''

        # Only Subversion 1.4 has reparent()
        if ra is None or not util.safehasattr(svn.ra, b'reparent'):
            self.client = svn.client.create_context(self.pool)
            ab = _create_auth_baton(self.pool)
            self.client.auth_baton = ab
            global svn_config
            if svn_config is None:
                svn_config = svn.core.svn_config_get_config(None)
            self.client.config = svn_config
            try:
                self.ra = svn.client.open_ra_session(
                    self.svn_url, self.client, self.pool
                )
            except SubversionException as xxx_todo_changeme:
                (inst, num) = xxx_todo_changeme.args
                if num in (
                    svn.core.SVN_ERR_RA_ILLEGAL_URL,
                    svn.core.SVN_ERR_RA_LOCAL_REPOS_OPEN_FAILED,
                    svn.core.SVN_ERR_BAD_URL,
                ):
                    raise NotBranchError(url)
                raise
        else:
            self.ra = ra
            svn.ra.reparent(self.ra, self.svn_url.encode('utf8'))
###END###
def do_update(self, revnum, path, *args, **kwargs):
        return self.Reporter(
            svn.ra.do_update(self.ra, revnum, path, *args, **kwargs)
        )
###END###
def __init__(self, reporter_data):
            self._reporter, self._baton = reporter_data
###END###
def set_path(self, path, revnum, start_empty, lock_token, pool=None):
            svn.ra.reporter2_invoke_set_path(
                self._reporter,
                self._baton,
                path,
                revnum,
                start_empty,
                lock_token,
                pool,
            )
###END###
def delete_path(self, path, pool=None):
            svn.ra.reporter2_invoke_delete_path(
                self._reporter, self._baton, path, pool
            )
###END###
def link_path(
            self, path, url, revision, start_empty, lock_token, pool=None
        ):
            svn.ra.reporter2_invoke_link_path(
                self._reporter,
                self._baton,
                path,
                url,
                revision,
                start_empty,
                lock_token,
                pool,
            )
###END###
def finish_report(self, pool=None):
            svn.ra.reporter2_invoke_finish_report(
                self._reporter, self._baton, pool
            )
###END###
def abort_report(self, pool=None):
            svn.ra.reporter2_invoke_abort_report(
                self._reporter, self._baton, pool
            )
###END###
def __init__(self, ui, repotype, path, revs=None):
        super(convert_cvs, self).__init__(ui, repotype, path, revs=revs)

        cvs = os.path.join(path, b"CVS")
        if not os.path.exists(cvs):
            raise NoRepo(_(b"%s does not look like a CVS checkout") % path)

        checktool(b'cvs')

        self.changeset = None
        self.files = {}
        self.tags = {}
        self.lastbranch = {}
        self.socket = None
        self.cvsroot = open(os.path.join(cvs, b"Root"), b'rb').read()[:-1]
        self.cvsrepo = open(os.path.join(cvs, b"Repository"), b'rb').read()[:-1]
        self.encoding = encoding.encoding

        self._connect()
###END###
def _parse(self):
        if self.changeset is not None:
            return
        self.changeset = {}

        maxrev = 0
        if self.revs:
            if len(self.revs) > 1:
                raise error.Abort(
                    _(
                        b'cvs source does not support specifying '
                        b'multiple revs'
                    )
                )
            # TODO: handle tags
            try:
                # patchset number?
                maxrev = int(self.revs[0])
            except ValueError:
                raise error.Abort(
                    _(b'revision %s is not a patchset number') % self.revs[0]
                )

        d = encoding.getcwd()
        try:
            os.chdir(self.path)

            cache = b'update'
            if not self.ui.configbool(b'convert', b'cvsps.cache'):
                cache = None
            db = cvsps.createlog(self.ui, cache=cache)
            db = cvsps.createchangeset(
                self.ui,
                db,
                fuzz=int(self.ui.config(b'convert', b'cvsps.fuzz')),
                mergeto=self.ui.config(b'convert', b'cvsps.mergeto'),
                mergefrom=self.ui.config(b'convert', b'cvsps.mergefrom'),
            )

            for cs in db:
                if maxrev and cs.id > maxrev:
                    break
                id = b"%d" % cs.id
                cs.author = self.recode(cs.author)
                self.lastbranch[cs.branch] = id
                cs.comment = self.recode(cs.comment)
                if self.ui.configbool(b'convert', b'localtimezone'):
                    cs.date = makedatetimestamp(cs.date[0])
                date = dateutil.datestr(cs.date, b'%Y-%m-%d %H:%M:%S %1%2')
                self.tags.update(dict.fromkeys(cs.tags, id))

                files = {}
                for f in cs.entries:
                    files[f.file] = b"%s%s" % (
                        b'.'.join([(b"%d" % x) for x in f.revision]),
                        [b'', b'(DEAD)'][f.dead],
                    )

                # add current commit to set
                c = commit(
                    author=cs.author,
                    date=date,
                    parents=[(b"%d" % p.id) for p in cs.parents],
                    desc=cs.comment,
                    branch=cs.branch or b'',
                )
                self.changeset[id] = c
                self.files[id] = files

            self.heads = self.lastbranch.values()
        finally:
            os.chdir(d)
###END###
def _connect(self):
        root = self.cvsroot
        conntype = None
        user, host = None, None
        cmd = [b'cvs', b'server']

        self.ui.status(_(b"connecting to %s\n") % root)

        if root.startswith(b":pserver:"):
            root = root[9:]
            m = re.match(r'(?:(.*?)(?::(.*?))?@)?([^:/]*)(?::(\d*))?(.*)', root)
            if m:
                conntype = b"pserver"
                user, passw, serv, port, root = m.groups()
                if not user:
                    user = b"anonymous"
                if not port:
                    port = 2401
                else:
                    port = int(port)
                format0 = b":pserver:%s@%s:%s" % (user, serv, root)
                format1 = b":pserver:%s@%s:%d%s" % (user, serv, port, root)

                if not passw:
                    passw = b"A"
                    cvspass = os.path.expanduser(b"~/.cvspass")
                    try:
                        pf = open(cvspass, b'rb')
                        for line in pf.read().splitlines():
                            part1, part2 = line.split(b' ', 1)
                            # /1 :pserver:user@example.com:2401/cvsroot/foo
                            # Ah<Z
                            if part1 == b'/1':
                                part1, part2 = part2.split(b' ', 1)
                                format = format1
                            # :pserver:user@example.com:/cvsroot/foo Ah<Z
                            else:
                                format = format0
                            if part1 == format:
                                passw = part2
                                break
                        pf.close()
                    except IOError as inst:
                        if inst.errno != errno.ENOENT:
                            if not getattr(inst, 'filename', None):
                                inst.filename = cvspass
                            raise

                sck = socket.socket()
                sck.connect((serv, port))
                sck.send(
                    b"\n".join(
                        [
                            b"BEGIN AUTH REQUEST",
                            root,
                            user,
                            passw,
                            b"END AUTH REQUEST",
                            b"",
                        ]
                    )
                )
                if sck.recv(128) != b"I LOVE YOU\n":
                    raise error.Abort(_(b"CVS pserver authentication failed"))

                self.writep = self.readp = sck.makefile(b'r+')

        if not conntype and root.startswith(b":local:"):
            conntype = b"local"
            root = root[7:]

        if not conntype:
            # :ext:user@host/home/user/path/to/cvsroot
            if root.startswith(b":ext:"):
                root = root[5:]
            m = re.match(br'(?:([^@:/]+)@)?([^:/]+):?(.*)', root)
            # Do not take Windows path "c:\foo\bar" for a connection strings
            if os.path.isdir(root) or not m:
                conntype = b"local"
            else:
                conntype = b"rsh"
                user, host, root = m.group(1), m.group(2), m.group(3)

        if conntype != b"pserver":
            if conntype == b"rsh":
                rsh = encoding.environ.get(b"CVS_RSH") or b"ssh"
                if user:
                    cmd = [rsh, b'-l', user, host] + cmd
                else:
                    cmd = [rsh, host] + cmd

            # popen2 does not support argument lists under Windows
            cmd = b' '.join(procutil.shellquote(arg) for arg in cmd)
            self.writep, self.readp = procutil.popen2(cmd)

        self.realroot = root

        self.writep.write(b"Root %s\n" % root)
        self.writep.write(
            b"Valid-responses ok error Valid-requests Mode"
            b" M Mbinary E Checked-in Created Updated"
            b" Merged Removed\n"
        )
        self.writep.write(b"valid-requests\n")
        self.writep.flush()
        r = self.readp.readline()
        if not r.startswith(b"Valid-requests"):
            raise error.Abort(
                _(
                    b'unexpected response from CVS server '
                    b'(expected "Valid-requests", but got %r)'
                )
                % r
            )
        if b"UseUnchanged" in r:
            self.writep.write(b"UseUnchanged\n")
            self.writep.flush()
            self.readp.readline()
###END###
def getheads(self):
        self._parse()
        return self.heads
###END###
def getfile(self, name, rev):
        def chunkedread(fp, count):
            # file-objects returned by socket.makefile() do not handle
            # large read() requests very well.
            chunksize = 65536
            output = stringio()
            while count > 0:
                data = fp.read(min(count, chunksize))
                if not data:
                    raise error.Abort(
                        _(b"%d bytes missing from remote file") % count
                    )
                count -= len(data)
                output.write(data)
            return output.getvalue()

        self._parse()
        if rev.endswith(b"(DEAD)"):
            return None, None

        args = (b"-N -P -kk -r %s --" % rev).split()
        args.append(self.cvsrepo + b'/' + name)
        for x in args:
            self.writep.write(b"Argument %s\n" % x)
        self.writep.write(b"Directory .\n%s\nco\n" % self.realroot)
        self.writep.flush()

        data = b""
        mode = None
        while True:
            line = self.readp.readline()
            if line.startswith(b"Created ") or line.startswith(b"Updated "):
                self.readp.readline()  # path
                self.readp.readline()  # entries
                mode = self.readp.readline()[:-1]
                count = int(self.readp.readline()[:-1])
                data = chunkedread(self.readp, count)
            elif line.startswith(b" "):
                data += line[1:]
            elif line.startswith(b"M "):
                pass
            elif line.startswith(b"Mbinary "):
                count = int(self.readp.readline()[:-1])
                data = chunkedread(self.readp, count)
            else:
                if line == b"ok\n":
                    if mode is None:
                        raise error.Abort(_(b'malformed response from CVS'))
                    return (data, b"x" in mode and b"x" or b"")
                elif line.startswith(b"E "):
                    self.ui.warn(_(b"cvs server: %s\n") % line[2:])
                elif line.startswith(b"Remove"):
                    self.readp.readline()
                else:
                    raise error.Abort(_(b"unknown CVS response: %s") % line)
###END###
def getchanges(self, rev, full):
        if full:
            raise error.Abort(_(b"convert from cvs does not support --full"))
        self._parse()
        return sorted(pycompat.iteritems(self.files[rev])), {}, set()
###END###
def getcommit(self, rev):
        self._parse()
        return self.changeset[rev]
###END###
def gettags(self):
        self._parse()
        return self.tags
###END###
def getchangedfiles(self, rev, i):
        self._parse()
        return sorted(self.files[rev])
###END###
def __init__(self, path, node, url):
        self.path = path
        self.node = node
        self.url = url
###END###
def hgsub(self):
        return b"%s = [git]%s" % (self.path, self.url)
###END###
def hgsubstate(self):
        return b"%s %s" % (self.node, self.path)
###END###
def _gitcmd(self, cmd, *args, **kwargs):
        return cmd(b'--git-dir=%s' % self.path, *args, **kwargs)
###END###
def gitrun0(self, *args, **kwargs):
        return self._gitcmd(self.run0, *args, **kwargs)
###END###
def gitrun(self, *args, **kwargs):
        return self._gitcmd(self.run, *args, **kwargs)
###END###
def gitrunlines0(self, *args, **kwargs):
        return self._gitcmd(self.runlines0, *args, **kwargs)
###END###
def gitrunlines(self, *args, **kwargs):
        return self._gitcmd(self.runlines, *args, **kwargs)
###END###
def gitpipe(self, *args, **kwargs):
        return self._gitcmd(self._run3, *args, **kwargs)
###END###
def __init__(self, ui, repotype, path, revs=None):
        super(convert_git, self).__init__(ui, repotype, path, revs=revs)
        common.commandline.__init__(self, ui, b'git')

        # Pass an absolute path to git to prevent from ever being interpreted
        # as a URL
        path = util.abspath(path)

        if os.path.isdir(path + b"/.git"):
            path += b"/.git"
        if not os.path.exists(path + b"/objects"):
            raise common.NoRepo(
                _(b"%s does not look like a Git repository") % path
            )

        # The default value (50) is based on the default for 'git diff'.
        similarity = ui.configint(b'convert', b'git.similarity')
        if similarity < 0 or similarity > 100:
            raise error.Abort(_(b'similarity must be between 0 and 100'))
        if similarity > 0:
            self.simopt = [b'-C%d%%' % similarity]
            findcopiesharder = ui.configbool(
                b'convert', b'git.findcopiesharder'
            )
            if findcopiesharder:
                self.simopt.append(b'--find-copies-harder')

            renamelimit = ui.configint(b'convert', b'git.renamelimit')
            self.simopt.append(b'-l%d' % renamelimit)
        else:
            self.simopt = []

        common.checktool(b'git', b'git')

        self.path = path
        self.submodules = []

        self.catfilepipe = self.gitpipe(b'cat-file', b'--batch')

        self.copyextrakeys = self.ui.configlist(b'convert', b'git.extrakeys')
        banned = set(self.copyextrakeys) & bannedextrakeys
        if banned:
            raise error.Abort(
                _(b'copying of extra key is forbidden: %s')
                % _(b', ').join(sorted(banned))
            )

        committeractions = self.ui.configlist(
            b'convert', b'git.committeractions'
        )

        messagedifferent = None
        messagealways = None
        for a in committeractions:
            if a.startswith((b'messagedifferent', b'messagealways')):
                k = a
                v = None
                if b'=' in a:
                    k, v = a.split(b'=', 1)

                if k == b'messagedifferent':
                    messagedifferent = v or b'committer:'
                elif k == b'messagealways':
                    messagealways = v or b'committer:'

        if messagedifferent and messagealways:
            raise error.Abort(
                _(
                    b'committeractions cannot define both '
                    b'messagedifferent and messagealways'
                )
            )

        dropcommitter = b'dropcommitter' in committeractions
        replaceauthor = b'replaceauthor' in committeractions

        if dropcommitter and replaceauthor:
            raise error.Abort(
                _(
                    b'committeractions cannot define both '
                    b'dropcommitter and replaceauthor'
                )
            )

        if dropcommitter and messagealways:
            raise error.Abort(
                _(
                    b'committeractions cannot define both '
                    b'dropcommitter and messagealways'
                )
            )

        if not messagedifferent and not messagealways:
            messagedifferent = b'committer:'

        self.committeractions = {
            b'dropcommitter': dropcommitter,
            b'replaceauthor': replaceauthor,
            b'messagedifferent': messagedifferent,
            b'messagealways': messagealways,
        }
###END###
def after(self):
        for f in self.catfilepipe:
            f.close()
###END###
def getheads(self):
        if not self.revs:
            output, status = self.gitrun(
                b'rev-parse', b'--branches', b'--remotes'
            )
            heads = output.splitlines()
            if status:
                raise error.Abort(_(b'cannot retrieve git heads'))
        else:
            heads = []
            for rev in self.revs:
                rawhead, ret = self.gitrun(b'rev-parse', b'--verify', rev)
                heads.append(rawhead[:-1])
                if ret:
                    raise error.Abort(_(b'cannot retrieve git head "%s"') % rev)
        return heads
###END###
def catfile(self, rev, ftype):
        if rev == sha1nodeconstants.nullhex:
            raise IOError
        self.catfilepipe[0].write(rev + b'\n')
        self.catfilepipe[0].flush()
        info = self.catfilepipe[1].readline().split()
        if info[1] != ftype:
            raise error.Abort(
                _(b'cannot read %r object at %s')
                % (pycompat.bytestr(ftype), rev)
            )
        size = int(info[2])
        data = self.catfilepipe[1].read(size)
        if len(data) < size:
            raise error.Abort(
                _(b'cannot read %r object at %s: unexpected size')
                % (ftype, rev)
            )
        # read the trailing newline
        self.catfilepipe[1].read(1)
        return data
###END###
def getfile(self, name, rev):
        if rev == sha1nodeconstants.nullhex:
            return None, None
        if name == b'.hgsub':
            data = b'\n'.join([m.hgsub() for m in self.submoditer()])
            mode = b''
        elif name == b'.hgsubstate':
            data = b'\n'.join([m.hgsubstate() for m in self.submoditer()])
            mode = b''
        else:
            data = self.catfile(rev, b"blob")
            mode = self.modecache[(name, rev)]
        return data, mode
###END###
def submoditer(self):
        null = sha1nodeconstants.nullhex
        for m in sorted(self.submodules, key=lambda p: p.path):
            if m.node != null:
                yield m
###END###
def parsegitmodules(self, content):
        """Parse the formatted .gitmodules file, example file format:
        [submodule "sub"]\n
        \tpath = sub\n
        \turl = git://giturl\n
        """
        self.submodules = []
        c = config.config()
        # Each item in .gitmodules starts with whitespace that cant be parsed
        c.parse(
            b'.gitmodules',
            b'\n'.join(line.strip() for line in content.split(b'\n')),
        )
        for sec in c.sections():
            # turn the config object into a real dict
            s = dict(c.items(sec))
            if b'url' in s and b'path' in s:
                self.submodules.append(submodule(s[b'path'], b'', s[b'url']))
###END###
def retrievegitmodules(self, version):
        modules, ret = self.gitrun(
            b'show', b'%s:%s' % (version, b'.gitmodules')
        )
        if ret:
            # This can happen if a file is in the repo that has permissions
            # 160000, but there is no .gitmodules file.
            self.ui.warn(
                _(b"warning: cannot read submodules config file in %s\n")
                % version
            )
            return

        try:
            self.parsegitmodules(modules)
        except error.ParseError:
            self.ui.warn(
                _(b"warning: unable to parse .gitmodules in %s\n") % version
            )
            return

        for m in self.submodules:
            node, ret = self.gitrun(b'rev-parse', b'%s:%s' % (version, m.path))
            if ret:
                continue
            m.node = node.strip()
###END###
def getchanges(self, version, full):
        if full:
            raise error.Abort(_(b"convert from git does not support --full"))
        self.modecache = {}
        cmd = (
            [b'diff-tree', b'-z', b'--root', b'-m', b'-r']
            + self.simopt
            + [version]
        )
        output, status = self.gitrun(*cmd)
        if status:
            raise error.Abort(_(b'cannot read changes in %s') % version)
        changes = []
        copies = {}
        seen = set()
        entry = None
        subexists = [False]
        subdeleted = [False]
        difftree = output.split(b'\x00')
        lcount = len(difftree)
        i = 0

        skipsubmodules = self.ui.configbool(b'convert', b'git.skipsubmodules')

        def add(entry, f, isdest):
            seen.add(f)
            h = entry[3]
            p = entry[1] == b"100755"
            s = entry[1] == b"120000"
            renamesource = not isdest and entry[4][0] == b'R'

            if f == b'.gitmodules':
                if skipsubmodules:
                    return

                subexists[0] = True
                if entry[4] == b'D' or renamesource:
                    subdeleted[0] = True
                    changes.append((b'.hgsub', sha1nodeconstants.nullhex))
                else:
                    changes.append((b'.hgsub', b''))
            elif entry[1] == b'160000' or entry[0] == b':160000':
                if not skipsubmodules:
                    subexists[0] = True
            else:
                if renamesource:
                    h = sha1nodeconstants.nullhex
                self.modecache[(f, h)] = (p and b"x") or (s and b"l") or b""
                changes.append((f, h))

        while i < lcount:
            l = difftree[i]
            i += 1
            if not entry:
                if not l.startswith(b':'):
                    continue
                entry = tuple(pycompat.bytestr(p) for p in l.split())
                continue
            f = l
            if entry[4][0] == b'C':
                copysrc = f
                copydest = difftree[i]
                i += 1
                f = copydest
                copies[copydest] = copysrc
            if f not in seen:
                add(entry, f, False)
            # A file can be copied multiple times, or modified and copied
            # simultaneously. So f can be repeated even if fdest isn't.
            if entry[4][0] == b'R':
                # rename: next line is the destination
                fdest = difftree[i]
                i += 1
                if fdest not in seen:
                    add(entry, fdest, True)
                    # .gitmodules isn't imported at all, so it being copied to
                    # and fro doesn't really make sense
                    if f != b'.gitmodules' and fdest != b'.gitmodules':
                        copies[fdest] = f
            entry = None

        if subexists[0]:
            if subdeleted[0]:
                changes.append((b'.hgsubstate', sha1nodeconstants.nullhex))
            else:
                self.retrievegitmodules(version)
                changes.append((b'.hgsubstate', b''))
        return (changes, copies, set())
###END###
def getcommit(self, version):
        c = self.catfile(version, b"commit")  # read the commit hash
        end = c.find(b"\n\n")
        message = c[end + 2 :]
        message = self.recode(message)
        l = c[:end].splitlines()
        parents = []
        author = committer = None
        extra = {}
        for e in l[1:]:
            n, v = e.split(b" ", 1)
            if n == b"author":
                p = v.split()
                tm, tz = p[-2:]
                author = b" ".join(p[:-2])
                if author[0] == b"<":
                    author = author[1:-1]
                author = self.recode(author)
            if n == b"committer":
                p = v.split()
                tm, tz = p[-2:]
                committer = b" ".join(p[:-2])
                if committer[0] == b"<":
                    committer = committer[1:-1]
                committer = self.recode(committer)
            if n == b"parent":
                parents.append(v)
            if n in self.copyextrakeys:
                extra[n] = v

        if self.committeractions[b'dropcommitter']:
            committer = None
        elif self.committeractions[b'replaceauthor']:
            author = committer

        if committer:
            messagealways = self.committeractions[b'messagealways']
            messagedifferent = self.committeractions[b'messagedifferent']
            if messagealways:
                message += b'\n%s %s\n' % (messagealways, committer)
            elif messagedifferent and author != committer:
                message += b'\n%s %s\n' % (messagedifferent, committer)

        tzs, tzh, tzm = tz[-5:-4] + b"1", tz[-4:-2], tz[-2:]
        tz = -int(tzs) * (int(tzh) * 3600 + int(tzm))
        date = tm + b" " + (b"%d" % tz)
        saverev = self.ui.configbool(b'convert', b'git.saverev')

        c = common.commit(
            parents=parents,
            date=date,
            author=author,
            desc=message,
            rev=version,
            extra=extra,
            saverev=saverev,
        )
        return c
###END###
def numcommits(self):
        output, ret = self.gitrunlines(b'rev-list', b'--all')
        if ret:
            raise error.Abort(
                _(b'cannot retrieve number of commits in %s') % self.path
            )
        return len(output)
###END###
def gettags(self):
        tags = {}
        alltags = {}
        output, status = self.gitrunlines(b'ls-remote', b'--tags', self.path)

        if status:
            raise error.Abort(_(b'cannot read tags from %s') % self.path)
        prefix = b'refs/tags/'

        # Build complete list of tags, both annotated and bare ones
        for line in output:
            line = line.strip()
            if line.startswith(b"error:") or line.startswith(b"fatal:"):
                raise error.Abort(_(b'cannot read tags from %s') % self.path)
            node, tag = line.split(None, 1)
            if not tag.startswith(prefix):
                continue
            alltags[tag[len(prefix) :]] = node

        # Filter out tag objects for annotated tag refs
        for tag in alltags:
            if tag.endswith(b'^{}'):
                tags[tag[:-3]] = alltags[tag]
            else:
                if tag + b'^{}' in alltags:
                    continue
                else:
                    tags[tag] = alltags[tag]

        return tags
###END###
def getchangedfiles(self, version, i):
        changes = []
        if i is None:
            output, status = self.gitrunlines(
                b'diff-tree', b'--root', b'-m', b'-r', version
            )
            if status:
                raise error.Abort(_(b'cannot read changes in %s') % version)
            for l in output:
                if b"\t" not in l:
                    continue
                m, f = l[:-1].split(b"\t")
                changes.append(f)
        else:
            output, status = self.gitrunlines(
                b'diff-tree',
                b'--name-only',
                b'--root',
                b'-r',
                version,
                b'%s^%d' % (version, i + 1),
                b'--',
            )
            if status:
                raise error.Abort(_(b'cannot read changes in %s') % version)
            changes = [f.rstrip(b'\n') for f in output]

        return changes
###END###
def getbookmarks(self):
        bookmarks = {}

        # Handle local and remote branches
        remoteprefix = self.ui.config(b'convert', b'git.remoteprefix')
        reftypes = [
            # (git prefix, hg prefix)
            (b'refs/remotes/origin/', remoteprefix + b'/'),
            (b'refs/heads/', b''),
        ]

        exclude = {
            b'refs/remotes/origin/HEAD',
        }

        try:
            output, status = self.gitrunlines(b'show-ref')
            for line in output:
                line = line.strip()
                rev, name = line.split(None, 1)
                # Process each type of branch
                for gitprefix, hgprefix in reftypes:
                    if not name.startswith(gitprefix) or name in exclude:
                        continue
                    name = b'%s%s' % (hgprefix, name[len(gitprefix) :])
                    bookmarks[name] = rev
        except Exception:
            pass

        return bookmarks
###END###
def checkrevformat(self, revstr, mapname=b'splicemap'):
        """git revision string is a 40 byte hex"""
        self.checkhexformat(revstr, mapname)
###END###
def __init__(self, ui, repotype, path, revs=None):
        super(gnuarch_source, self).__init__(ui, repotype, path, revs=revs)

        if not os.path.exists(os.path.join(path, b'{arch}')):
            raise common.NoRepo(
                _(b"%s does not look like a GNU Arch repository") % path
            )

        # Could use checktool, but we want to check for baz or tla.
        self.execmd = None
        if procutil.findexe(b'baz'):
            self.execmd = b'baz'
        else:
            if procutil.findexe(b'tla'):
                self.execmd = b'tla'
            else:
                raise error.Abort(_(b'cannot find a GNU Arch tool'))

        common.commandline.__init__(self, ui, self.execmd)

        self.path = os.path.realpath(path)
        self.tmppath = None

        self.treeversion = None
        self.lastrev = None
        self.changes = {}
        self.parents = {}
        self.tags = {}
        self.encoding = encoding.encoding
        self.archives = []
###END###
def before(self):
        # Get registered archives
        self.archives = [
            i.rstrip(b'\n') for i in self.runlines0(b'archives', b'-n')
        ]

        if self.execmd == b'tla':
            output = self.run0(b'tree-version', self.path)
        else:
            output = self.run0(b'tree-version', b'-d', self.path)
        self.treeversion = output.strip()

        # Get name of temporary directory
        version = self.treeversion.split(b'/')
        self.tmppath = os.path.join(
            pycompat.fsencode(tempfile.gettempdir()), b'hg-%s' % version[1]
        )

        # Generate parents dictionary
        self.parents[None] = []
        treeversion = self.treeversion
        child = None
        while treeversion:
            self.ui.status(_(b'analyzing tree version %s...\n') % treeversion)

            archive = treeversion.split(b'/')[0]
            if archive not in self.archives:
                self.ui.status(
                    _(
                        b'tree analysis stopped because it points to '
                        b'an unregistered archive %s...\n'
                    )
                    % archive
                )
                break

            # Get the complete list of revisions for that tree version
            output, status = self.runlines(
                b'revisions', b'-r', b'-f', treeversion
            )
            self.checkexit(
                status, b'failed retrieving revisions for %s' % treeversion
            )

            # No new iteration unless a revision has a continuation-of header
            treeversion = None

            for l in output:
                rev = l.strip()
                self.changes[rev] = self.gnuarch_rev(rev)
                self.parents[rev] = []

                # Read author, date and summary
                catlog, status = self.run(b'cat-log', b'-d', self.path, rev)
                if status:
                    catlog = self.run0(b'cat-archive-log', rev)
                self._parsecatlog(catlog, rev)

                # Populate the parents map
                self.parents[child].append(rev)

                # Keep track of the current revision as the child of the next
                # revision scanned
                child = rev

                # Check if we have to follow the usual incremental history
                # or if we have to 'jump' to a different treeversion given
                # by the continuation-of header.
                if self.changes[rev].continuationof:
                    treeversion = b'--'.join(
                        self.changes[rev].continuationof.split(b'--')[:-1]
                    )
                    break

                # If we reached a base-0 revision w/o any continuation-of
                # header, it means the tree history ends here.
                if rev[-6:] == b'base-0':
                    break
###END###
def after(self):
        self.ui.debug(b'cleaning up %s\n' % self.tmppath)
        shutil.rmtree(self.tmppath, ignore_errors=True)
###END###
def getheads(self):
        return self.parents[None]
###END###
def getfile(self, name, rev):
        if rev != self.lastrev:
            raise error.Abort(_(b'internal calling inconsistency'))

        if not os.path.lexists(os.path.join(self.tmppath, name)):
            return None, None

        return self._getfile(name, rev)
###END###
def getchanges(self, rev, full):
        if full:
            raise error.Abort(_(b"convert from arch does not support --full"))
        self._update(rev)
        changes = []
        copies = {}

        for f in self.changes[rev].add_files:
            changes.append((f, rev))

        for f in self.changes[rev].mod_files:
            changes.append((f, rev))

        for f in self.changes[rev].del_files:
            changes.append((f, rev))

        for src in self.changes[rev].ren_files:
            to = self.changes[rev].ren_files[src]
            changes.append((src, rev))
            changes.append((to, rev))
            copies[to] = src

        for src in self.changes[rev].ren_dirs:
            to = self.changes[rev].ren_dirs[src]
            chgs, cps = self._rendirchanges(src, to)
            changes += [(f, rev) for f in chgs]
            copies.update(cps)

        self.lastrev = rev
        return sorted(set(changes)), copies, set()
###END###
def getcommit(self, rev):
        changes = self.changes[rev]
        return common.commit(
            author=changes.author,
            date=changes.date,
            desc=changes.summary,
            parents=self.parents[rev],
            rev=rev,
        )
###END###
def gettags(self):
        return self.tags
###END###
def _execute(self, cmd, *args, **kwargs):
        cmdline = [self.execmd, cmd]
        cmdline += args
        cmdline = [procutil.shellquote(arg) for arg in cmdline]
        bdevnull = pycompat.bytestr(os.devnull)
        cmdline += [b'>', bdevnull, b'2>', bdevnull]
        cmdline = b' '.join(cmdline)
        self.ui.debug(cmdline, b'\n')
        return os.system(pycompat.rapply(procutil.tonativestr, cmdline))
###END###
def _update(self, rev):
        self.ui.debug(b'applying revision %s...\n' % rev)
        changeset, status = self.runlines(b'replay', b'-d', self.tmppath, rev)
        if status:
            # Something went wrong while merging (baz or tla
            # issue?), get latest revision and try from there
            shutil.rmtree(self.tmppath, ignore_errors=True)
            self._obtainrevision(rev)
        else:
            old_rev = self.parents[rev][0]
            self.ui.debug(
                b'computing changeset between %s and %s...\n' % (old_rev, rev)
            )
            self._parsechangeset(changeset, rev)
###END###
def _getfile(self, name, rev):
        mode = os.lstat(os.path.join(self.tmppath, name)).st_mode
        if stat.S_ISLNK(mode):
            data = util.readlink(os.path.join(self.tmppath, name))
            if mode:
                mode = b'l'
            else:
                mode = b''
        else:
            data = util.readfile(os.path.join(self.tmppath, name))
            mode = (mode & 0o111) and b'x' or b''
        return data, mode
###END###
def _exclude(self, name):
        exclude = [b'{arch}', b'.arch-ids', b'.arch-inventory']
        for exc in exclude:
            if name.find(exc) != -1:
                return True
        return False
###END###
def _readcontents(self, path):
        files = []
        contents = os.listdir(path)
        while len(contents) > 0:
            c = contents.pop()
            p = os.path.join(path, c)
            # os.walk could be used, but here we avoid internal GNU
            # Arch files and directories, thus saving a lot time.
            if not self._exclude(p):
                if os.path.isdir(p):
                    contents += [os.path.join(c, f) for f in os.listdir(p)]
                else:
                    files.append(c)
        return files
###END###
def _rendirchanges(self, src, dest):
        changes = []
        copies = {}
        files = self._readcontents(os.path.join(self.tmppath, dest))
        for f in files:
            s = os.path.join(src, f)
            d = os.path.join(dest, f)
            changes.append(s)
            changes.append(d)
            copies[d] = s
        return changes, copies
###END###
def _obtainrevision(self, rev):
        self.ui.debug(b'obtaining revision %s...\n' % rev)
        output = self._execute(b'get', rev, self.tmppath)
        self.checkexit(output)
        self.ui.debug(b'analyzing revision %s...\n' % rev)
        files = self._readcontents(self.tmppath)
        self.changes[rev].add_files += files
###END###
def _stripbasepath(self, path):
        if path.startswith(b'./'):
            return path[2:]
        return path
###END###
def _parsecatlog(self, data, rev):
        try:
            catlog = mail.parsebytes(data)

            # Commit date
            self.changes[rev].date = dateutil.datestr(
                dateutil.strdate(catlog['Standard-date'], b'%Y-%m-%d %H:%M:%S')
            )

            # Commit author
            self.changes[rev].author = self.recode(catlog['Creator'])

            # Commit description
            self.changes[rev].summary = b'\n\n'.join(
                (
                    self.recode(catlog['Summary']),
                    self.recode(catlog.get_payload()),
                )
            )
            self.changes[rev].summary = self.recode(self.changes[rev].summary)

            # Commit revision origin when dealing with a branch or tag
            if 'Continuation-of' in catlog:
                self.changes[rev].continuationof = self.recode(
                    catlog['Continuation-of']
                )
        except Exception:
            raise error.Abort(_(b'could not parse cat-log of %s') % rev)
###END###
def _parsechangeset(self, data, rev):
        for l in data:
            l = l.strip()
            # Added file (ignore added directory)
            if l.startswith(b'A') and not l.startswith(b'A/'):
                file = self._stripbasepath(l[1:].strip())
                if not self._exclude(file):
                    self.changes[rev].add_files.append(file)
            # Deleted file (ignore deleted directory)
            elif l.startswith(b'D') and not l.startswith(b'D/'):
                file = self._stripbasepath(l[1:].strip())
                if not self._exclude(file):
                    self.changes[rev].del_files.append(file)
            # Modified binary file
            elif l.startswith(b'Mb'):
                file = self._stripbasepath(l[2:].strip())
                if not self._exclude(file):
                    self.changes[rev].mod_files.append(file)
            # Modified link
            elif l.startswith(b'M->'):
                file = self._stripbasepath(l[3:].strip())
                if not self._exclude(file):
                    self.changes[rev].mod_files.append(file)
            # Modified file
            elif l.startswith(b'M'):
                file = self._stripbasepath(l[1:].strip())
                if not self._exclude(file):
                    self.changes[rev].mod_files.append(file)
            # Renamed file (or link)
            elif l.startswith(b'=>'):
                files = l[2:].strip().split(b' ')
                if len(files) == 1:
                    files = l[2:].strip().split(b'\t')
                src = self._stripbasepath(files[0])
                dst = self._stripbasepath(files[1])
                if not self._exclude(src) and not self._exclude(dst):
                    self.changes[rev].ren_files[src] = dst
            # Conversion from file to link or from link to file (modified)
            elif l.startswith(b'ch'):
                file = self._stripbasepath(l[2:].strip())
                if not self._exclude(file):
                    self.changes[rev].mod_files.append(file)
            # Renamed directory
            elif l.startswith(b'/>'):
                dirs = l[2:].strip().split(b' ')
                if len(dirs) == 1:
                    dirs = l[2:].strip().split(b'\t')
                src = self._stripbasepath(dirs[0])
                dst = self._stripbasepath(dirs[1])
                if not self._exclude(src) and not self._exclude(dst):
                    self.changes[rev].ren_dirs[src] = dst
###END###
def __init__(self, rev):
            self.rev = rev
            self.summary = b''
            self.date = None
            self.author = b''
            self.continuationof = None
            self.add_files = []
            self.mod_files = []
            self.del_files = []
            self.ren_files = {}
            self.ren_dirs = {}
###END###
def __init__(self, ui, repotype, path, revs=None):
        common.converter_source.__init__(self, ui, repotype, path, revs=revs)
        common.commandline.__init__(self, ui, b'darcs')

        # check for _darcs, ElementTree so that we can easily skip
        # test-convert-darcs if ElementTree is not around
        if not os.path.exists(os.path.join(path, b'_darcs')):
            raise NoRepo(_(b"%s does not look like a darcs repository") % path)

        common.checktool(b'darcs')
        version = self.run0(b'--version').splitlines()[0].strip()
        if version < b'2.1':
            raise error.Abort(
                _(b'darcs version 2.1 or newer needed (found %r)') % version
            )

        if b"ElementTree" not in globals():
            raise error.Abort(_(b"Python ElementTree module is not available"))

        self.path = os.path.realpath(path)

        self.lastrev = None
        self.changes = {}
        self.parents = {}
        self.tags = {}

        # Check darcs repository format
        format = self.format()
        if format:
            if format in (b'darcs-1.0', b'hashed'):
                raise NoRepo(
                    _(
                        b"%s repository format is unsupported, "
                        b"please upgrade"
                    )
                    % format
                )
        else:
            self.ui.warn(_(b'failed to detect repository format!'))
###END###
def before(self):
        self.tmppath = pycompat.mkdtemp(
            prefix=b'convert-' + os.path.basename(self.path) + b'-'
        )
        output, status = self.run(b'init', repodir=self.tmppath)
        self.checkexit(status)

        tree = self.xml(
            b'changes', xml_output=True, summary=True, repodir=self.path
        )
        tagname = None
        child = None
        for elt in tree.findall(b'patch'):
            node = elt.get(b'hash')
            name = elt.findtext(b'name', b'')
            if name.startswith(b'TAG '):
                tagname = name[4:].strip()
            elif tagname is not None:
                self.tags[tagname] = node
                tagname = None
            self.changes[node] = elt
            self.parents[child] = [node]
            child = node
        self.parents[child] = []
###END###
def after(self):
        self.ui.debug(b'cleaning up %s\n' % self.tmppath)
        shutil.rmtree(self.tmppath, ignore_errors=True)
###END###
def recode(self, s, encoding=None):
        if isinstance(s, pycompat.unicode):
            # XMLParser returns unicode objects for anything it can't
            # encode into ASCII. We convert them back to str to get
            # recode's normal conversion behavior.
            s = s.encode('latin-1')
        return super(darcs_source, self).recode(s, encoding)
###END###
def xml(self, cmd, **kwargs):
        # NOTE: darcs is currently encoding agnostic and will print
        # patch metadata byte-for-byte, even in the XML changelog.
        etree = ElementTree()
        # While we are decoding the XML as latin-1 to be as liberal as
        # possible, etree will still raise an exception if any
        # non-printable characters are in the XML changelog.
        parser = XMLParser(encoding=b'latin-1')
        p = self._run(cmd, **kwargs)
        etree.parse(p.stdout, parser=parser)
        p.wait()
        self.checkexit(p.returncode)
        return etree.getroot()
###END###
def format(self):
        output, status = self.run(b'show', b'repo', repodir=self.path)
        self.checkexit(status)
        m = re.search(r'^\s*Format:\s*(.*)$', output, re.MULTILINE)
        if not m:
            return None
        return b','.join(sorted(f.strip() for f in m.group(1).split(b',')))
###END###
def manifest(self):
        man = []
        output, status = self.run(
            b'show', b'files', no_directories=True, repodir=self.tmppath
        )
        self.checkexit(status)
        for line in output.split(b'\n'):
            path = line[2:]
            if path:
                man.append(path)
        return man
###END###
def getheads(self):
        return self.parents[None]
###END###
def getcommit(self, rev):
        elt = self.changes[rev]
        dateformat = b'%a %b %d %H:%M:%S %Z %Y'
        date = dateutil.strdate(elt.get(b'local_date'), dateformat)
        desc = elt.findtext(b'name') + b'\n' + elt.findtext(b'comment', b'')
        # etree can return unicode objects for name, comment, and author,
        # so recode() is used to ensure str objects are emitted.
        newdateformat = b'%Y-%m-%d %H:%M:%S %1%2'
        return common.commit(
            author=self.recode(elt.get(b'author')),
            date=dateutil.datestr(date, newdateformat),
            desc=self.recode(desc).strip(),
            parents=self.parents[rev],
        )
###END###
def pull(self, rev):
        output, status = self.run(
            b'pull',
            self.path,
            all=True,
            match=b'hash %s' % rev,
            no_test=True,
            no_posthook=True,
            external_merge=b'/bin/false',
            repodir=self.tmppath,
        )
        if status:
            if output.find(b'We have conflicts in') == -1:
                self.checkexit(status, output)
            output, status = self.run(b'revert', all=True, repodir=self.tmppath)
            self.checkexit(status, output)
###END###
def getchanges(self, rev, full):
        if full:
            raise error.Abort(_(b"convert from darcs does not support --full"))
        copies = {}
        changes = []
        man = None
        for elt in self.changes[rev].find(b'summary').getchildren():
            if elt.tag in (b'add_directory', b'remove_directory'):
                continue
            if elt.tag == b'move':
                if man is None:
                    man = self.manifest()
                source, dest = elt.get(b'from'), elt.get(b'to')
                if source in man:
                    # File move
                    changes.append((source, rev))
                    changes.append((dest, rev))
                    copies[dest] = source
                else:
                    # Directory move, deduce file moves from manifest
                    source = source + b'/'
                    for f in man:
                        if not f.startswith(source):
                            continue
                        fdest = dest + b'/' + f[len(source) :]
                        changes.append((f, rev))
                        changes.append((fdest, rev))
                        copies[fdest] = f
            else:
                changes.append((elt.text.strip(), rev))
        self.pull(rev)
        self.lastrev = rev
        return sorted(changes), copies, set()
###END###
def getfile(self, name, rev):
        if rev != self.lastrev:
            raise error.Abort(_(b'internal calling inconsistency'))
        path = os.path.join(self.tmppath, name)
        try:
            data = util.readfile(path)
            mode = os.lstat(path).st_mode
        except IOError as inst:
            if inst.errno == errno.ENOENT:
                return None, None
            raise
        mode = (mode & 0o111) and b'x' or b''
        return data, mode
###END###
def gettags(self):
        return self.tags
###END###
def __init__(self):
        self.pipeo = self.pipei = self.pipee = None
        self.subprocess = None
        self.connected = False
###END###
def connect(self, cachecommand):
        if self.pipeo:
            raise error.Abort(_(b"cache connection already open"))
        self.pipei, self.pipeo, self.pipee, self.subprocess = procutil.popen4(
            cachecommand
        )
        self.connected = True
###END###
def close(self):
        def tryclose(pipe):
            try:
                pipe.close()
            except Exception:
                pass

        if self.connected:
            try:
                self.pipei.write(b"exit\n")
            except Exception:
                pass
            tryclose(self.pipei)
            self.pipei = None
            tryclose(self.pipeo)
            self.pipeo = None
            tryclose(self.pipee)
            self.pipee = None
            try:
                # Wait for process to terminate, making sure to avoid deadlock.
                # See https://docs.python.org/2/library/subprocess.html for
                # warnings about wait() and deadlocking.
                self.subprocess.communicate()
            except Exception:
                pass
            self.subprocess = None
        self.connected = False
###END###
def request(self, request, flush=True):
        if self.connected:
            try:
                self.pipei.write(request)
                if flush:
                    self.pipei.flush()
            except IOError:
                self.close()
###END###
def receiveline(self):
        if not self.connected:
            return None
        try:
            result = self.pipeo.readline()[:-1]
            if not result:
                self.close()
        except IOError:
            self.close()

        return result
###END###
def __init__(self, repo):
        ui = repo.ui
        self.repo = repo
        self.ui = ui
        self.cacheprocess = ui.config(b"remotefilelog", b"cacheprocess")
        if self.cacheprocess:
            self.cacheprocess = util.expandpath(self.cacheprocess)

        # This option causes remotefilelog to pass the full file path to the
        # cacheprocess instead of a hashed key.
        self.cacheprocesspasspath = ui.configbool(
            b"remotefilelog", b"cacheprocess.includepath"
        )

        self.debugoutput = ui.configbool(b"remotefilelog", b"debug")

        self.remotecache = cacheconnection()
###END###
def setstore(self, datastore, historystore, writedata, writehistory):
        self.datastore = datastore
        self.historystore = historystore
        self.writedata = writedata
        self.writehistory = writehistory
###END###
def _connect(self):
        return self.repo.connectionpool.get(self.repo.fallbackpath)
###END###
def request(self, fileids):
        """Takes a list of filename/node pairs and fetches them from the
        server. Files are stored in the local cache.
        A list of nodes that the server couldn't find is returned.
        If the connection fails, an exception is raised.
        """
        if not self.remotecache.connected:
            self.connect()
        cache = self.remotecache
        writedata = self.writedata

        repo = self.repo
        total = len(fileids)
        request = b"get\n%d\n" % total
        idmap = {}
        reponame = repo.name
        for file, id in fileids:
            fullid = getcachekey(reponame, file, id)
            if self.cacheprocesspasspath:
                request += file + b'\0'
            request += fullid + b"\n"
            idmap[fullid] = file

        cache.request(request)

        progress = self.ui.makeprogress(_(b'downloading'), total=total)
        progress.update(0)

        missed = []
        while True:
            missingid = cache.receiveline()
            if not missingid:
                missedset = set(missed)
                for missingid in idmap:
                    if not missingid in missedset:
                        missed.append(missingid)
                self.ui.warn(
                    _(
                        b"warning: cache connection closed early - "
                        + b"falling back to server\n"
                    )
                )
                break
            if missingid == b"0":
                break
            if missingid.startswith(b"_hits_"):
                # receive progress reports
                parts = missingid.split(b"_")
                progress.increment(int(parts[2]))
                continue

            missed.append(missingid)

        global fetchmisses
        fetchmisses += len(missed)

        fromcache = total - len(missed)
        progress.update(fromcache, total=total)
        self.ui.log(
            b"remotefilelog",
            b"remote cache hit rate is %r of %r\n",
            fromcache,
            total,
            hit=fromcache,
            total=total,
        )

        oldumask = os.umask(0o002)
        try:
            # receive cache misses from master
            if missed:
                # When verbose is true, sshpeer prints 'running ssh...'
                # to stdout, which can interfere with some command
                # outputs
                verbose = self.ui.verbose
                self.ui.verbose = False
                try:
                    with self._connect() as conn:
                        remote = conn.peer
                        if remote.capable(
                            constants.NETWORK_CAP_LEGACY_SSH_GETFILES
                        ):
                            if not isinstance(remote, _sshv1peer):
                                raise error.Abort(
                                    b'remotefilelog requires ssh servers'
                                )
                            step = self.ui.configint(
                                b'remotefilelog', b'getfilesstep'
                            )
                            getfilestype = self.ui.config(
                                b'remotefilelog', b'getfilestype'
                            )
                            if getfilestype == b'threaded':
                                _getfiles = _getfiles_threaded
                            else:
                                _getfiles = _getfiles_optimistic
                            _getfiles(
                                remote,
                                self.receivemissing,
                                progress.increment,
                                missed,
                                idmap,
                                step,
                            )
                        elif remote.capable(b"x_rfl_getfile"):
                            if remote.capable(b'batch'):
                                batchdefault = 100
                            else:
                                batchdefault = 10
                            batchsize = self.ui.configint(
                                b'remotefilelog', b'batchsize', batchdefault
                            )
                            self.ui.debug(
                                b'requesting %d files from '
                                b'remotefilelog server...\n' % len(missed)
                            )
                            _getfilesbatch(
                                remote,
                                self.receivemissing,
                                progress.increment,
                                missed,
                                idmap,
                                batchsize,
                            )
                        else:
                            raise error.Abort(
                                b"configured remotefilelog server"
                                b" does not support remotefilelog"
                            )

                    self.ui.log(
                        b"remotefilefetchlog",
                        b"Success\n",
                        fetched_files=progress.pos - fromcache,
                        total_to_fetch=total - fromcache,
                    )
                except Exception:
                    self.ui.log(
                        b"remotefilefetchlog",
                        b"Fail\n",
                        fetched_files=progress.pos - fromcache,
                        total_to_fetch=total - fromcache,
                    )
                    raise
                finally:
                    self.ui.verbose = verbose
                # send to memcache
                request = b"set\n%d\n%s\n" % (len(missed), b"\n".join(missed))
                cache.request(request)

            progress.complete()

            # mark ourselves as a user of this cache
            writedata.markrepo(self.repo.path)
        finally:
            os.umask(oldumask)
###END###
def receivemissing(self, pipe, filename, node):
        line = pipe.readline()[:-1]
        if not line:
            raise error.ResponseError(
                _(b"error downloading file contents:"),
                _(b"connection closed early"),
            )
        size = int(line)
        data = pipe.read(size)
        if len(data) != size:
            raise error.ResponseError(
                _(b"error downloading file contents:"),
                _(b"only received %s of %s bytes") % (len(data), size),
            )

        self.writedata.addremotefilelognode(
            filename, bin(node), zlib.decompress(data)
        )
###END###
def connect(self):
        if self.cacheprocess:
            cmd = b"%s %s" % (self.cacheprocess, self.writedata._path)
            self.remotecache.connect(cmd)
        else:
            # If no cache process is specified, we fake one that always
            # returns cache misses.  This enables tests to run easily
            # and may eventually allow us to be a drop in replacement
            # for the largefiles extension.
            class simplecache(object):
                def __init__(self):
                    self.missingids = []
                    self.connected = True

                def close(self):
                    pass

                def request(self, value, flush=True):
                    lines = value.split(b"\n")
                    if lines[0] != b"get":
                        return
                    self.missingids = lines[2:-1]
                    self.missingids.append(b'0')

                def receiveline(self):
                    if len(self.missingids) > 0:
                        return self.missingids.pop(0)
                    return None

            self.remotecache = simplecache()
###END###
def close(self):
        if fetches:
            msg = (
                b"%d files fetched over %d fetches - "
                + b"(%d misses, %0.2f%% hit ratio) over %0.2fs\n"
            ) % (
                fetched,
                fetches,
                fetchmisses,
                float(fetched - fetchmisses) / float(fetched) * 100.0,
                fetchcost,
            )
            if self.debugoutput:
                self.ui.warn(msg)
            self.ui.log(
                b"remotefilelog.prefetch",
                msg.replace(b"%", b"%%"),
                remotefilelogfetched=fetched,
                remotefilelogfetches=fetches,
                remotefilelogfetchmisses=fetchmisses,
                remotefilelogfetchtime=fetchcost * 1000,
            )

        if self.remotecache.connected:
            self.remotecache.close()
###END###
def prefetch(
        self, fileids, force=False, fetchdata=True, fetchhistory=False
    ):
        """downloads the given file versions to the cache"""
        repo = self.repo
        idstocheck = []
        for file, id in fileids:
            # hack
            # - we don't use .hgtags
            # - workingctx produces ids with length 42,
            #   which we skip since they aren't in any cache
            if (
                file == b'.hgtags'
                or len(id) == 42
                or not repo.shallowmatch(file)
            ):
                continue

            idstocheck.append((file, bin(id)))

        datastore = self.datastore
        historystore = self.historystore
        if force:
            datastore = contentstore.unioncontentstore(*repo.shareddatastores)
            historystore = metadatastore.unionmetadatastore(
                *repo.sharedhistorystores
            )

        missingids = set()
        if fetchdata:
            missingids.update(datastore.getmissing(idstocheck))
        if fetchhistory:
            missingids.update(historystore.getmissing(idstocheck))

        # partition missing nodes into nullid and not-nullid so we can
        # warn about this filtering potentially shadowing bugs.
        nullids = len(
            [None for unused, id in missingids if id == self.repo.nullid]
        )
        if nullids:
            missingids = [
                (f, id) for f, id in missingids if id != self.repo.nullid
            ]
            repo.ui.develwarn(
                (
                    b'remotefilelog not fetching %d null revs'
                    b' - this is likely hiding bugs' % nullids
                ),
                config=b'remotefilelog-ext',
            )
        if missingids:
            global fetches, fetched, fetchcost
            fetches += 1

            # We want to be able to detect excess individual file downloads, so
            # let's log that information for debugging.
            if fetches >= 15 and fetches < 18:
                if fetches == 15:
                    fetchwarning = self.ui.config(
                        b'remotefilelog', b'fetchwarning'
                    )
                    if fetchwarning:
                        self.ui.warn(fetchwarning + b'\n')
                self.logstacktrace()
            missingids = [(file, hex(id)) for file, id in sorted(missingids)]
            fetched += len(missingids)
            start = time.time()
            missingids = self.request(missingids)
            if missingids:
                raise error.Abort(
                    _(b"unable to download %d files") % len(missingids)
                )
            fetchcost += time.time() - start
            self._lfsprefetch(fileids)
###END###
def _lfsprefetch(self, fileids):
        if not _lfsmod or not util.safehasattr(
            self.repo.svfs, b'lfslocalblobstore'
        ):
            return
        if not _lfsmod.wrapper.candownload(self.repo):
            return
        pointers = []
        store = self.repo.svfs.lfslocalblobstore
        for file, id in fileids:
            node = bin(id)
            rlog = self.repo.file(file)
            if rlog.flags(node) & revlog.REVIDX_EXTSTORED:
                text = rlog.rawdata(node)
                p = _lfsmod.pointer.deserialize(text)
                oid = p.oid()
                if not store.has(oid):
                    pointers.append(p)
        if len(pointers) > 0:
            self.repo.svfs.lfsremoteblobstore.readbatch(pointers, store)
            assert all(store.has(p.oid()) for p in pointers)
###END###
def logstacktrace(self):
        import traceback

        self.ui.log(
            b'remotefilelog',
            b'excess remotefilelog fetching:\n%s\n',
            b''.join(pycompat.sysbytes(s) for s in traceback.format_stack()),
        )
###END###
def x_rfl_getfile(self, file, node):
            if not self.capable(b'x_rfl_getfile'):
                raise error.Abort(
                    b'configured remotefile server does not support getfile'
                )
            f = wireprotov1peer.future()
            yield {b'file': file, b'node': node}, f
            code, data = f.value.split(b'\0', 1)
            if int(code):
                raise error.LookupError(file, node, data)
            yield data
###END###
def x_rfl_getflogheads(self, path):
            if not self.capable(b'x_rfl_getflogheads'):
                raise error.Abort(
                    b'configured remotefile server does not '
                    b'support getflogheads'
                )
            f = wireprotov1peer.future()
            yield {b'path': path}, f
            heads = f.value.split(b'\n') if f.value else []
            yield heads
###END###
def _updatecallstreamopts(self, command, opts):
            if command != b'getbundle':
                return
            if (
                constants.NETWORK_CAP_LEGACY_SSH_GETFILES
                not in self.capabilities()
            ):
                return
            if not util.safehasattr(self, '_localrepo'):
                return
            if (
                constants.SHALLOWREPO_REQUIREMENT
                not in self._localrepo.requirements
            ):
                return

            bundlecaps = opts.get(b'bundlecaps')
            if bundlecaps:
                bundlecaps = [bundlecaps]
            else:
                bundlecaps = []

            # shallow, includepattern, and excludepattern are a hacky way of
            # carrying over data from the local repo to this getbundle
            # command. We need to do it this way because bundle1 getbundle
            # doesn't provide any other place we can hook in to manipulate
            # getbundle args before it goes across the wire. Once we get rid
            # of bundle1, we can use bundle2's _pullbundle2extraprepare to
            # do this more cleanly.
            bundlecaps.append(constants.BUNDLE2_CAPABLITY)
            if self._localrepo.includepattern:
                patterns = b'\0'.join(self._localrepo.includepattern)
                includecap = b"includepattern=" + patterns
                bundlecaps.append(includecap)
            if self._localrepo.excludepattern:
                patterns = b'\0'.join(self._localrepo.excludepattern)
                excludecap = b"excludepattern=" + patterns
                bundlecaps.append(excludecap)
            opts[b'bundlecaps'] = b','.join(bundlecaps)
###END###
def _sendrequest(self, command, args, **opts):
            self._updatecallstreamopts(command, args)
            return super(remotefilepeer, self)._sendrequest(
                command, args, **opts
            )
###END###
def _callstream(self, command, **opts):
            supertype = super(remotefilepeer, self)
            if not util.safehasattr(supertype, '_sendrequest'):
                self._updatecallstreamopts(command, pycompat.byteskwargs(opts))
            return super(remotefilepeer, self)._callstream(command, **opts)
###END###
def __init__(self):
                    self.missingids = []
                    self.connected = True
###END###
def close(self):
                    pass
###END###
def request(self, value, flush=True):
                    lines = value.split(b"\n")
                    if lines[0] != b"get":
                        return
                    self.missingids = lines[2:-1]
                    self.missingids.append(b'0')
###END###
def receiveline(self):
                    if len(self.missingids) > 0:
                        return self.missingids.pop(0)
                    return None
###END###
def generate(self, commonrevs, clnodes, fastpathlinkrev, source, **kwargs):
        if shallowutil.isenabled(self._repo):
            fastpathlinkrev = False

        return super(shallowcg1packer, self).generate(
            commonrevs, clnodes, fastpathlinkrev, source, **kwargs
        )
###END###
def group(self, nodelist, rlog, lookup, units=None, reorder=None):
        return shallowgroup(
            shallowcg1packer, self, nodelist, rlog, lookup, units=units
        )
###END###
def generatefiles(self, changedfiles, *args, **kwargs):
        try:
            linknodes, commonrevs, source = args
        except ValueError:
            commonrevs, source, mfdicts, fastpathlinkrev, fnodes, clrevs = args
        if shallowutil.isenabled(self._repo):
            repo = self._repo
            if isinstance(repo, bundlerepo.bundlerepository):
                # If the bundle contains filelogs, we can't pull from it, since
                # bundlerepo is heavily tied to revlogs. Instead require that
                # the user use unbundle instead.
                # Force load the filelog data.
                bundlerepo.bundlerepository.file(repo, b'foo')
                if repo._cgfilespos:
                    raise error.Abort(
                        b"cannot pull from full bundles",
                        hint=b"use `hg unbundle` instead",
                    )
                return []
            filestosend = self.shouldaddfilegroups(source)
            if filestosend == NoFiles:
                changedfiles = list(
                    [f for f in changedfiles if not repo.shallowmatch(f)]
                )

        return super(shallowcg1packer, self).generatefiles(
            changedfiles, *args, **kwargs
        )
###END###
def shouldaddfilegroups(self, source):
        repo = self._repo
        if not shallowutil.isenabled(repo):
            return AllFiles

        if source == b"push" or source == b"bundle":
            return AllFiles

        # We won't actually strip the files, but we should put them in any
        # backup bundle generated by strip (especially for cases like narrow's
        # `hg tracked --removeinclude`, as failing to do so means that the
        # "saved" changesets during a strip won't have their files reapplied and
        # thus their linknode adjusted, if necessary).
        if source == b"strip":
            cfg = repo.ui.config(b'remotefilelog', b'strip.includefiles')
            if cfg == b'local':
                return LocalFiles
            elif cfg != b'none':
                return AllFiles

        caps = self._bundlecaps or []
        if source == b"serve" or source == b"pull":
            if constants.BUNDLE2_CAPABLITY in caps:
                return LocalFiles
            else:
                # Serving to a full repo requires us to serve everything
                repo.ui.warn(_(b"pulling from a shallow repo\n"))
                return AllFiles

        return NoFiles
###END###
def prune(self, rlog, missing, commonrevs):
        if not isinstance(rlog, remotefilelog.remotefilelog):
            return super(shallowcg1packer, self).prune(
                rlog, missing, commonrevs
            )

        repo = self._repo
        results = []
        for fnode in missing:
            fctx = repo.filectx(rlog.filename, fileid=fnode)
            if fctx.linkrev() not in commonrevs:
                results.append(fnode)
        return results
###END###
def nodechunk(self, revlog, node, prevnode, linknode):
        prefix = b''
        if prevnode == revlog.nullid:
            delta = revlog.rawdata(node)
            prefix = mdiff.trivialdiffheader(len(delta))
        else:
            # Actually uses remotefilelog.revdiff which works on nodes, not revs
            delta = revlog.revdiff(prevnode, node)
        p1, p2 = revlog.parents(node)
        flags = revlog.flags(node)
        meta = self.builddeltaheader(node, p1, p2, prevnode, linknode, flags)
        meta += prefix
        l = len(meta) + len(delta)
        yield changegroup.chunkheader(l)
        yield meta
        yield delta
###END###
def __init__(self, ui, path):
        super(datapackstore, self).__init__(ui, path)
###END###
def getpack(self, path):
        return datapack(path)
###END###
def get(self, name, node):
        raise RuntimeError(b"must use getdeltachain with datapackstore")
###END###
def getmeta(self, name, node):
        for pack in self.packs:
            try:
                return pack.getmeta(name, node)
            except KeyError:
                pass

        for pack in self.refresh():
            try:
                return pack.getmeta(name, node)
            except KeyError:
                pass

        raise KeyError((name, hex(node)))
###END###
def getdelta(self, name, node):
        for pack in self.packs:
            try:
                return pack.getdelta(name, node)
            except KeyError:
                pass

        for pack in self.refresh():
            try:
                return pack.getdelta(name, node)
            except KeyError:
                pass

        raise KeyError((name, hex(node)))
###END###
def getdeltachain(self, name, node):
        for pack in self.packs:
            try:
                return pack.getdeltachain(name, node)
            except KeyError:
                pass

        for pack in self.refresh():
            try:
                return pack.getdeltachain(name, node)
            except KeyError:
                pass

        raise KeyError((name, hex(node)))
###END###
def add(self, name, node, data):
        raise RuntimeError(b"cannot add to datapackstore")
###END###
def getmissing(self, keys):
        missing = []
        for name, node in keys:
            value = self._find(node)
            if not value:
                missing.append((name, node))

        return missing
###END###
def get(self, name, node):
        raise RuntimeError(
            b"must use getdeltachain with datapack (%s:%s)" % (name, hex(node))
        )
###END###
def getmeta(self, name, node):
        value = self._find(node)
        if value is None:
            raise KeyError((name, hex(node)))

        node, deltabaseoffset, offset, size = value
        rawentry = self._data[offset : offset + size]

        # see docstring of mutabledatapack for the format
        offset = 0
        offset += struct.unpack_from(b'!H', rawentry, offset)[0] + 2  # filename
        offset += 40  # node, deltabase node
        offset += struct.unpack_from(b'!Q', rawentry, offset)[0] + 8  # delta

        metalen = struct.unpack_from(b'!I', rawentry, offset)[0]
        offset += 4

        meta = shallowutil.parsepackmeta(rawentry[offset : offset + metalen])

        return meta
###END###
def getdelta(self, name, node):
        value = self._find(node)
        if value is None:
            raise KeyError((name, hex(node)))

        node, deltabaseoffset, offset, size = value
        entry = self._readentry(offset, size, getmeta=True)
        filename, node, deltabasenode, delta, meta = entry

        # If we've read a lot of data from the mmap, free some memory.
        self.freememory()

        return delta, filename, deltabasenode, meta
###END###
def getdeltachain(self, name, node):
        value = self._find(node)
        if value is None:
            raise KeyError((name, hex(node)))

        params = self.params

        # Precompute chains
        chain = [value]
        deltabaseoffset = value[1]
        entrylen = self.INDEXENTRYLENGTH
        while (
            deltabaseoffset != FULLTEXTINDEXMARK
            and deltabaseoffset != NOBASEINDEXMARK
        ):
            loc = params.indexstart + deltabaseoffset
            value = struct.unpack(
                self.INDEXFORMAT, self._index[loc : loc + entrylen]
            )
            deltabaseoffset = value[1]
            chain.append(value)

        # Read chain data
        deltachain = []
        for node, deltabaseoffset, offset, size in chain:
            filename, node, deltabasenode, delta = self._readentry(offset, size)
            deltachain.append((filename, node, filename, deltabasenode, delta))

        # If we've read a lot of data from the mmap, free some memory.
        self.freememory()

        return deltachain
###END###
def _readentry(self, offset, size, getmeta=False):
        rawentry = self._data[offset : offset + size]
        self._pagedin += len(rawentry)

        # <2 byte len> + <filename>
        lengthsize = 2
        filenamelen = struct.unpack(b'!H', rawentry[:2])[0]
        filename = rawentry[lengthsize : lengthsize + filenamelen]

        # <20 byte node> + <20 byte deltabase>
        nodestart = lengthsize + filenamelen
        deltabasestart = nodestart + NODELENGTH
        node = rawentry[nodestart:deltabasestart]
        deltabasenode = rawentry[deltabasestart : deltabasestart + NODELENGTH]

        # <8 byte len> + <delta>
        deltastart = deltabasestart + NODELENGTH
        rawdeltalen = rawentry[deltastart : deltastart + 8]
        deltalen = struct.unpack(b'!Q', rawdeltalen)[0]

        delta = rawentry[deltastart + 8 : deltastart + 8 + deltalen]
        delta = self._decompress(delta)

        if getmeta:
            metastart = deltastart + 8 + deltalen
            metalen = struct.unpack_from(b'!I', rawentry, metastart)[0]

            rawmeta = rawentry[metastart + 4 : metastart + 4 + metalen]
            meta = shallowutil.parsepackmeta(rawmeta)
            return filename, node, deltabasenode, delta, meta
        else:
            return filename, node, deltabasenode, delta
###END###
def _decompress(self, data):
        return zlib.decompress(data)
###END###
def add(self, name, node, data):
        raise RuntimeError(b"cannot add to datapack (%s:%s)" % (name, node))
###END###
def _find(self, node):
        params = self.params
        fanoutkey = struct.unpack(
            params.fanoutstruct, node[: params.fanoutprefix]
        )[0]
        fanout = self._fanouttable

        start = fanout[fanoutkey] + params.indexstart
        indexend = self._indexend

        # Scan forward to find the first non-same entry, which is the upper
        # bound.
        for i in pycompat.xrange(fanoutkey + 1, params.fanoutcount):
            end = fanout[i] + params.indexstart
            if end != start:
                break
        else:
            end = indexend

        # Bisect between start and end to find node
        index = self._index
        startnode = index[start : start + NODELENGTH]
        endnode = index[end : end + NODELENGTH]
        entrylen = self.INDEXENTRYLENGTH
        if startnode == node:
            entry = index[start : start + entrylen]
        elif endnode == node:
            entry = index[end : end + entrylen]
        else:
            while start < end - entrylen:
                mid = start + (end - start) // 2
                mid = mid - ((mid - params.indexstart) % entrylen)
                midnode = index[mid : mid + NODELENGTH]
                if midnode == node:
                    entry = index[mid : mid + entrylen]
                    break
                if node > midnode:
                    start = mid
                elif node < midnode:
                    end = mid
            else:
                return None

        return struct.unpack(self.INDEXFORMAT, entry)
###END###
def markledger(self, ledger, options=None):
        for filename, node in self:
            ledger.markdataentry(self, filename, node)
###END###
def cleanup(self, ledger):
        entries = ledger.sources.get(self, [])
        allkeys = set(self)
        repackedkeys = {
            (e.filename, e.node) for e in entries if e.datarepacked or e.gced
        }

        if len(allkeys - repackedkeys) == 0:
            if self.path not in ledger.created:
                util.unlinkpath(self.indexpath, ignoremissing=True)
                util.unlinkpath(self.packpath, ignoremissing=True)
###END###
def __iter__(self):
        for f, n, deltabase, deltalen in self.iterentries():
            yield f, n
###END###
def iterentries(self):
        # Start at 1 to skip the header
        offset = 1
        data = self._data
        while offset < self.datasize:
            oldoffset = offset

            # <2 byte len> + <filename>
            filenamelen = struct.unpack(b'!H', data[offset : offset + 2])[0]
            offset += 2
            filename = data[offset : offset + filenamelen]
            offset += filenamelen

            # <20 byte node>
            node = data[offset : offset + constants.NODESIZE]
            offset += constants.NODESIZE
            # <20 byte deltabase>
            deltabase = data[offset : offset + constants.NODESIZE]
            offset += constants.NODESIZE

            # <8 byte len> + <delta>
            rawdeltalen = data[offset : offset + 8]
            deltalen = struct.unpack(b'!Q', rawdeltalen)[0]
            offset += 8

            # TODO(augie): we should store a header that is the
            # uncompressed size.
            uncompressedlen = len(
                self._decompress(data[offset : offset + deltalen])
            )
            offset += deltalen

            # <4 byte len> + <metadata-list>
            metalen = struct.unpack_from(b'!I', data, offset)[0]
            offset += 4 + metalen

            yield (filename, node, deltabase, uncompressedlen)

            # If we've read a lot of data from the mmap, free some memory.
            self._pagedin += offset - oldoffset
            if self.freememory():
                data = self._data
###END###
def _compress(self, data):
        return zlib.compress(data)
###END###
def add(self, name, node, deltabasenode, delta, metadata=None):
        # metadata is a dict, ex. {METAKEYFLAG: flag}
        if len(name) > 2 ** 16:
            raise RuntimeError(_(b"name too long %s") % name)
        if len(node) != 20:
            raise RuntimeError(_(b"node should be 20 bytes %s") % node)

        if node in self.entries:
            # The revision has already been added
            return

        # TODO: allow configurable compression
        delta = self._compress(delta)

        rawdata = b''.join(
            (
                struct.pack(b'!H', len(name)),  # unsigned 2 byte int
                name,
                node,
                deltabasenode,
                struct.pack(b'!Q', len(delta)),  # unsigned 8 byte int
                delta,
            )
        )

        # v1 support metadata
        rawmeta = shallowutil.buildpackmeta(metadata)
        rawdata += struct.pack(b'!I', len(rawmeta))  # unsigned 4 byte
        rawdata += rawmeta

        offset = self.packfp.tell()

        size = len(rawdata)

        self.entries[node] = (deltabasenode, offset, size)

        self.writeraw(rawdata)
###END###
def createindex(self, nodelocations, indexoffset):
        entries = sorted(
            (n, db, o, s) for n, (db, o, s) in pycompat.iteritems(self.entries)
        )

        rawindex = b''
        fmt = self.INDEXFORMAT
        for node, deltabase, offset, size in entries:
            if deltabase == sha1nodeconstants.nullid:
                deltabaselocation = FULLTEXTINDEXMARK
            else:
                # Instead of storing the deltabase node in the index, let's
                # store a pointer directly to the index entry for the deltabase.
                deltabaselocation = nodelocations.get(
                    deltabase, NOBASEINDEXMARK
                )

            entry = struct.pack(fmt, node, deltabaselocation, offset, size)
            rawindex += entry

        return rawindex
###END###
