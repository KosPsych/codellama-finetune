def commit(self, *innerpats, **inneropts):
            extras = opts.get('extra')
            for raw in extras:
                if b'=' not in raw:
                    msg = _(
                        b"unable to parse '%s', should follow "
                        b"KEY=VALUE format"
                    )
                    raise error.Abort(msg % raw)
                k, v = raw.split(b'=', 1)
                if not k:
                    msg = _(b"unable to parse '%s', keys can't be empty")
                    raise error.Abort(msg % raw)
                if re.search(br'[^\w-]', k):
                    msg = _(
                        b"keys can only contain ascii letters, digits,"
                        b" '_' and '-'"
                    )
                    raise error.Abort(msg)
                if k in usedinternally:
                    msg = _(
                        b"key '%s' is used internally, can't be set "
                        b"manually"
                    )
                    raise error.Abort(msg % k)
                inneropts['extra'][k] = v
            return super(repoextra, self).commit(*innerpats, **inneropts)
###END###
def __init__(self, node, name):
        self.node, self.name = node, name
###END###
def __bytes__(self):
        return hex(self.node) + b':' + self.name
###END###
def __init__(self, pf, plainmode=False):
        def eatdiff(lines):
            while lines:
                l = lines[-1]
                if (
                    l.startswith(b"diff -")
                    or l.startswith(b"Index:")
                    or l.startswith(b"===========")
                ):
                    del lines[-1]
                else:
                    break

        def eatempty(lines):
            while lines:
                if not lines[-1].strip():
                    del lines[-1]
                else:
                    break

        message = []
        comments = []
        user = None
        date = None
        parent = None
        format = None
        subject = None
        branch = None
        nodeid = None
        diffstart = 0

        for line in open(pf, b'rb'):
            line = line.rstrip()
            if line.startswith(b'diff --git') or (
                diffstart and line.startswith(b'+++ ')
            ):
                diffstart = 2
                break
            diffstart = 0  # reset
            if line.startswith(b"--- "):
                diffstart = 1
                continue
            elif format == b"hgpatch":
                # parse values when importing the result of an hg export
                if line.startswith(b"# User "):
                    user = line[7:]
                elif line.startswith(b"# Date "):
                    date = line[7:]
                elif line.startswith(b"# Parent "):
                    parent = line[9:].lstrip()  # handle double trailing space
                elif line.startswith(b"# Branch "):
                    branch = line[9:]
                elif line.startswith(b"# Node ID "):
                    nodeid = line[10:]
                elif not line.startswith(b"# ") and line:
                    message.append(line)
                    format = None
            elif line == b'# HG changeset patch':
                message = []
                format = b"hgpatch"
            elif format != b"tagdone" and (
                line.startswith(b"Subject: ") or line.startswith(b"subject: ")
            ):
                subject = line[9:]
                format = b"tag"
            elif format != b"tagdone" and (
                line.startswith(b"From: ") or line.startswith(b"from: ")
            ):
                user = line[6:]
                format = b"tag"
            elif format != b"tagdone" and (
                line.startswith(b"Date: ") or line.startswith(b"date: ")
            ):
                date = line[6:]
                format = b"tag"
            elif format == b"tag" and line == b"":
                # when looking for tags (subject: from: etc) they
                # end once you find a blank line in the source
                format = b"tagdone"
            elif message or line:
                message.append(line)
            comments.append(line)

        eatdiff(message)
        eatdiff(comments)
        # Remember the exact starting line of the patch diffs before consuming
        # empty lines, for external use by TortoiseHg and others
        self.diffstartline = len(comments)
        eatempty(message)
        eatempty(comments)

        # make sure message isn't empty
        if format and format.startswith(b"tag") and subject:
            message.insert(0, subject)

        self.message = message
        self.comments = comments
        self.user = user
        self.date = date
        self.parent = parent
        # nodeid and branch are for external use by TortoiseHg and others
        self.nodeid = nodeid
        self.branch = branch
        self.haspatch = diffstart > 1
        self.plainmode = (
            plainmode
            or b'# HG changeset patch' not in self.comments
            and any(
                c.startswith(b'Date: ') or c.startswith(b'From: ')
                for c in self.comments
            )
        )
###END###
def setuser(self, user):
        try:
            inserthgheader(self.comments, b'# User ', user)
        except ValueError:
            if self.plainmode:
                insertplainheader(self.comments, b'From', user)
            else:
                tmp = [b'# HG changeset patch', b'# User ' + user]
                self.comments = tmp + self.comments
        self.user = user
###END###
def setdate(self, date):
        try:
            inserthgheader(self.comments, b'# Date ', date)
        except ValueError:
            if self.plainmode:
                insertplainheader(self.comments, b'Date', date)
            else:
                tmp = [b'# HG changeset patch', b'# Date ' + date]
                self.comments = tmp + self.comments
        self.date = date
###END###
def setparent(self, parent):
        try:
            inserthgheader(self.comments, b'# Parent  ', parent)
        except ValueError:
            if not self.plainmode:
                tmp = [b'# HG changeset patch', b'# Parent  ' + parent]
                self.comments = tmp + self.comments
        self.parent = parent
###END###
def setmessage(self, message):
        if self.comments:
            self._delmsg()
        self.message = [message]
        if message:
            if self.plainmode and self.comments and self.comments[-1]:
                self.comments.append(b'')
            self.comments.append(message)
###END###
def __bytes__(self):
        s = b'\n'.join(self.comments).rstrip()
        if not s:
            return b''
        return s + b'\n\n'
###END###
def _delmsg(self):
        """Remove existing message, keeping the rest of the comments fields.
        If comments contains 'subject: ', message will prepend
        the field and a blank line."""
        if self.message:
            subj = b'subject: ' + self.message[0].lower()
            for i in pycompat.xrange(len(self.comments)):
                if subj == self.comments[i].lower():
                    del self.comments[i]
                    self.message = self.message[2:]
                    break
        ci = 0
        for mi in self.message:
            while mi != self.comments[ci]:
                ci += 1
            del self.comments[ci]
###END###
def __init__(self, ui, baseui, path, patchdir=None):
        self.basepath = path
        try:
            with open(os.path.join(path, b'patches.queue'), 'rb') as fh:
                cur = fh.read().rstrip()

            if not cur:
                curpath = os.path.join(path, b'patches')
            else:
                curpath = os.path.join(path, b'patches-' + cur)
        except IOError:
            curpath = os.path.join(path, b'patches')
        self.path = patchdir or curpath
        self.opener = vfsmod.vfs(self.path)
        self.ui = ui
        self.baseui = baseui
        self.applieddirty = False
        self.seriesdirty = False
        self.added = []
        self.seriespath = b"series"
        self.statuspath = b"status"
        self.guardspath = b"guards"
        self.activeguards = None
        self.guardsdirty = False
        # Handle mq.git as a bool with extended values
        gitmode = ui.config(b'mq', b'git').lower()
        boolmode = stringutil.parsebool(gitmode)
        if boolmode is not None:
            if boolmode:
                gitmode = b'yes'
            else:
                gitmode = b'no'
        self.gitmode = gitmode
        # deprecated config: mq.plain
        self.plainmode = ui.configbool(b'mq', b'plain')
        self.checkapplied = True
###END###
def applied(self):
        def parselines(lines):
            for l in lines:
                entry = l.split(b':', 1)
                if len(entry) > 1:
                    n, name = entry
                    yield statusentry(bin(n), name)
                elif l.strip():
                    self.ui.warn(
                        _(b'malformated mq status line: %s\n')
                        % stringutil.pprint(entry)
                    )
                # else we ignore empty lines

        try:
            lines = self.opener.read(self.statuspath).splitlines()
            return list(parselines(lines))
        except IOError as e:
            if e.errno == errno.ENOENT:
                return []
            raise
###END###
def fullseries(self):
        try:
            return self.opener.read(self.seriespath).splitlines()
        except IOError as e:
            if e.errno == errno.ENOENT:
                return []
            raise
###END###
def series(self):
        self.parseseries()
        return self.series
###END###
def seriesguards(self):
        self.parseseries()
        return self.seriesguards
###END###
def invalidate(self):
        for a in 'applied fullseries series seriesguards'.split():
            if a in self.__dict__:
                delattr(self, a)
        self.applieddirty = False
        self.seriesdirty = False
        self.guardsdirty = False
        self.activeguards = None
###END###
def diffopts(self, opts=None, patchfn=None, plain=False):
        """Return diff options tweaked for this mq use, possibly upgrading to
        git format, and possibly plain and without lossy options."""
        diffopts = patchmod.difffeatureopts(
            self.ui,
            opts,
            git=True,
            whitespace=not plain,
            formatchanging=not plain,
        )
        if self.gitmode == b'auto':
            diffopts.upgrade = True
        elif self.gitmode == b'keep':
            pass
        elif self.gitmode in (b'yes', b'no'):
            diffopts.git = self.gitmode == b'yes'
        else:
            raise error.Abort(
                _(b'mq.git option can be auto/keep/yes/no got %s')
                % self.gitmode
            )
        if patchfn:
            diffopts = self.patchopts(diffopts, patchfn)
        return diffopts
###END###
def patchopts(self, diffopts, *patches):
        """Return a copy of input diff options with git set to true if
        referenced patch is a git patch and should be preserved as such.
        """
        diffopts = diffopts.copy()
        if not diffopts.git and self.gitmode == b'keep':
            for patchfn in patches:
                patchf = self.opener(patchfn, b'r')
                # if the patch was a git patch, refresh it as a git patch
                diffopts.git = any(
                    line.startswith(b'diff --git') for line in patchf
                )
                patchf.close()
        return diffopts
###END###
def join(self, *p):
        return os.path.join(self.path, *p)
###END###
def findseries(self, patch):
        def matchpatch(l):
            l = l.split(b'#', 1)[0]
            return l.strip() == patch

        for index, l in enumerate(self.fullseries):
            if matchpatch(l):
                return index
        return None
###END###
def parseseries(self):
        self.series = []
        self.seriesguards = []
        for l in self.fullseries:
            h = l.find(b'#')
            if h == -1:
                patch = l
                comment = b''
            elif h == 0:
                continue
            else:
                patch = l[:h]
                comment = l[h:]
            patch = patch.strip()
            if patch:
                if patch in self.series:
                    raise error.Abort(
                        _(b'%s appears more than once in %s')
                        % (patch, self.join(self.seriespath))
                    )
                self.series.append(patch)
                self.seriesguards.append(self.guard_re.findall(comment))
###END###
def checkguard(self, guard):
        if not guard:
            return _(b'guard cannot be an empty string')
        bad_chars = b'# \t\r\n\f'
        first = guard[0]
        if first in b'-+':
            return _(b'guard %r starts with invalid character: %r') % (
                guard,
                first,
            )
        for c in bad_chars:
            if c in guard:
                return _(b'invalid character in guard %r: %r') % (guard, c)
###END###
def setactive(self, guards):
        for guard in guards:
            bad = self.checkguard(guard)
            if bad:
                raise error.Abort(bad)
        guards = sorted(set(guards))
        self.ui.debug(b'active guards: %s\n' % b' '.join(guards))
        self.activeguards = guards
        self.guardsdirty = True
###END###
def active(self):
        if self.activeguards is None:
            self.activeguards = []
            try:
                guards = self.opener.read(self.guardspath).split()
            except IOError as err:
                if err.errno != errno.ENOENT:
                    raise
                guards = []
            for i, guard in enumerate(guards):
                bad = self.checkguard(guard)
                if bad:
                    self.ui.warn(
                        b'%s:%d: %s\n'
                        % (self.join(self.guardspath), i + 1, bad)
                    )
                else:
                    self.activeguards.append(guard)
        return self.activeguards
###END###
def setguards(self, idx, guards):
        for g in guards:
            if len(g) < 2:
                raise error.Abort(_(b'guard %r too short') % g)
            if g[0] not in b'-+':
                raise error.Abort(_(b'guard %r starts with invalid char') % g)
            bad = self.checkguard(g[1:])
            if bad:
                raise error.Abort(bad)
        drop = self.guard_re.sub(b'', self.fullseries[idx])
        self.fullseries[idx] = drop + b''.join([b' #' + g for g in guards])
        self.parseseries()
        self.seriesdirty = True
###END###
def pushable(self, idx):
        if isinstance(idx, bytes):
            idx = self.series.index(idx)
        patchguards = self.seriesguards[idx]
        if not patchguards:
            return True, None
        guards = self.active()
        exactneg = [
            g for g in patchguards if g.startswith(b'-') and g[1:] in guards
        ]
        if exactneg:
            return False, stringutil.pprint(exactneg[0])
        pos = [g for g in patchguards if g.startswith(b'+')]
        exactpos = [g for g in pos if g[1:] in guards]
        if pos:
            if exactpos:
                return True, stringutil.pprint(exactpos[0])
            return False, b' '.join([stringutil.pprint(p) for p in pos])
        return True, b''
###END###
def explainpushable(self, idx, all_patches=False):
        if all_patches:
            write = self.ui.write
        else:
            write = self.ui.warn

        if all_patches or self.ui.verbose:
            if isinstance(idx, bytes):
                idx = self.series.index(idx)
            pushable, why = self.pushable(idx)
            if all_patches and pushable:
                if why is None:
                    write(
                        _(b'allowing %s - no guards in effect\n')
                        % self.series[idx]
                    )
                else:
                    if not why:
                        write(
                            _(b'allowing %s - no matching negative guards\n')
                            % self.series[idx]
                        )
                    else:
                        write(
                            _(b'allowing %s - guarded by %s\n')
                            % (self.series[idx], why)
                        )
            if not pushable:
                if why:
                    write(
                        _(b'skipping %s - guarded by %s\n')
                        % (self.series[idx], why)
                    )
                else:
                    write(
                        _(b'skipping %s - no matching guards\n')
                        % self.series[idx]
                    )
###END###
def savedirty(self):
        def writelist(items, path):
            fp = self.opener(path, b'wb')
            for i in items:
                fp.write(b"%s\n" % i)
            fp.close()

        if self.applieddirty:
            writelist(map(bytes, self.applied), self.statuspath)
            self.applieddirty = False
        if self.seriesdirty:
            writelist(self.fullseries, self.seriespath)
            self.seriesdirty = False
        if self.guardsdirty:
            writelist(self.activeguards, self.guardspath)
            self.guardsdirty = False
        if self.added:
            qrepo = self.qrepo()
            if qrepo:
                qrepo[None].add(f for f in self.added if f not in qrepo[None])
            self.added = []
###END###
def removeundo(self, repo):
        undo = repo.sjoin(b'undo')
        if not os.path.exists(undo):
            return
        try:
            os.unlink(undo)
        except OSError as inst:
            self.ui.warn(
                _(b'error removing undo: %s\n') % stringutil.forcebytestr(inst)
            )
###END###
def backup(self, repo, files, copy=False):
        # backup local changes in --force case
        for f in sorted(files):
            absf = repo.wjoin(f)
            if os.path.lexists(absf):
                absorig = scmutil.backuppath(self.ui, repo, f)
                self.ui.note(
                    _(b'saving current version of %s as %s\n')
                    % (f, os.path.relpath(absorig))
                )

                if copy:
                    util.copyfile(absf, absorig)
                else:
                    util.rename(absf, absorig)
###END###
def printdiff(
        self,
        repo,
        diffopts,
        node1,
        node2=None,
        files=None,
        fp=None,
        changes=None,
        opts=None,
    ):
        if opts is None:
            opts = {}
        stat = opts.get(b'stat')
        m = scmutil.match(repo[node1], files, opts)
        logcmdutil.diffordiffstat(
            self.ui,
            repo,
            diffopts,
            repo[node1],
            repo[node2],
            m,
            changes,
            stat,
            fp,
        )
###END###
def mergeone(self, repo, mergeq, head, patch, rev, diffopts):
        # first try just applying the patch
        (err, n) = self.apply(
            repo, [patch], update_status=False, strict=True, merge=rev
        )

        if err == 0:
            return (err, n)

        if n is None:
            raise error.Abort(_(b"apply failed for patch %s") % patch)

        self.ui.warn(_(b"patch didn't work out, merging %s\n") % patch)

        # apply failed, strip away that rev and merge.
        hg.clean(repo, head)
        strip(self.ui, repo, [n], update=False, backup=False)

        ctx = repo[rev]
        ret = hg.merge(ctx, remind=False)
        if ret:
            raise error.Abort(_(b"update returned %d") % ret)
        n = newcommit(repo, None, ctx.description(), ctx.user(), force=True)
        if n is None:
            raise error.Abort(_(b"repo commit failed"))
        try:
            ph = patchheader(mergeq.join(patch), self.plainmode)
        except Exception:
            raise error.Abort(_(b"unable to read %s") % patch)

        diffopts = self.patchopts(diffopts, patch)
        patchf = self.opener(patch, b"w")
        comments = bytes(ph)
        if comments:
            patchf.write(comments)
        self.printdiff(repo, diffopts, head, n, fp=patchf)
        patchf.close()
        self.removeundo(repo)
        return (0, n)
###END###
def qparents(self, repo, rev=None):
        """return the mq handled parent or p1

        In some case where mq get himself in being the parent of a merge the
        appropriate parent may be p2.
        (eg: an in progress merge started with mq disabled)

        If no parent are managed by mq, p1 is returned.
        """
        if rev is None:
            (p1, p2) = repo.dirstate.parents()
            if p2 == repo.nullid:
                return p1
            if not self.applied:
                return None
            return self.applied[-1].node
        p1, p2 = repo.changelog.parents(rev)
        if p2 != repo.nullid and p2 in [x.node for x in self.applied]:
            return p2
        return p1
###END###
def mergepatch(self, repo, mergeq, series, diffopts):
        if not self.applied:
            # each of the patches merged in will have two parents.  This
            # can confuse the qrefresh, qdiff, and strip code because it
            # needs to know which parent is actually in the patch queue.
            # so, we insert a merge marker with only one parent.  This way
            # the first patch in the queue is never a merge patch
            #
            pname = b".hg.patches.merge.marker"
            n = newcommit(repo, None, b'[mq]: merge marker', force=True)
            self.removeundo(repo)
            self.applied.append(statusentry(n, pname))
            self.applieddirty = True

        head = self.qparents(repo)

        for patch in series:
            patch = mergeq.lookup(patch, strict=True)
            if not patch:
                self.ui.warn(_(b"patch %s does not exist\n") % patch)
                return (1, None)
            pushable, reason = self.pushable(patch)
            if not pushable:
                self.explainpushable(patch, all_patches=True)
                continue
            info = mergeq.isapplied(patch)
            if not info:
                self.ui.warn(_(b"patch %s is not applied\n") % patch)
                return (1, None)
            rev = info[1]
            err, head = self.mergeone(repo, mergeq, head, patch, rev, diffopts)
            if head:
                self.applied.append(statusentry(head, patch))
                self.applieddirty = True
            if err:
                return (err, head)
        self.savedirty()
        return (0, head)
###END###
def patch(self, repo, patchfile):
        """Apply patchfile  to the working directory.
        patchfile: name of patch file"""
        files = set()
        try:
            fuzz = patchmod.patch(
                self.ui, repo, patchfile, strip=1, files=files, eolmode=None
            )
            return (True, list(files), fuzz)
        except Exception as inst:
            self.ui.note(stringutil.forcebytestr(inst) + b'\n')
            if not self.ui.verbose:
                self.ui.warn(_(b"patch failed, unable to continue (try -v)\n"))
            self.ui.traceback()
            return (False, list(files), False)
###END###
def apply(
        self,
        repo,
        series,
        list=False,
        update_status=True,
        strict=False,
        patchdir=None,
        merge=None,
        all_files=None,
        tobackup=None,
        keepchanges=False,
    ):
        wlock = lock = tr = None
        try:
            wlock = repo.wlock()
            lock = repo.lock()
            tr = repo.transaction(b"qpush")
            try:
                ret = self._apply(
                    repo,
                    series,
                    list,
                    update_status,
                    strict,
                    patchdir,
                    merge,
                    all_files=all_files,
                    tobackup=tobackup,
                    keepchanges=keepchanges,
                )
                tr.close()
                self.savedirty()
                return ret
            except AbortNoCleanup:
                tr.close()
                self.savedirty()
                raise
            except:  # re-raises
                try:
                    tr.abort()
                finally:
                    self.invalidate()
                raise
        finally:
            release(tr, lock, wlock)
            self.removeundo(repo)
###END###
def _apply(
        self,
        repo,
        series,
        list=False,
        update_status=True,
        strict=False,
        patchdir=None,
        merge=None,
        all_files=None,
        tobackup=None,
        keepchanges=False,
    ):
        """returns (error, hash)

        error = 1 for unable to read, 2 for patch failed, 3 for patch
        fuzz. tobackup is None or a set of files to backup before they
        are modified by a patch.
        """
        # TODO unify with commands.py
        if not patchdir:
            patchdir = self.path
        err = 0
        n = None
        for patchname in series:
            pushable, reason = self.pushable(patchname)
            if not pushable:
                self.explainpushable(patchname, all_patches=True)
                continue
            self.ui.status(_(b"applying %s\n") % patchname)
            pf = os.path.join(patchdir, patchname)

            try:
                ph = patchheader(self.join(patchname), self.plainmode)
            except IOError:
                self.ui.warn(_(b"unable to read %s\n") % patchname)
                err = 1
                break

            message = ph.message
            if not message:
                # The commit message should not be translated
                message = b"imported patch %s\n" % patchname
            else:
                if list:
                    # The commit message should not be translated
                    message.append(b"\nimported patch %s" % patchname)
                message = b'\n'.join(message)

            if ph.haspatch:
                if tobackup:
                    touched = patchmod.changedfiles(self.ui, repo, pf)
                    touched = set(touched) & tobackup
                    if touched and keepchanges:
                        raise AbortNoCleanup(
                            _(b"conflicting local changes found"),
                            hint=_(b"did you forget to qrefresh?"),
                        )
                    self.backup(repo, touched, copy=True)
                    tobackup = tobackup - touched
                (patcherr, files, fuzz) = self.patch(repo, pf)
                if all_files is not None:
                    all_files.update(files)
                patcherr = not patcherr
            else:
                self.ui.warn(_(b"patch %s is empty\n") % patchname)
                patcherr, files, fuzz = 0, [], 0

            if merge and files:
                # Mark as removed/merged and update dirstate parent info
                with repo.dirstate.parentchange():
                    for f in files:
                        repo.dirstate.update_file_p1(f, p1_tracked=True)
                    p1 = repo.dirstate.p1()
                    repo.setparents(p1, merge)

            if all_files and b'.hgsubstate' in all_files:
                wctx = repo[None]
                pctx = repo[b'.']
                overwrite = False
                mergedsubstate = subrepoutil.submerge(
                    repo, pctx, wctx, wctx, overwrite
                )
                files += mergedsubstate.keys()

            match = scmutil.matchfiles(repo, files or [])
            oldtip = repo.changelog.tip()
            n = newcommit(
                repo, None, message, ph.user, ph.date, match=match, force=True
            )
            if repo.changelog.tip() == oldtip:
                raise error.Abort(
                    _(b"qpush exactly duplicates child changeset")
                )
            if n is None:
                raise error.Abort(_(b"repository commit failed"))

            if update_status:
                self.applied.append(statusentry(n, patchname))

            if patcherr:
                self.ui.warn(
                    _(b"patch failed, rejects left in working directory\n")
                )
                err = 2
                break

            if fuzz and strict:
                self.ui.warn(_(b"fuzz found when applying patch, stopping\n"))
                err = 3
                break
        return (err, n)
###END###
def _cleanup(self, patches, numrevs, keep=False):
        if not keep:
            r = self.qrepo()
            if r:
                r[None].forget(patches)
            for p in patches:
                try:
                    os.unlink(self.join(p))
                except OSError as inst:
                    if inst.errno != errno.ENOENT:
                        raise

        qfinished = []
        if numrevs:
            qfinished = self.applied[:numrevs]
            del self.applied[:numrevs]
            self.applieddirty = True

        unknown = []

        sortedseries = []
        for p in patches:
            idx = self.findseries(p)
            if idx is None:
                sortedseries.append((-1, p))
            else:
                sortedseries.append((idx, p))

        sortedseries.sort(reverse=True)
        for (i, p) in sortedseries:
            if i != -1:
                del self.fullseries[i]
            else:
                unknown.append(p)

        if unknown:
            if numrevs:
                rev = {entry.name: entry.node for entry in qfinished}
                for p in unknown:
                    msg = _(b'revision %s refers to unknown patches: %s\n')
                    self.ui.warn(msg % (short(rev[p]), p))
            else:
                msg = _(b'unknown patches: %s\n')
                raise error.Abort(b''.join(msg % p for p in unknown))

        self.parseseries()
        self.seriesdirty = True
        return [entry.node for entry in qfinished]
###END###
def _revpatches(self, repo, revs):
        firstrev = repo[self.applied[0].node].rev()
        patches = []
        for i, rev in enumerate(revs):

            if rev < firstrev:
                raise error.Abort(_(b'revision %d is not managed') % rev)

            ctx = repo[rev]
            base = self.applied[i].node
            if ctx.node() != base:
                msg = _(b'cannot delete revision %d above applied patches')
                raise error.Abort(msg % rev)

            patch = self.applied[i].name
            for fmt in (b'[mq]: %s', b'imported patch %s'):
                if ctx.description() == fmt % patch:
                    msg = _(b'patch %s finalized without changeset message\n')
                    repo.ui.status(msg % patch)
                    break

            patches.append(patch)
        return patches
###END###
def finish(self, repo, revs):
        # Manually trigger phase computation to ensure phasedefaults is
        # executed before we remove the patches.
        repo._phasecache
        patches = self._revpatches(repo, sorted(revs))
        qfinished = self._cleanup(patches, len(patches))
        if qfinished and repo.ui.configbool(b'mq', b'secret'):
            # only use this logic when the secret option is added
            oldqbase = repo[qfinished[0]]
            tphase = phases.newcommitphase(repo.ui)
            if oldqbase.phase() > tphase and oldqbase.p1().phase() <= tphase:
                with repo.transaction(b'qfinish') as tr:
                    phases.advanceboundary(repo, tr, tphase, qfinished)
###END###
def delete(self, repo, patches, opts):
        if not patches and not opts.get(b'rev'):
            raise error.Abort(
                _(b'qdelete requires at least one revision or patch name')
            )

        realpatches = []
        for patch in patches:
            patch = self.lookup(patch, strict=True)
            info = self.isapplied(patch)
            if info:
                raise error.Abort(_(b"cannot delete applied patch %s") % patch)
            if patch not in self.series:
                raise error.Abort(_(b"patch %s not in series file") % patch)
            if patch not in realpatches:
                realpatches.append(patch)

        numrevs = 0
        if opts.get(b'rev'):
            if not self.applied:
                raise error.Abort(_(b'no patches applied'))
            revs = scmutil.revrange(repo, opts.get(b'rev'))
            revs.sort()
            revpatches = self._revpatches(repo, revs)
            realpatches += revpatches
            numrevs = len(revpatches)

        self._cleanup(realpatches, numrevs, opts.get(b'keep'))
###END###
def checktoppatch(self, repo):
        '''check that working directory is at qtip'''
        if self.applied:
            top = self.applied[-1].node
            patch = self.applied[-1].name
            if repo.dirstate.p1() != top:
                raise error.Abort(_(b"working directory revision is not qtip"))
            return top, patch
        return None, None
###END###
def putsubstate2changes(self, substatestate, changes):
        if isinstance(changes, list):
            mar = changes[:3]
        else:
            mar = (changes.modified, changes.added, changes.removed)
        if any((b'.hgsubstate' in files for files in mar)):
            return  # already listed up
        # not yet listed up
        if substatestate in b'a?':
            mar[1].append(b'.hgsubstate')
        elif substatestate in b'r':
            mar[2].append(b'.hgsubstate')
        else:  # modified
            mar[0].append(b'.hgsubstate')
###END###
def checklocalchanges(self, repo, force=False, refresh=True):
        excsuffix = b''
        if refresh:
            excsuffix = b', qrefresh first'
            # plain versions for i18n tool to detect them
            _(b"local changes found, qrefresh first")
            _(b"local changed subrepos found, qrefresh first")

        s = repo.status()
        if not force:
            cmdutil.checkunfinished(repo)
            if s.modified or s.added or s.removed or s.deleted:
                _(b"local changes found")  # i18n tool detection
                raise error.Abort(_(b"local changes found" + excsuffix))
            if checksubstate(repo):
                _(b"local changed subrepos found")  # i18n tool detection
                raise error.Abort(
                    _(b"local changed subrepos found" + excsuffix)
                )
        else:
            cmdutil.checkunfinished(repo, skipmerge=True)
        return s
###END###
def checkreservedname(self, name):
        if name in self._reserved:
            raise error.Abort(
                _(b'"%s" cannot be used as the name of a patch') % name
            )
        if name != name.strip():
            # whitespace is stripped by parseseries()
            raise error.Abort(
                _(b'patch name cannot begin or end with whitespace')
            )
        for prefix in (b'.hg', b'.mq'):
            if name.startswith(prefix):
                raise error.Abort(
                    _(b'patch name cannot begin with "%s"') % prefix
                )
        for c in (b'#', b':', b'\r', b'\n'):
            if c in name:
                raise error.Abort(
                    _(b'%r cannot be used in the name of a patch')
                    % pycompat.bytestr(c)
                )
###END###
def checkpatchname(self, name, force=False):
        self.checkreservedname(name)
        if not force and os.path.exists(self.join(name)):
            if os.path.isdir(self.join(name)):
                raise error.Abort(
                    _(b'"%s" already exists as a directory') % name
                )
            else:
                raise error.Abort(_(b'patch "%s" already exists') % name)
###END###
def makepatchname(self, title, fallbackname):
        """Return a suitable filename for title, adding a suffix to make
        it unique in the existing list"""
        namebase = re.sub(br'[\s\W_]+', b'_', title.lower()).strip(b'_')
        namebase = namebase[:75]  # avoid too long name (issue5117)
        if namebase:
            try:
                self.checkreservedname(namebase)
            except error.Abort:
                namebase = fallbackname
        else:
            namebase = fallbackname
        name = namebase
        i = 0
        while True:
            if name not in self.fullseries:
                try:
                    self.checkpatchname(name)
                    break
                except error.Abort:
                    pass
            i += 1
            name = b'%s__%d' % (namebase, i)
        return name
###END###
def checkkeepchanges(self, keepchanges, force):
        if force and keepchanges:
            raise error.Abort(_(b'cannot use both --force and --keep-changes'))
###END###
def new(self, repo, patchfn, *pats, **opts):
        """options:
        msg: a string or a no-argument function returning a string
        """
        opts = pycompat.byteskwargs(opts)
        msg = opts.get(b'msg')
        edit = opts.get(b'edit')
        editform = opts.get(b'editform', b'mq.qnew')
        user = opts.get(b'user')
        date = opts.get(b'date')
        if date:
            date = dateutil.parsedate(date)
        diffopts = self.diffopts({b'git': opts.get(b'git')}, plain=True)
        if opts.get(b'checkname', True):
            self.checkpatchname(patchfn)
        inclsubs = checksubstate(repo)
        if inclsubs:
            substatestate = repo.dirstate[b'.hgsubstate']
        if opts.get(b'include') or opts.get(b'exclude') or pats:
            # detect missing files in pats
            def badfn(f, msg):
                if f != b'.hgsubstate':  # .hgsubstate is auto-created
                    raise error.Abort(b'%s: %s' % (f, msg))

            match = scmutil.match(repo[None], pats, opts, badfn=badfn)
            changes = repo.status(match=match)
        else:
            changes = self.checklocalchanges(repo, force=True)
        commitfiles = list(inclsubs)
        commitfiles.extend(changes.modified)
        commitfiles.extend(changes.added)
        commitfiles.extend(changes.removed)
        match = scmutil.matchfiles(repo, commitfiles)
        if len(repo[None].parents()) > 1:
            raise error.Abort(_(b'cannot manage merge changesets'))
        self.checktoppatch(repo)
        insert = self.fullseriesend()
        with repo.wlock():
            try:
                # if patch file write fails, abort early
                p = self.opener(patchfn, b"w")
            except IOError as e:
                raise error.Abort(
                    _(b'cannot write patch "%s": %s')
                    % (patchfn, encoding.strtolocal(e.strerror))
                )
            try:
                defaultmsg = b"[mq]: %s" % patchfn
                editor = cmdutil.getcommiteditor(editform=editform)
                if edit:

                    def finishdesc(desc):
                        if desc.rstrip():
                            return desc
                        else:
                            return defaultmsg

                    # i18n: this message is shown in editor with "HG: " prefix
                    extramsg = _(b'Leave message empty to use default message.')
                    editor = cmdutil.getcommiteditor(
                        finishdesc=finishdesc,
                        extramsg=extramsg,
                        editform=editform,
                    )
                    commitmsg = msg
                else:
                    commitmsg = msg or defaultmsg

                n = newcommit(
                    repo,
                    None,
                    commitmsg,
                    user,
                    date,
                    match=match,
                    force=True,
                    editor=editor,
                )
                if n is None:
                    raise error.Abort(_(b"repo commit failed"))
                try:
                    self.fullseries[insert:insert] = [patchfn]
                    self.applied.append(statusentry(n, patchfn))
                    self.parseseries()
                    self.seriesdirty = True
                    self.applieddirty = True
                    nctx = repo[n]
                    ph = patchheader(self.join(patchfn), self.plainmode)
                    if user:
                        ph.setuser(user)
                    if date:
                        ph.setdate(b'%d %d' % date)
                    ph.setparent(hex(nctx.p1().node()))
                    msg = nctx.description().strip()
                    if msg == defaultmsg.strip():
                        msg = b''
                    ph.setmessage(msg)
                    p.write(bytes(ph))
                    if commitfiles:
                        parent = self.qparents(repo, n)
                        if inclsubs:
                            self.putsubstate2changes(substatestate, changes)
                        chunks = patchmod.diff(
                            repo,
                            node1=parent,
                            node2=n,
                            changes=changes,
                            opts=diffopts,
                        )
                        for chunk in chunks:
                            p.write(chunk)
                    p.close()
                    r = self.qrepo()
                    if r:
                        r[None].add([patchfn])
                except:  # re-raises
                    repo.rollback()
                    raise
            except Exception:
                patchpath = self.join(patchfn)
                try:
                    os.unlink(patchpath)
                except OSError:
                    self.ui.warn(_(b'error unlinking %s\n') % patchpath)
                raise
            self.removeundo(repo)
###END###
def isapplied(self, patch):
        """returns (index, rev, patch)"""
        for i, a in enumerate(self.applied):
            if a.name == patch:
                return (i, a.node, a.name)
        return None
###END###
def lookup(self, patch, strict=False):
        def partialname(s):
            if s in self.series:
                return s
            matches = [x for x in self.series if s in x]
            if len(matches) > 1:
                self.ui.warn(_(b'patch name "%s" is ambiguous:\n') % s)
                for m in matches:
                    self.ui.warn(b'  %s\n' % m)
                return None
            if matches:
                return matches[0]
            if self.series and self.applied:
                if s == b'qtip':
                    return self.series[self.seriesend(True) - 1]
                if s == b'qbase':
                    return self.series[0]
            return None

        if patch in self.series:
            return patch

        if not os.path.isfile(self.join(patch)):
            try:
                sno = int(patch)
            except (ValueError, OverflowError):
                pass
            else:
                if -len(self.series) <= sno < len(self.series):
                    return self.series[sno]

            if not strict:
                res = partialname(patch)
                if res:
                    return res
                minus = patch.rfind(b'-')
                if minus >= 0:
                    res = partialname(patch[:minus])
                    if res:
                        i = self.series.index(res)
                        try:
                            off = int(patch[minus + 1 :] or 1)
                        except (ValueError, OverflowError):
                            pass
                        else:
                            if i - off >= 0:
                                return self.series[i - off]
                plus = patch.rfind(b'+')
                if plus >= 0:
                    res = partialname(patch[:plus])
                    if res:
                        i = self.series.index(res)
                        try:
                            off = int(patch[plus + 1 :] or 1)
                        except (ValueError, OverflowError):
                            pass
                        else:
                            if i + off < len(self.series):
                                return self.series[i + off]
        raise error.Abort(_(b"patch %s not in series") % patch)
###END###
def push(
        self,
        repo,
        patch=None,
        force=False,
        list=False,
        mergeq=None,
        all=False,
        move=False,
        exact=False,
        nobackup=False,
        keepchanges=False,
    ):
        self.checkkeepchanges(keepchanges, force)
        diffopts = self.diffopts()
        with repo.wlock():
            heads = []
            for hs in repo.branchmap().iterheads():
                heads.extend(hs)
            if not heads:
                heads = [repo.nullid]
            if repo.dirstate.p1() not in heads and not exact:
                self.ui.status(_(b"(working directory not at a head)\n"))

            if not self.series:
                self.ui.warn(_(b'no patches in series\n'))
                return 0

            # Suppose our series file is: A B C and the current 'top'
            # patch is B. qpush C should be performed (moving forward)
            # qpush B is a NOP (no change) qpush A is an error (can't
            # go backwards with qpush)
            if patch:
                patch = self.lookup(patch)
                info = self.isapplied(patch)
                if info and info[0] >= len(self.applied) - 1:
                    self.ui.warn(
                        _(b'qpush: %s is already at the top\n') % patch
                    )
                    return 0

                pushable, reason = self.pushable(patch)
                if pushable:
                    if self.series.index(patch) < self.seriesend():
                        raise error.Abort(
                            _(b"cannot push to a previous patch: %s") % patch
                        )
                else:
                    if reason:
                        reason = _(b'guarded by %s') % reason
                    else:
                        reason = _(b'no matching guards')
                    self.ui.warn(
                        _(b"cannot push '%s' - %s\n") % (patch, reason)
                    )
                    return 1
            elif all:
                patch = self.series[-1]
                if self.isapplied(patch):
                    self.ui.warn(_(b'all patches are currently applied\n'))
                    return 0

            # Following the above example, starting at 'top' of B:
            # qpush should be performed (pushes C), but a subsequent
            # qpush without an argument is an error (nothing to
            # apply). This allows a loop of "...while hg qpush..." to
            # work as it detects an error when done
            start = self.seriesend()
            if start == len(self.series):
                self.ui.warn(_(b'patch series already fully applied\n'))
                return 1
            if not force and not keepchanges:
                self.checklocalchanges(repo, refresh=self.applied)

            if exact:
                if keepchanges:
                    raise error.Abort(
                        _(b"cannot use --exact and --keep-changes together")
                    )
                if move:
                    raise error.Abort(
                        _(b'cannot use --exact and --move together')
                    )
                if self.applied:
                    raise error.Abort(
                        _(b'cannot push --exact with applied patches')
                    )
                root = self.series[start]
                target = patchheader(self.join(root), self.plainmode).parent
                if not target:
                    raise error.Abort(
                        _(b"%s does not have a parent recorded") % root
                    )
                if not repo[target] == repo[b'.']:
                    hg.update(repo, target)

            if move:
                if not patch:
                    raise error.Abort(_(b"please specify the patch to move"))
                for fullstart, rpn in enumerate(self.fullseries):
                    # strip markers for patch guards
                    if self.guard_re.split(rpn, 1)[0] == self.series[start]:
                        break
                for i, rpn in enumerate(self.fullseries[fullstart:]):
                    # strip markers for patch guards
                    if self.guard_re.split(rpn, 1)[0] == patch:
                        break
                index = fullstart + i
                assert index < len(self.fullseries)
                fullpatch = self.fullseries[index]
                del self.fullseries[index]
                self.fullseries.insert(fullstart, fullpatch)
                self.parseseries()
                self.seriesdirty = True

            self.applieddirty = True
            if start > 0:
                self.checktoppatch(repo)
            if not patch:
                patch = self.series[start]
                end = start + 1
            else:
                end = self.series.index(patch, start) + 1

            tobackup = set()
            if (not nobackup and force) or keepchanges:
                status = self.checklocalchanges(repo, force=True)
                if keepchanges:
                    tobackup.update(
                        status.modified
                        + status.added
                        + status.removed
                        + status.deleted
                    )
                else:
                    tobackup.update(status.modified + status.added)

            s = self.series[start:end]
            all_files = set()
            try:
                if mergeq:
                    ret = self.mergepatch(repo, mergeq, s, diffopts)
                else:
                    ret = self.apply(
                        repo,
                        s,
                        list,
                        all_files=all_files,
                        tobackup=tobackup,
                        keepchanges=keepchanges,
                    )
            except AbortNoCleanup:
                raise
            except:  # re-raises
                self.ui.warn(_(b'cleaning up working directory...\n'))
                cmdutil.revert(
                    self.ui,
                    repo,
                    repo[b'.'],
                    no_backup=True,
                )
                # only remove unknown files that we know we touched or
                # created while patching
                for f in all_files:
                    if f not in repo.dirstate:
                        repo.wvfs.unlinkpath(f, ignoremissing=True)
                self.ui.warn(_(b'done\n'))
                raise

            if not self.applied:
                return ret[0]
            top = self.applied[-1].name
            if ret[0] and ret[0] > 1:
                msg = _(b"errors during apply, please fix and qrefresh %s\n")
                self.ui.write(msg % top)
            else:
                self.ui.write(_(b"now at: %s\n") % top)
            return ret[0]
###END###
def pop(
        self,
        repo,
        patch=None,
        force=False,
        update=True,
        all=False,
        nobackup=False,
        keepchanges=False,
    ):
        self.checkkeepchanges(keepchanges, force)
        with repo.wlock():
            if patch:
                # index, rev, patch
                info = self.isapplied(patch)
                if not info:
                    patch = self.lookup(patch)
                info = self.isapplied(patch)
                if not info:
                    raise error.Abort(_(b"patch %s is not applied") % patch)

            if not self.applied:
                # Allow qpop -a to work repeatedly,
                # but not qpop without an argument
                self.ui.warn(_(b"no patches applied\n"))
                return not all

            if all:
                start = 0
            elif patch:
                start = info[0] + 1
            else:
                start = len(self.applied) - 1

            if start >= len(self.applied):
                self.ui.warn(_(b"qpop: %s is already at the top\n") % patch)
                return

            if not update:
                parents = repo.dirstate.parents()
                rr = [x.node for x in self.applied]
                for p in parents:
                    if p in rr:
                        self.ui.warn(_(b"qpop: forcing dirstate update\n"))
                        update = True
            else:
                parents = [p.node() for p in repo[None].parents()]
                update = any(
                    entry.node in parents for entry in self.applied[start:]
                )

            tobackup = set()
            if update:
                s = self.checklocalchanges(repo, force=force or keepchanges)
                if force:
                    if not nobackup:
                        tobackup.update(s.modified + s.added)
                elif keepchanges:
                    tobackup.update(
                        s.modified + s.added + s.removed + s.deleted
                    )

            self.applieddirty = True
            end = len(self.applied)
            rev = self.applied[start].node

            try:
                heads = repo.changelog.heads(rev)
            except error.LookupError:
                node = short(rev)
                raise error.Abort(_(b'trying to pop unknown node %s') % node)

            if heads != [self.applied[-1].node]:
                raise error.Abort(
                    _(
                        b"popping would remove a revision not "
                        b"managed by this patch queue"
                    )
                )
            if not repo[self.applied[-1].node].mutable():
                raise error.Abort(
                    _(b"popping would remove a public revision"),
                    hint=_(b"see 'hg help phases' for details"),
                )

            # we know there are no local changes, so we can make a simplified
            # form of hg.update.
            if update:
                qp = self.qparents(repo, rev)
                ctx = repo[qp]
                st = repo.status(qp, b'.')
                m, a, r, d = st.modified, st.added, st.removed, st.deleted
                if d:
                    raise error.Abort(_(b"deletions found between repo revs"))

                tobackup = set(a + m + r) & tobackup
                if keepchanges and tobackup:
                    raise error.Abort(_(b"local changes found, qrefresh first"))
                self.backup(repo, tobackup)
                with repo.dirstate.parentchange():
                    for f in a:
                        repo.wvfs.unlinkpath(f, ignoremissing=True)
                        repo.dirstate.update_file(
                            f, p1_tracked=False, wc_tracked=False
                        )
                    for f in m + r:
                        fctx = ctx[f]
                        repo.wwrite(f, fctx.data(), fctx.flags())
                        repo.dirstate.update_file(
                            f, p1_tracked=True, wc_tracked=True
                        )
                    repo.setparents(qp, repo.nullid)
            for patch in reversed(self.applied[start:end]):
                self.ui.status(_(b"popping %s\n") % patch.name)
            del self.applied[start:end]
            strip(self.ui, repo, [rev], update=False, backup=False)
            for s, state in repo[b'.'].substate.items():
                repo[b'.'].sub(s).get(state)
            if self.applied:
                self.ui.write(_(b"now at: %s\n") % self.applied[-1].name)
            else:
                self.ui.write(_(b"patch queue now empty\n"))
###END###
def diff(self, repo, pats, opts):
        top, patch = self.checktoppatch(repo)
        if not top:
            self.ui.write(_(b"no patches applied\n"))
            return
        qp = self.qparents(repo, top)
        if opts.get(b'reverse'):
            node1, node2 = None, qp
        else:
            node1, node2 = qp, None
        diffopts = self.diffopts(opts, patch)
        self.printdiff(repo, diffopts, node1, node2, files=pats, opts=opts)
###END###
def refresh(self, repo, pats=None, **opts):
        opts = pycompat.byteskwargs(opts)
        if not self.applied:
            self.ui.write(_(b"no patches applied\n"))
            return 1
        msg = opts.get(b'msg', b'').rstrip()
        edit = opts.get(b'edit')
        editform = opts.get(b'editform', b'mq.qrefresh')
        newuser = opts.get(b'user')
        newdate = opts.get(b'date')
        if newdate:
            newdate = b'%d %d' % dateutil.parsedate(newdate)
        wlock = repo.wlock()

        try:
            self.checktoppatch(repo)
            (top, patchfn) = (self.applied[-1].node, self.applied[-1].name)
            if repo.changelog.heads(top) != [top]:
                raise error.Abort(
                    _(b"cannot qrefresh a revision with children")
                )
            if not repo[top].mutable():
                raise error.Abort(
                    _(b"cannot qrefresh public revision"),
                    hint=_(b"see 'hg help phases' for details"),
                )

            cparents = repo.changelog.parents(top)
            patchparent = self.qparents(repo, top)

            inclsubs = checksubstate(repo, patchparent)
            if inclsubs:
                substatestate = repo.dirstate[b'.hgsubstate']

            ph = patchheader(self.join(patchfn), self.plainmode)
            diffopts = self.diffopts(
                {b'git': opts.get(b'git')}, patchfn, plain=True
            )
            if newuser:
                ph.setuser(newuser)
            if newdate:
                ph.setdate(newdate)
            ph.setparent(hex(patchparent))

            # only commit new patch when write is complete
            patchf = self.opener(patchfn, b'w', atomictemp=True)

            # update the dirstate in place, strip off the qtip commit
            # and then commit.
            #
            # this should really read:
            #   st = repo.status(top, patchparent)
            # but we do it backwards to take advantage of manifest/changelog
            # caching against the next repo.status call
            st = repo.status(patchparent, top)
            mm, aa, dd = st.modified, st.added, st.removed
            ctx = repo[top]
            aaa = aa[:]
            match1 = scmutil.match(repo[None], pats, opts)
            # in short mode, we only diff the files included in the
            # patch already plus specified files
            if opts.get(b'short'):
                # if amending a patch, we start with existing
                # files plus specified files - unfiltered
                match = scmutil.matchfiles(repo, mm + aa + dd + match1.files())
                # filter with include/exclude options
                match1 = scmutil.match(repo[None], opts=opts)
            else:
                match = scmutil.matchall(repo)
            stb = repo.status(match=match)
            m, a, r, d = stb.modified, stb.added, stb.removed, stb.deleted
            mm = set(mm)
            aa = set(aa)
            dd = set(dd)

            # we might end up with files that were added between
            # qtip and the dirstate parent, but then changed in the
            # local dirstate. in this case, we want them to only
            # show up in the added section
            for x in m:
                if x not in aa:
                    mm.add(x)
            # we might end up with files added by the local dirstate that
            # were deleted by the patch.  In this case, they should only
            # show up in the changed section.
            for x in a:
                if x in dd:
                    dd.remove(x)
                    mm.add(x)
                else:
                    aa.add(x)
            # make sure any files deleted in the local dirstate
            # are not in the add or change column of the patch
            forget = []
            for x in d + r:
                if x in aa:
                    aa.remove(x)
                    forget.append(x)
                    continue
                else:
                    mm.discard(x)
                dd.add(x)

            m = list(mm)
            r = list(dd)
            a = list(aa)

            # create 'match' that includes the files to be recommitted.
            # apply match1 via repo.status to ensure correct case handling.
            st = repo.status(patchparent, match=match1)
            cm, ca, cr, cd = st.modified, st.added, st.removed, st.deleted
            allmatches = set(cm + ca + cr + cd)
            refreshchanges = [x.intersection(allmatches) for x in (mm, aa, dd)]

            files = set(inclsubs)
            for x in refreshchanges:
                files.update(x)
            match = scmutil.matchfiles(repo, files)

            bmlist = repo[top].bookmarks()

            with repo.dirstate.parentchange():
                # XXX do we actually need the dirstateguard
                dsguard = None
                try:
                    dsguard = dirstateguard.dirstateguard(repo, b'mq.refresh')
                    if diffopts.git or diffopts.upgrade:
                        copies = {}
                        for dst in a:
                            src = repo.dirstate.copied(dst)
                            # during qfold, the source file for copies may
                            # be removed. Treat this as a simple add.
                            if src is not None and src in repo.dirstate:
                                copies.setdefault(src, []).append(dst)
                            repo.dirstate.update_file(
                                dst, p1_tracked=False, wc_tracked=True
                            )
                        # remember the copies between patchparent and qtip
                        for dst in aaa:
                            src = ctx[dst].copysource()
                            if src:
                                copies.setdefault(src, []).extend(
                                    copies.get(dst, [])
                                )
                                if dst in a:
                                    copies[src].append(dst)
                            # we can't copy a file created by the patch itself
                            if dst in copies:
                                del copies[dst]
                        for src, dsts in pycompat.iteritems(copies):
                            for dst in dsts:
                                repo.dirstate.copy(src, dst)
                    else:
                        for dst in a:
                            repo.dirstate.update_file(
                                dst, p1_tracked=False, wc_tracked=True
                            )
                        # Drop useless copy information
                        for f in list(repo.dirstate.copies()):
                            repo.dirstate.copy(None, f)
                    for f in r:
                        repo.dirstate.update_file_p1(f, p1_tracked=True)
                    # if the patch excludes a modified file, mark that
                    # file with mtime=0 so status can see it.
                    mm = []
                    for i in pycompat.xrange(len(m) - 1, -1, -1):
                        if not match1(m[i]):
                            mm.append(m[i])
                            del m[i]
                    for f in m:
                        repo.dirstate.update_file_p1(f, p1_tracked=True)
                    for f in mm:
                        repo.dirstate.update_file_p1(f, p1_tracked=True)
                    for f in forget:
                        repo.dirstate.update_file_p1(f, p1_tracked=False)

                    user = ph.user or ctx.user()

                    oldphase = repo[top].phase()

                    # assumes strip can roll itself back if interrupted
                    repo.setparents(*cparents)
                    self.applied.pop()
                    self.applieddirty = True
                    strip(self.ui, repo, [top], update=False, backup=False)
                    dsguard.close()
                finally:
                    release(dsguard)

            try:
                # might be nice to attempt to roll back strip after this

                defaultmsg = b"[mq]: %s" % patchfn
                editor = cmdutil.getcommiteditor(editform=editform)
                if edit:

                    def finishdesc(desc):
                        if desc.rstrip():
                            ph.setmessage(desc)
                            return desc
                        return defaultmsg

                    # i18n: this message is shown in editor with "HG: " prefix
                    extramsg = _(b'Leave message empty to use default message.')
                    editor = cmdutil.getcommiteditor(
                        finishdesc=finishdesc,
                        extramsg=extramsg,
                        editform=editform,
                    )
                    message = msg or b"\n".join(ph.message)
                elif not msg:
                    if not ph.message:
                        message = defaultmsg
                    else:
                        message = b"\n".join(ph.message)
                else:
                    message = msg
                    ph.setmessage(msg)

                # Ensure we create a new changeset in the same phase than
                # the old one.
                lock = tr = None
                try:
                    lock = repo.lock()
                    tr = repo.transaction(b'mq')
                    n = newcommit(
                        repo,
                        oldphase,
                        message,
                        user,
                        ph.date,
                        match=match,
                        force=True,
                        editor=editor,
                    )
                    # only write patch after a successful commit
                    c = [list(x) for x in refreshchanges]
                    if inclsubs:
                        self.putsubstate2changes(substatestate, c)
                    chunks = patchmod.diff(
                        repo, patchparent, changes=c, opts=diffopts
                    )
                    comments = bytes(ph)
                    if comments:
                        patchf.write(comments)
                    for chunk in chunks:
                        patchf.write(chunk)
                    patchf.close()

                    marks = repo._bookmarks
                    marks.applychanges(repo, tr, [(bm, n) for bm in bmlist])
                    tr.close()

                    self.applied.append(statusentry(n, patchfn))
                finally:
                    lockmod.release(tr, lock)
            except:  # re-raises
                ctx = repo[cparents[0]]
                repo.dirstate.rebuild(ctx.node(), ctx.manifest())
                self.savedirty()
                self.ui.warn(
                    _(
                        b'qrefresh interrupted while patch was popped! '
                        b'(revert --all, qpush to recover)\n'
                    )
                )
                raise
        finally:
            wlock.release()
            self.removeundo(repo)
###END###
def init(self, repo, create=False):
        if not create and os.path.isdir(self.path):
            raise error.Abort(_(b"patch queue directory already exists"))
        try:
            os.mkdir(self.path)
        except OSError as inst:
            if inst.errno != errno.EEXIST or not create:
                raise
        if create:
            return self.qrepo(create=True)
###END###
def unapplied(self, repo, patch=None):
        if patch and patch not in self.series:
            raise error.Abort(_(b"patch %s is not in series file") % patch)
        if not patch:
            start = self.seriesend()
        else:
            start = self.series.index(patch) + 1
        unapplied = []
        for i in pycompat.xrange(start, len(self.series)):
            pushable, reason = self.pushable(i)
            if pushable:
                unapplied.append((i, self.series[i]))
            self.explainpushable(i)
        return unapplied
###END###
def qseries(
        self,
        repo,
        missing=None,
        start=0,
        length=None,
        status=None,
        summary=False,
    ):
        def displayname(pfx, patchname, state):
            if pfx:
                self.ui.write(pfx)
            if summary:
                ph = patchheader(self.join(patchname), self.plainmode)
                if ph.message:
                    msg = ph.message[0]
                else:
                    msg = b''

                if self.ui.formatted():
                    width = self.ui.termwidth() - len(pfx) - len(patchname) - 2
                    if width > 0:
                        msg = stringutil.ellipsis(msg, width)
                    else:
                        msg = b''
                self.ui.write(patchname, label=b'qseries.' + state)
                self.ui.write(b': ')
                self.ui.write(msg, label=b'qseries.message.' + state)
            else:
                self.ui.write(patchname, label=b'qseries.' + state)
            self.ui.write(b'\n')

        applied = {p.name for p in self.applied}
        if length is None:
            length = len(self.series) - start
        if not missing:
            if self.ui.verbose:
                idxwidth = len(b"%d" % (start + length - 1))
            for i in pycompat.xrange(start, start + length):
                patch = self.series[i]
                if patch in applied:
                    char, state = b'A', b'applied'
                elif self.pushable(i)[0]:
                    char, state = b'U', b'unapplied'
                else:
                    char, state = b'G', b'guarded'
                pfx = b''
                if self.ui.verbose:
                    pfx = b'%*d %s ' % (idxwidth, i, char)
                elif status and status != char:
                    continue
                displayname(pfx, patch, state)
        else:
            msng_list = []
            for root, dirs, files in os.walk(self.path):
                d = root[len(self.path) + 1 :]
                for f in files:
                    fl = os.path.join(d, f)
                    if (
                        fl not in self.series
                        and fl
                        not in (
                            self.statuspath,
                            self.seriespath,
                            self.guardspath,
                        )
                        and not fl.startswith(b'.')
                    ):
                        msng_list.append(fl)
            for x in sorted(msng_list):
                pfx = self.ui.verbose and b'D ' or b''
                displayname(pfx, x, b'missing')
###END###
def issaveline(self, l):
        if l.name == b'.hg.patches.save.line':
            return True
###END###
def qrepo(self, create=False):
        ui = self.baseui.copy()
        # copy back attributes set by ui.pager()
        if self.ui.pageractive and not ui.pageractive:
            ui.pageractive = self.ui.pageractive
            # internal config: ui.formatted
            ui.setconfig(
                b'ui',
                b'formatted',
                self.ui.config(b'ui', b'formatted'),
                b'mqpager',
            )
            ui.setconfig(
                b'ui',
                b'interactive',
                self.ui.config(b'ui', b'interactive'),
                b'mqpager',
            )
        if create or os.path.isdir(self.join(b".hg")):
            return hg.repository(ui, path=self.path, create=create)
###END###
def restore(self, repo, rev, delete=None, qupdate=None):
        desc = repo[rev].description().strip()
        lines = desc.splitlines()
        datastart = None
        series = []
        applied = []
        qpp = None
        for i, line in enumerate(lines):
            if line == b'Patch Data:':
                datastart = i + 1
            elif line.startswith(b'Dirstate:'):
                l = line.rstrip()
                l = l[10:].split(b' ')
                qpp = [bin(x) for x in l]
            elif datastart is not None:
                l = line.rstrip()
                n, name = l.split(b':', 1)
                if n:
                    applied.append(statusentry(bin(n), name))
                else:
                    series.append(l)
        if datastart is None:
            self.ui.warn(_(b"no saved patch data found\n"))
            return 1
        self.ui.warn(_(b"restoring status: %s\n") % lines[0])
        self.fullseries = series
        self.applied = applied
        self.parseseries()
        self.seriesdirty = True
        self.applieddirty = True
        heads = repo.changelog.heads()
        if delete:
            if rev not in heads:
                self.ui.warn(_(b"save entry has children, leaving it alone\n"))
            else:
                self.ui.warn(_(b"removing save entry %s\n") % short(rev))
                pp = repo.dirstate.parents()
                if rev in pp:
                    update = True
                else:
                    update = False
                strip(self.ui, repo, [rev], update=update, backup=False)
        if qpp:
            self.ui.warn(
                _(b"saved queue repository parents: %s %s\n")
                % (short(qpp[0]), short(qpp[1]))
            )
            if qupdate:
                self.ui.status(_(b"updating queue directory\n"))
                r = self.qrepo()
                if not r:
                    self.ui.warn(_(b"unable to load queue repository\n"))
                    return 1
                hg.clean(r, qpp[0])
###END###
def save(self, repo, msg=None):
        if not self.applied:
            self.ui.warn(_(b"save: no patches applied, exiting\n"))
            return 1
        if self.issaveline(self.applied[-1]):
            self.ui.warn(_(b"status is already saved\n"))
            return 1

        if not msg:
            msg = _(b"hg patches saved state")
        else:
            msg = b"hg patches: " + msg.rstrip(b'\r\n')
        r = self.qrepo()
        if r:
            pp = r.dirstate.parents()
            msg += b"\nDirstate: %s %s" % (hex(pp[0]), hex(pp[1]))
        msg += b"\n\nPatch Data:\n"
        msg += b''.join(b'%s\n' % x for x in self.applied)
        msg += b''.join(b':%s\n' % x for x in self.fullseries)
        n = repo.commit(msg, force=True)
        if not n:
            self.ui.warn(_(b"repo commit failed\n"))
            return 1
        self.applied.append(statusentry(n, b'.hg.patches.save.line'))
        self.applieddirty = True
        self.removeundo(repo)
###END###
def fullseriesend(self):
        if self.applied:
            p = self.applied[-1].name
            end = self.findseries(p)
            if end is None:
                return len(self.fullseries)
            return end + 1
        return 0
###END###
def seriesend(self, all_patches=False):
        """If all_patches is False, return the index of the next pushable patch
        in the series, or the series length. If all_patches is True, return the
        index of the first patch past the last applied one.
        """
        end = 0

        def nextpatch(start):
            if all_patches or start >= len(self.series):
                return start
            for i in pycompat.xrange(start, len(self.series)):
                p, reason = self.pushable(i)
                if p:
                    return i
                self.explainpushable(i)
            return len(self.series)

        if self.applied:
            p = self.applied[-1].name
            try:
                end = self.series.index(p)
            except ValueError:
                return 0
            return nextpatch(end + 1)
        return nextpatch(end)
###END###
def appliedname(self, index):
        pname = self.applied[index].name
        if not self.ui.verbose:
            p = pname
        else:
            p = (b"%d" % self.series.index(pname)) + b" " + pname
        return p
###END###
def qimport(
        self,
        repo,
        files,
        patchname=None,
        rev=None,
        existing=None,
        force=None,
        git=False,
    ):
        def checkseries(patchname):
            if patchname in self.series:
                raise error.Abort(
                    _(b'patch %s is already in the series file') % patchname
                )

        if rev:
            if files:
                raise error.Abort(
                    _(b'option "-r" not valid when importing files')
                )
            rev = scmutil.revrange(repo, rev)
            rev.sort(reverse=True)
        elif not files:
            raise error.Abort(_(b'no files or revisions specified'))
        if (len(files) > 1 or len(rev) > 1) and patchname:
            raise error.Abort(
                _(b'option "-n" not valid when importing multiple patches')
            )
        imported = []
        if rev:
            # If mq patches are applied, we can only import revisions
            # that form a linear path to qbase.
            # Otherwise, they should form a linear path to a head.
            heads = repo.changelog.heads(repo.changelog.node(rev.first()))
            if len(heads) > 1:
                raise error.Abort(
                    _(b'revision %d is the root of more than one branch')
                    % rev.last()
                )
            if self.applied:
                base = repo.changelog.node(rev.first())
                if base in [n.node for n in self.applied]:
                    raise error.Abort(
                        _(b'revision %d is already managed') % rev.first()
                    )
                if heads != [self.applied[-1].node]:
                    raise error.Abort(
                        _(b'revision %d is not the parent of the queue')
                        % rev.first()
                    )
                base = repo.changelog.rev(self.applied[0].node)
                lastparent = repo.changelog.parentrevs(base)[0]
            else:
                if heads != [repo.changelog.node(rev.first())]:
                    raise error.Abort(
                        _(b'revision %d has unmanaged children') % rev.first()
                    )
                lastparent = None

            diffopts = self.diffopts({b'git': git})
            with repo.transaction(b'qimport') as tr:
                for r in rev:
                    if not repo[r].mutable():
                        raise error.Abort(
                            _(b'revision %d is not mutable') % r,
                            hint=_(b"see 'hg help phases' " b'for details'),
                        )
                    p1, p2 = repo.changelog.parentrevs(r)
                    n = repo.changelog.node(r)
                    if p2 != nullrev:
                        raise error.Abort(
                            _(b'cannot import merge revision %d') % r
                        )
                    if lastparent and lastparent != r:
                        raise error.Abort(
                            _(b'revision %d is not the parent of %d')
                            % (r, lastparent)
                        )
                    lastparent = p1

                    if not patchname:
                        patchname = self.makepatchname(
                            repo[r].description().split(b'\n', 1)[0],
                            b'%d.diff' % r,
                        )
                    checkseries(patchname)
                    self.checkpatchname(patchname, force)
                    self.fullseries.insert(0, patchname)

                    with self.opener(patchname, b"w") as fp:
                        cmdutil.exportfile(repo, [n], fp, opts=diffopts)

                    se = statusentry(n, patchname)
                    self.applied.insert(0, se)

                    self.added.append(patchname)
                    imported.append(patchname)
                    patchname = None
                    if rev and repo.ui.configbool(b'mq', b'secret'):
                        # if we added anything with --rev, move the secret root
                        phases.retractboundary(repo, tr, phases.secret, [n])
                    self.parseseries()
                    self.applieddirty = True
                    self.seriesdirty = True

        for i, filename in enumerate(files):
            if existing:
                if filename == b'-':
                    raise error.Abort(
                        _(b'-e is incompatible with import from -')
                    )
                filename = normname(filename)
                self.checkreservedname(filename)
                if urlutil.url(filename).islocal():
                    originpath = self.join(filename)
                    if not os.path.isfile(originpath):
                        raise error.Abort(
                            _(b"patch %s does not exist") % filename
                        )

                if patchname:
                    self.checkpatchname(patchname, force)

                    self.ui.write(
                        _(b'renaming %s to %s\n') % (filename, patchname)
                    )
                    util.rename(originpath, self.join(patchname))
                else:
                    patchname = filename

            else:
                if filename == b'-' and not patchname:
                    raise error.Abort(
                        _(b'need --name to import a patch from -')
                    )
                elif not patchname:
                    patchname = normname(
                        os.path.basename(filename.rstrip(b'/'))
                    )
                self.checkpatchname(patchname, force)
                try:
                    if filename == b'-':
                        text = self.ui.fin.read()
                    else:
                        fp = hg.openpath(self.ui, filename)
                        text = fp.read()
                        fp.close()
                except (OSError, IOError):
                    raise error.Abort(_(b"unable to read file %s") % filename)
                patchf = self.opener(patchname, b"w")
                patchf.write(text)
                patchf.close()
            if not force:
                checkseries(patchname)
            if patchname not in self.series:
                index = self.fullseriesend() + i
                self.fullseries[index:index] = [patchname]
            self.parseseries()
            self.seriesdirty = True
            self.ui.warn(_(b"adding %s to series file\n") % patchname)
            self.added.append(patchname)
            imported.append(patchname)
            patchname = None

        self.removeundo(repo)
        return imported
###END###
def mq(self):
            return queue(self.ui, self.baseui, self.path)
###END###
def invalidateall(self):
            super(mqrepo, self).invalidateall()
            if localrepo.hasunfilteredcache(self, 'mq'):
                # recreate mq in case queue path was changed
                delattr(self.unfiltered(), 'mq')
###END###
def abortifwdirpatched(self, errmsg, force=False):
            if self.mq.applied and self.mq.checkapplied and not force:
                parents = self.dirstate.parents()
                patches = [s.node for s in self.mq.applied]
                if any(p in patches for p in parents):
                    raise error.Abort(errmsg)
###END###
def commit(
            self,
            text=b"",
            user=None,
            date=None,
            match=None,
            force=False,
            editor=False,
            extra=None,
        ):
            if extra is None:
                extra = {}
            self.abortifwdirpatched(
                _(b'cannot commit over an applied mq patch'), force
            )

            return super(mqrepo, self).commit(
                text, user, date, match, force, editor, extra
            )
###END###
def checkpush(self, pushop):
            if self.mq.applied and self.mq.checkapplied and not pushop.force:
                outapplied = [e.node for e in self.mq.applied]
                if pushop.revs:
                    # Assume applied patches have no non-patch descendants and
                    # are not on remote already. Filtering any changeset not
                    # pushed.
                    heads = set(pushop.revs)
                    for node in reversed(outapplied):
                        if node in heads:
                            break
                        else:
                            outapplied.pop()
                # looking for pushed and shared changeset
                for node in outapplied:
                    if self[node].phase() < phases.secret:
                        raise error.Abort(_(b'source has mq patches applied'))
                # no non-secret patches pushed
            super(mqrepo, self).checkpush(pushop)
###END###
def _findtags(self):
            '''augment tags from base class with patch tags'''
            result = super(mqrepo, self)._findtags()

            q = self.mq
            if not q.applied:
                return result

            mqtags = [(patch.node, patch.name) for patch in q.applied]

            try:
                # for now ignore filtering business
                self.unfiltered().changelog.rev(mqtags[-1][0])
            except error.LookupError:
                self.ui.warn(
                    _(b'mq status file refers to unknown node %s\n')
                    % short(mqtags[-1][0])
                )
                return result

            # do not add fake tags for filtered revisions
            included = self.changelog.hasnode
            mqtags = [mqt for mqt in mqtags if included(mqt[0])]
            if not mqtags:
                return result

            mqtags.append((mqtags[-1][0], b'qtip'))
            mqtags.append((mqtags[0][0], b'qbase'))
            mqtags.append((self.changelog.parents(mqtags[0][0])[0], b'qparent'))
            tags = result[0]
            for patch in mqtags:
                if patch[1] in tags:
                    self.ui.warn(
                        _(b'tag %s overrides mq patch of the same name\n')
                        % patch[1]
                    )
                else:
                    tags[patch[1]] = patch[0]

            return result
###END###
def debug(self, msg):
            pass
###END###
def log(self, event, msgfmt, *msgargs, **opts):
            pass
###END###
def fromstorage(cls, line):
        (
            time,
            user,
            command,
            namespace,
            name,
            oldhashes,
            newhashes,
        ) = line.split(b'\n')
        timestamp, tz = time.split()
        timestamp, tz = float(timestamp), int(tz)
        oldhashes = tuple(bin(hash) for hash in oldhashes.split(b','))
        newhashes = tuple(bin(hash) for hash in newhashes.split(b','))
        return cls(
            (timestamp, tz),
            user,
            command,
            namespace,
            name,
            oldhashes,
            newhashes,
        )
###END###
def __bytes__(self):
        """bytes representation for storage"""
        time = b' '.join(map(pycompat.bytestr, self.timestamp))
        oldhashes = b','.join([hex(hash) for hash in self.oldhashes])
        newhashes = b','.join([hex(hash) for hash in self.newhashes])
        return b'\n'.join(
            (
                time,
                self.user,
                self.command,
                self.namespace,
                self.name,
                oldhashes,
                newhashes,
            )
        )
###END###
def __init__(self, repo):
        self.user = procutil.getuser()
        self.ui = repo.ui
        self.vfs = repo.vfs

        # is this working copy using a shared storage?
        self.sharedfeatures = self.sharedvfs = None
        if repo.shared():
            features = _readsharedfeatures(repo)
            sharedrepo = hg.sharedreposource(repo)
            if sharedrepo is not None and b'journal' in features:
                self.sharedvfs = sharedrepo.vfs
                self.sharedfeatures = features
###END###
def command(self):
        commandstr = b' '.join(
            map(procutil.shellquote, journalstorage._currentcommand)
        )
        if b'\n' in commandstr:
            # truncate multi-line commands
            commandstr = commandstr.partition(b'\n')[0] + b' ...'
        return commandstr
###END###
def recordcommand(cls, *fullargs):
        """Set the current hg arguments, stored with recorded entries"""
        # Set the current command on the class because we may have started
        # with a non-local repo (cloning for example).
        cls._currentcommand = fullargs
###END###
def _currentlock(self, lockref):
        """Returns the lock if it's held, or None if it's not.

        (This is copied from the localrepo class)
        """
        if lockref is None:
            return None
        l = lockref()
        if l is None or not l.held:
            return None
        return l
###END###
def jlock(self, vfs):
        """Create a lock for the journal file"""
        if self._currentlock(self._lockref) is not None:
            raise error.Abort(_(b'journal lock does not support nesting'))
        desc = _(b'journal of %s') % vfs.base
        try:
            l = lock.lock(vfs, b'namejournal.lock', 0, desc=desc)
        except error.LockHeld as inst:
            self.ui.warn(
                _(b"waiting for lock on %s held by %r\n") % (desc, inst.locker)
            )
            # default to 600 seconds timeout
            l = lock.lock(
                vfs,
                b'namejournal.lock',
                self.ui.configint(b"ui", b"timeout"),
                desc=desc,
            )
            self.ui.warn(_(b"got lock after %s seconds\n") % l.delay)
        self._lockref = weakref.ref(l)
        return l
###END###
def record(self, namespace, name, oldhashes, newhashes):
        """Record a new journal entry

        * namespace: an opaque string; this can be used to filter on the type
          of recorded entries.
        * name: the name defining this entry; for bookmarks, this is the
          bookmark name. Can be filtered on when retrieving entries.
        * oldhashes and newhashes: each a single binary hash, or a list of
          binary hashes. These represent the old and new position of the named
          item.

        """
        if not isinstance(oldhashes, list):
            oldhashes = [oldhashes]
        if not isinstance(newhashes, list):
            newhashes = [newhashes]

        entry = journalentry(
            dateutil.makedate(),
            self.user,
            self.command,
            namespace,
            name,
            oldhashes,
            newhashes,
        )

        vfs = self.vfs
        if self.sharedvfs is not None:
            # write to the shared repository if this feature is being
            # shared between working copies.
            if sharednamespaces.get(namespace) in self.sharedfeatures:
                vfs = self.sharedvfs

        self._write(vfs, entry)
###END###
def _write(self, vfs, entry):
        with self.jlock(vfs):
            # open file in amend mode to ensure it is created if missing
            with vfs(b'namejournal', mode=b'a+b') as f:
                f.seek(0, os.SEEK_SET)
                # Read just enough bytes to get a version number (up to 2
                # digits plus separator)
                version = f.read(3).partition(b'\0')[0]
                if version and version != b"%d" % storageversion:
                    # different version of the storage. Exit early (and not
                    # write anything) if this is not a version we can handle or
                    # the file is corrupt. In future, perhaps rotate the file
                    # instead?
                    self.ui.warn(
                        _(b"unsupported journal file version '%s'\n") % version
                    )
                    return
                if not version:
                    # empty file, write version first
                    f.write((b"%d" % storageversion) + b'\0')
                f.seek(0, os.SEEK_END)
                f.write(bytes(entry) + b'\0')
###END###
def filtered(self, namespace=None, name=None):
        """Yield all journal entries with the given namespace or name

        Both the namespace and the name are optional; if neither is given all
        entries in the journal are produced.

        Matching supports regular expressions by using the `re:` prefix
        (use `literal:` to match names or namespaces that start with `re:`)

        """
        if namespace is not None:
            namespace = stringutil.stringmatcher(namespace)[-1]
        if name is not None:
            name = stringutil.stringmatcher(name)[-1]
        for entry in self:
            if namespace is not None and not namespace(entry.namespace):
                continue
            if name is not None and not name(entry.name):
                continue
            yield entry
###END###
def __iter__(self):
        """Iterate over the storage

        Yields journalentry instances for each contained journal record.

        """
        local = self._open(self.vfs)

        if self.sharedvfs is None:
            return local

        # iterate over both local and shared entries, but only those
        # shared entries that are among the currently shared features
        shared = (
            e
            for e in self._open(self.sharedvfs)
            if sharednamespaces.get(e.namespace) in self.sharedfeatures
        )
        return _mergeentriesiter(local, shared)
###END###
def _open(self, vfs, filename=b'namejournal', _newestfirst=True):
        if not vfs.exists(filename):
            return

        with vfs(filename) as f:
            raw = f.read()

        lines = raw.split(b'\0')
        version = lines and lines[0]
        if version != b"%d" % storageversion:
            version = version or _(b'not available')
            raise error.Abort(_(b"unknown journal file version '%s'") % version)

        # Skip the first line, it's a version number. Normally we iterate over
        # these in reverse order to list newest first; only when copying across
        # a shared storage do we forgo reversing.
        lines = lines[1:]
        if _newestfirst:
            lines = reversed(lines)
        for line in lines:
            if not line:
                continue
            yield journalentry.fromstorage(line)
###END###
def __init__(
        self, command, pattern, linerange, priority, metadata, skipclean
    ):
        self._command = command
        self._pattern = pattern
        self._linerange = linerange
        self._priority = priority
        self._metadata = metadata
        self._skipclean = skipclean
###END###
def affects(self, opts, fixctx, path):
        """Should this fixer run on the file at the given path and context?"""
        repo = fixctx.repo()
        matcher = matchmod.match(
            repo.root, repo.root, [self._pattern], ctx=fixctx
        )
        return matcher(path)
###END###
def shouldoutputmetadata(self):
        """Should the stdout of this fixer start with JSON and a null byte?"""
        return self._metadata
###END###
def command(self, ui, path, ranges):
        """A shell command to use to invoke this fixer on the given file/lines

        May return None if there is no appropriate command to run for the given
        parameters.
        """
        expand = cmdutil.rendercommandtemplate
        parts = [
            expand(
                ui,
                self._command,
                {b'rootpath': path, b'basename': os.path.basename(path)},
            )
        ]
        if self._linerange:
            if self._skipclean and not ranges:
                # No line ranges to fix, so don't run the fixer.
                return None
            for first, last in ranges:
                parts.append(
                    expand(
                        ui, self._linerange, {b'first': first, b'last': last}
                    )
                )
        return b' '.join(parts)
###END###
def __init__(self, ui, repo, inc, exc):
        self.ui = ui
        self._repo = weakref.ref(repo)
        self.match = match.match(repo.root, b'', [], inc, exc)
        self.restrict = kwtools[b'hgcmd'] in restricted.split()
        self.postcommit = False

        kwmaps = self.ui.configitems(b'keywordmaps')
        if kwmaps:  # override default templates
            self.templates = dict(kwmaps)
        else:
            self.templates = _defaultkwmaps(self.ui)
###END###
def repo(self):
        return self._repo()
###END###
def escape(self):
        '''Returns bar-separated and escaped keywords.'''
        return b'|'.join(map(stringutil.reescape, self.templates.keys()))
###END###
def rekw(self):
        '''Returns regex for unexpanded keywords.'''
        return re.compile(br'\$(%s)\$' % self.escape)
###END###
def rekwexp(self):
        '''Returns regex for expanded keywords.'''
        return re.compile(br'\$(%s): [^$\n\r]*? \$' % self.escape)
###END###
def substitute(self, data, path, ctx, subfunc):
        '''Replaces keywords in data with expanded template.'''

        def kwsub(mobj):
            kw = mobj.group(1)
            ct = logcmdutil.maketemplater(
                self.ui, self.repo, self.templates[kw]
            )
            self.ui.pushbuffer()
            ct.show(ctx, root=self.repo.root, file=path)
            ekw = templatefilters.firstline(self.ui.popbuffer())
            return b'$%s: %s $' % (kw, ekw)

        return subfunc(kwsub, data)
###END###
def linkctx(self, path, fileid):
        '''Similar to filelog.linkrev, but returns a changectx.'''
        return self.repo.filectx(path, fileid=fileid).changectx()
###END###
def expand(self, path, node, data):
        '''Returns data with keywords expanded.'''
        if (
            not self.restrict
            and self.match(path)
            and not stringutil.binary(data)
        ):
            ctx = self.linkctx(path, node)
            return self.substitute(data, path, ctx, self.rekw.sub)
        return data
###END###
def iskwfile(self, cand, ctx):
        """Returns subset of candidates which are configured for keyword
        expansion but are not symbolic links."""
        return [f for f in cand if self.match(f) and b'l' not in ctx.flags(f)]
###END###
def overwrite(self, ctx, candidates, lookup, expand, rekw=False):
        '''Overwrites selected files expanding/shrinking keywords.'''
        if self.restrict or lookup or self.postcommit:  # exclude kw_copy
            candidates = self.iskwfile(candidates, ctx)
        if not candidates:
            return
        kwcmd = self.restrict and lookup  # kwexpand/kwshrink
        if self.restrict or expand and lookup:
            mf = ctx.manifest()
        if self.restrict or rekw:
            re_kw = self.rekw
        else:
            re_kw = self.rekwexp
        if expand:
            msg = _(b'overwriting %s expanding keywords\n')
        else:
            msg = _(b'overwriting %s shrinking keywords\n')
        for f in candidates:
            if self.restrict:
                data = self.repo.file(f).read(mf[f])
            else:
                data = self.repo.wread(f)
            if stringutil.binary(data):
                continue
            if expand:
                parents = ctx.parents()
                if lookup:
                    ctx = self.linkctx(f, mf[f])
                elif self.restrict and len(parents) > 1:
                    # merge commit
                    # in case of conflict f is in modified state during
                    # merge, even if f does not differ from f in parent
                    for p in parents:
                        if f in p and not p[f].cmp(ctx[f]):
                            ctx = p[f].changectx()
                            break
                data, found = self.substitute(data, f, ctx, re_kw.subn)
            elif self.restrict:
                found = re_kw.search(data)
            else:
                data, found = _shrinktext(data, re_kw.subn)
            if found:
                self.ui.note(msg % f)
                fp = self.repo.wvfs(f, b"wb", atomictemp=True)
                fp.write(data)
                fp.close()
                if kwcmd:
                    self.repo.dirstate.set_clean(f)
                elif self.postcommit:
                    self.repo.dirstate.update_file_p1(f, p1_tracked=True)
###END###
def shrink(self, fname, text):
        '''Returns text with all keyword substitutions removed.'''
        if self.match(fname) and not stringutil.binary(text):
            return _shrinktext(text, self.rekwexp.sub)
        return text
###END###
def shrinklines(self, fname, lines):
        '''Returns lines with keyword substitutions removed.'''
        if self.match(fname):
            text = b''.join(lines)
            if not stringutil.binary(text):
                return _shrinktext(text, self.rekwexp.sub).splitlines(True)
        return lines
###END###
def wread(self, fname, data):
        """If in restricted mode returns data read from wdir with
        keyword substitutions removed."""
        if self.restrict:
            return self.shrink(fname, data)
        return data
###END###
def __init__(self, opener, kwt, path):
        super(kwfilelog, self).__init__(opener, path)
        self.kwt = kwt
        self.path = path
###END###
def read(self, node):
        '''Expands keywords when reading filelog.'''
        data = super(kwfilelog, self).read(node)
        if self.renamed(node):
            return data
        return self.kwt.expand(self.path, node, data)
###END###
def add(self, text, meta, tr, link, p1=None, p2=None):
        '''Removes keyword substitutions when adding to filelog.'''
        text = self.kwt.shrink(self.path, text)
        return super(kwfilelog, self).add(text, meta, tr, link, p1, p2)
###END###
def cmp(self, node, text):
        '''Removes keyword substitutions for comparison.'''
        text = self.kwt.shrink(self.path, text)
        return super(kwfilelog, self).cmp(node, text)
###END###
def file(self, f):
            if f[0] == b'/':
                f = f[1:]
            return kwfilelog(self.svfs, kwt, f)
###END###
def wread(self, filename):
            data = super(kwrepo, self).wread(filename)
            return kwt.wread(filename, data)
###END###
def commit(self, *args, **opts):
            # use custom commitctx for user commands
            # other extensions can still wrap repo.commitctx directly
            self.commitctx = self.kwcommitctx
            try:
                return super(kwrepo, self).commit(*args, **opts)
            finally:
                del self.commitctx
###END###
def kwcommitctx(self, ctx, error=False, origctx=None):
            n = super(kwrepo, self).commitctx(ctx, error, origctx)
            # no lock needed, only called from repo.commit() which already locks
            if not kwt.postcommit:
                restrict = kwt.restrict
                kwt.restrict = True
                kwt.overwrite(
                    self[n], sorted(ctx.added() + ctx.modified()), False, True
                )
                kwt.restrict = restrict
            return n
###END###
def rollback(self, dryrun=False, force=False):
            with self.wlock():
                origrestrict = kwt.restrict
                try:
                    if not dryrun:
                        changed = self[b'.'].files()
                    ret = super(kwrepo, self).rollback(dryrun, force)
                    if not dryrun:
                        ctx = self[b'.']
                        modified, added = _preselect(ctx.status(), changed)
                        kwt.restrict = False
                        kwt.overwrite(ctx, modified, True, True)
                        kwt.overwrite(ctx, added, True, False)
                    return ret
                finally:
                    kwt.restrict = origrestrict
###END###
def __init__(self, path, key=None):
        self.path = path
        self.key = (key and b" --local-user \"%s\"" % key) or b""
###END###
def sign(self, data):
        gpgcmd = b"%s --sign --detach-sign%s" % (self.path, self.key)
        return procutil.filter(data, gpgcmd)
###END###
def verify(self, data, sig):
        """returns of the good and bad signatures"""
        sigfile = datafile = None
        try:
            # create temporary files
            fd, sigfile = pycompat.mkstemp(prefix=b"hg-gpg-", suffix=b".sig")
            fp = os.fdopen(fd, 'wb')
            fp.write(sig)
            fp.close()
            fd, datafile = pycompat.mkstemp(prefix=b"hg-gpg-", suffix=b".txt")
            fp = os.fdopen(fd, 'wb')
            fp.write(data)
            fp.close()
            gpgcmd = (
                b"%s --logger-fd 1 --status-fd 1 --verify \"%s\" \"%s\""
                % (
                    self.path,
                    sigfile,
                    datafile,
                )
            )
            ret = procutil.filter(b"", gpgcmd)
        finally:
            for f in (sigfile, datafile):
                try:
                    if f:
                        os.unlink(f)
                except OSError:
                    pass
        keys = []
        key, fingerprint = None, None
        for l in ret.splitlines():
            # see DETAILS in the gnupg documentation
            # filter the logger output
            if not l.startswith(b"[GNUPG:]"):
                continue
            l = l[9:]
            if l.startswith(b"VALIDSIG"):
                # fingerprint of the primary key
                fingerprint = l.split()[10]
            elif l.startswith(b"ERRSIG"):
                key = l.split(b" ", 3)[:2]
                key.append(b"")
                fingerprint = None
            elif (
                l.startswith(b"GOODSIG")
                or l.startswith(b"EXPSIG")
                or l.startswith(b"EXPKEYSIG")
                or l.startswith(b"BADSIG")
            ):
                if key is not None:
                    keys.append(key + [fingerprint])
                key = l.split(b" ", 2)
                fingerprint = None
        if key is not None:
            keys.append(key + [fingerprint])
        return keys
###END###
def __init__(self, ui):
        self.ui = ui
        self.resetstate()
###END###
def resetstate(self):
        self.topics = []
        self.topicstates = {}
        self.starttimes = {}
        self.startvals = {}
        self.printed = False
        self.lastprint = time.time() + float(self.ui.config(
            'progress', 'delay', default=3))
        self.lasttopic = None
        self.indetcount = 0
        self.refresh = float(self.ui.config(
            'progress', 'refresh', default=0.1))
        self.changedelay = max(3 * self.refresh,
                               float(self.ui.config(
                                   'progress', 'changedelay', default=1)))
        self.order = self.ui.configlist(
            'progress', 'format',
            default=['topic', 'bar', 'number', 'estimate'])
###END###
def show(self, now, topic, pos, item, unit, total):
        if not shouldprint(self.ui):
            return
        termwidth = self.width()
        self.printed = True
        head = ''
        needprogress = False
        tail = ''
        for indicator in self.order:
            add = ''
            if indicator == 'topic':
                add = topic
            elif indicator == 'number':
                if total:
                    add = ('% ' + str(len(str(total))) +
                           's/%s') % (pos, total)
                else:
                    add = str(pos)
            elif indicator.startswith('item') and item:
                slice = 'end'
                if '-' in indicator:
                    wid = int(indicator.split('-')[1])
                elif '+' in indicator:
                    slice = 'beginning'
                    wid = int(indicator.split('+')[1])
                else:
                    wid = 20
                if slice == 'end':
                    add = item[-wid:]
                else:
                    add = item[:wid]
                add += (wid - len(add)) * ' '
            elif indicator == 'bar':
                add = ''
                needprogress = True
            elif indicator == 'unit' and unit:
                add = unit
            elif indicator == 'estimate':
                add = self.estimate(topic, pos, total, now)
            elif indicator == 'speed':
                add = self.speed(topic, pos, unit, now)
            if not needprogress:
                head = spacejoin(head, add)
            else:
                tail = spacejoin(tail, add)
        if needprogress:
            used = 0
            if head:
                used += len(head) + 1
            if tail:
                used += len(tail) + 1
            progwidth = termwidth - used - 3
            if total and pos <= total:
                amt = pos * progwidth // total
                bar = '=' * (amt - 1)
                if amt > 0:
                    bar += '>'
                bar += ' ' * (progwidth - amt)
            else:
                progwidth -= 3
                self.indetcount += 1
                # mod the count by twice the width so we can make the
                # cursor bounce between the right and left sides
                amt = self.indetcount % (2 * progwidth)
                amt -= progwidth
                bar = (' ' * int(progwidth - abs(amt)) + '<=>' +
                       ' ' * int(abs(amt)))
            prog = ''.join(('[', bar , ']'))
            out = spacejoin(head, prog, tail)
        else:
            out = spacejoin(head, tail)
        sys.stderr.write('\r' + out[:termwidth])
        self.lasttopic = topic
        sys.stderr.flush()
###END###
def clear(self):
        if not shouldprint(self.ui):
            return
        sys.stderr.write('\r%s\r' % (' ' * self.width()))
###END###
def complete(self):
        if not shouldprint(self.ui):
            return
        if self.ui.configbool('progress', 'clear-complete', default=True):
            self.clear()
        else:
            sys.stderr.write('\n')
        sys.stderr.flush()
###END###
def width(self):
        tw = self.ui.termwidth()
        return min(int(self.ui.config('progress', 'width', default=tw)), tw)
###END###
def estimate(self, topic, pos, total, now):
        if total is None:
            return ''
        initialpos = self.startvals[topic]
        target = total - initialpos
        delta = pos - initialpos
        if delta > 0:
            elapsed = now - self.starttimes[topic]
            if elapsed > float(
                self.ui.config('progress', 'estimate', default=2)):
                seconds = (elapsed * (target - delta)) // delta + 1
                return fmtremaining(seconds)
        return ''
###END###
def speed(self, topic, pos, unit, now):
        initialpos = self.startvals[topic]
        delta = pos - initialpos
        elapsed = now - self.starttimes[topic]
        if elapsed > float(
            self.ui.config('progress', 'estimate', default=2)):
            return _('%d %s/sec') % (delta / elapsed, unit)
        return ''
###END###
def progress(self, topic, pos, item='', unit='', total=None):
        now = time.time()
        if pos is None:
            self.starttimes.pop(topic, None)
            self.startvals.pop(topic, None)
            self.topicstates.pop(topic, None)
            # reset the progress bar if this is the outermost topic
            if self.topics and self.topics[0] == topic and self.printed:
                self.complete()
                self.resetstate()
            # truncate the list of topics assuming all topics within
            # this one are also closed
            if topic in self.topics:
                self.topics = self.topics[:self.topics.index(topic)]
        else:
            if topic not in self.topics:
                self.starttimes[topic] = now
                self.startvals[topic] = pos
                self.topics.append(topic)
            self.topicstates[topic] = pos, item, unit, total
            if now - self.lastprint >= self.refresh and self.topics:
                if (self.lasttopic is None # first time we printed
                    # not a topic change
                    or topic == self.lasttopic
                    # it's been long enough we should print anyway
                    or now - self.lastprint >= self.changedelay):
                    self.lastprint = now
                    self.show(now, topic, *self.topicstates[topic])
###END###
def _quiet(self):
            return self.debugflag or self.quiet
###END###
def progress(self, *args, **opts):
            if not self._quiet():
                self._progbar.progress(*args, **opts)
            return super(progressui, self).progress(*args, **opts)
###END###
def write(self, *args, **opts):
            if not self._quiet() and self._progbar.printed:
                self._progbar.clear()
            return super(progressui, self).write(*args, **opts)
###END###
def write_err(self, *args, **opts):
            if not self._quiet() and self._progbar.printed:
                self._progbar.clear()
            return super(progressui, self).write_err(*args, **opts)
###END###
def __init__(self, ui, repo):
        self._repo = repo
        self._trackedevents = set(ui.configlist(b'blackbox', b'track'))
        self._ignoredevents = set(ui.configlist(b'blackbox', b'ignore'))
        self._maxfiles = ui.configint(b'blackbox', b'maxfiles')
        self._maxsize = ui.configbytes(b'blackbox', b'maxsize')
        self._inlog = False
###END###
def tracked(self, event):
        return (
            b'*' in self._trackedevents and event not in self._ignoredevents
        ) or event in self._trackedevents
###END###
def log(self, ui, event, msg, opts):
        # self._log() -> ctx.dirty() may create new subrepo instance, which
        # ui is derived from baseui. So the recursion guard in ui.log()
        # doesn't work as it's local to the ui instance.
        if self._inlog:
            return
        self._inlog = True
        try:
            self._log(ui, event, msg, opts)
        finally:
            self._inlog = False
###END###
def _log(self, ui, event, msg, opts):
        default = ui.configdate(b'devel', b'default-date')
        date = dateutil.datestr(default, ui.config(b'blackbox', b'date-format'))
        user = procutil.getuser()
        pid = b'%d' % procutil.getpid()
        changed = b''
        ctx = self._repo[None]
        parents = ctx.parents()
        rev = b'+'.join([hex(p.node()) for p in parents])
        if ui.configbool(b'blackbox', b'dirty') and ctx.dirty(
            missing=True, merge=False, branch=False
        ):
            changed = b'+'
        if ui.configbool(b'blackbox', b'logsource'):
            src = b' [%s]' % event
        else:
            src = b''
        try:
            fmt = b'%s %s @%s%s (%s)%s> %s'
            args = (date, user, rev, changed, pid, src, msg)
            with loggingutil.openlogfile(
                ui,
                self._repo.vfs,
                name=b'blackbox.log',
                maxfiles=self._maxfiles,
                maxsize=self._maxsize,
            ) as fp:
                fp.write(fmt % args)
        except (IOError, OSError) as err:
            # deactivate this to avoid failed logging again
            self._trackedevents.clear()
            ui.debug(
                b'warning: cannot write to blackbox.log: %s\n'
                % encoding.strtolocal(err.strerror)
            )
            return
        _lastlogger.logger = self
###END###
def __init__(self, url, scheme, templater):
        self.scheme = scheme
        self.templater = templater
        self.url = url
        try:
            self.parts = max(map(int, _partre.findall(self.url)))
        except ValueError:
            self.parts = 0
###END###
def __repr__(self):
        return b'<ShortRepository: %s>' % self.scheme
###END###
def instance(self, ui, url, create, intents=None, createopts=None):
        url = self.resolve(url)
        return hg._peerlookup(url).instance(
            ui, url, create, intents=intents, createopts=createopts
        )
###END###
def resolve(self, url):
        # Should this use the urlutil.url class, or is manual parsing better?
        try:
            url = url.split(b'://', 1)[1]
        except IndexError:
            raise error.Abort(_(b"no '://' in scheme url '%s'") % url)
        parts = url.split(b'/', self.parts)
        if len(parts) > self.parts:
            tail = parts[-1]
            parts = parts[:-1]
        else:
            tail = b''
        context = {b'%d' % (i + 1): v for i, v in enumerate(parts)}
        return b''.join(self.templater.process(self.url, context)) + tail
###END###
def __init__(self, ui):
        self._scripts = dict(ui.configitems(b'logtoprocess'))
###END###
def tracked(self, event):
        return bool(self._scripts.get(event))
###END###
def log(self, ui, event, msg, opts):
        script = self._scripts[event]
        maxmsg = 100000
        if len(msg) > maxmsg:
            # Each env var has a 128KiB limit on linux. msg can be long, in
            # particular for command event, where it's the full command line.
            # Prefer truncating the message than raising "Argument list too
            # long" error.
            msg = msg[:maxmsg] + b' (truncated)'
        env = {
            b'EVENT': event,
            b'HGPID': os.getpid(),
            b'MSG1': msg,
        }
        # keyword arguments get prefixed with OPT_ and uppercased
        env.update(
            (b'OPT_%s' % key.upper(), value) for key, value in opts.items()
        )
        fullenv = procutil.shellenviron(env)
        procutil.runbgcommand(script, fullenv, shell=True)
###END###
def __init__(self, repo):
        self.repo = repo
        self.actions = None
        self.keep = None
        self.topmost = None
        self.parentctxnode = None
        self.lock = None
        self.wlock = None
        self.backupfile = None
        self.stateobj = statemod.cmdstate(repo, b'histedit-state')
        self.replacements = []
###END###
def read(self):
        """Load histedit state from disk and set fields appropriately."""
        if not self.stateobj.exists():
            cmdutil.wrongtooltocontinue(self.repo, _(b'histedit'))

        data = self._read()

        self.parentctxnode = data[b'parentctxnode']
        actions = parserules(data[b'rules'], self)
        self.actions = actions
        self.keep = data[b'keep']
        self.topmost = data[b'topmost']
        self.replacements = data[b'replacements']
        self.backupfile = data[b'backupfile']
###END###
def _read(self):
        fp = self.repo.vfs.read(b'histedit-state')
        if fp.startswith(b'v1\n'):
            data = self._load()
            parentctxnode, rules, keep, topmost, replacements, backupfile = data
        else:
            data = pickle.loads(fp)
            parentctxnode, rules, keep, topmost, replacements = data
            backupfile = None
        rules = b"\n".join([b"%s %s" % (verb, rest) for [verb, rest] in rules])

        return {
            b'parentctxnode': parentctxnode,
            b"rules": rules,
            b"keep": keep,
            b"topmost": topmost,
            b"replacements": replacements,
            b"backupfile": backupfile,
        }
###END###
def write(self, tr=None):
        if tr:
            tr.addfilegenerator(
                b'histedit-state',
                (b'histedit-state',),
                self._write,
                location=b'plain',
            )
        else:
            with self.repo.vfs(b"histedit-state", b"w") as f:
                self._write(f)
###END###
def _write(self, fp):
        fp.write(b'v1\n')
        fp.write(b'%s\n' % hex(self.parentctxnode))
        fp.write(b'%s\n' % hex(self.topmost))
        fp.write(b'%s\n' % (b'True' if self.keep else b'False'))
        fp.write(b'%d\n' % len(self.actions))
        for action in self.actions:
            fp.write(b'%s\n' % action.tostate())
        fp.write(b'%d\n' % len(self.replacements))
        for replacement in self.replacements:
            fp.write(
                b'%s%s\n'
                % (
                    hex(replacement[0]),
                    b''.join(hex(r) for r in replacement[1]),
                )
            )
        backupfile = self.backupfile
        if not backupfile:
            backupfile = b''
        fp.write(b'%s\n' % backupfile)
###END###
def _load(self):
        fp = self.repo.vfs(b'histedit-state', b'r')
        lines = [l[:-1] for l in fp.readlines()]

        index = 0
        lines[index]  # version number
        index += 1

        parentctxnode = bin(lines[index])
        index += 1

        topmost = bin(lines[index])
        index += 1

        keep = lines[index] == b'True'
        index += 1

        # Rules
        rules = []
        rulelen = int(lines[index])
        index += 1
        for i in pycompat.xrange(rulelen):
            ruleaction = lines[index]
            index += 1
            rule = lines[index]
            index += 1
            rules.append((ruleaction, rule))

        # Replacements
        replacements = []
        replacementlen = int(lines[index])
        index += 1
        for i in pycompat.xrange(replacementlen):
            replacement = lines[index]
            original = bin(replacement[:40])
            succ = [
                bin(replacement[i : i + 40])
                for i in range(40, len(replacement), 40)
            ]
            replacements.append((original, succ))
            index += 1

        backupfile = lines[index]
        index += 1

        fp.close()

        return parentctxnode, rules, keep, topmost, replacements, backupfile
###END###
def clear(self):
        if self.inprogress():
            self.repo.vfs.unlink(b'histedit-state')
###END###
def inprogress(self):
        return self.repo.vfs.exists(b'histedit-state')
###END###
def __init__(self, state, node):
        self.state = state
        self.repo = state.repo
        self.node = node
###END###
def fromrule(cls, state, rule):
        """Parses the given rule, returning an instance of the histeditaction."""
        ruleid = rule.strip().split(b' ', 1)[0]
        # ruleid can be anything from rev numbers, hashes, "bookmarks" etc
        # Check for validation of rule ids and get the rulehash
        try:
            rev = bin(ruleid)
        except TypeError:
            try:
                _ctx = scmutil.revsingle(state.repo, ruleid)
                rulehash = _ctx.hex()
                rev = bin(rulehash)
            except error.RepoLookupError:
                raise error.ParseError(_(b"invalid changeset %s") % ruleid)
        return cls(state, rev)
###END###
def verify(self, prev, expected, seen):
        """Verifies semantic correctness of the rule"""
        repo = self.repo
        ha = hex(self.node)
        self.node = scmutil.resolvehexnodeidprefix(repo, ha)
        if self.node is None:
            raise error.ParseError(_(b'unknown changeset %s listed') % ha[:12])
        self._verifynodeconstraints(prev, expected, seen)
###END###
def _verifynodeconstraints(self, prev, expected, seen):
        # by default command need a node in the edited list
        if self.node not in expected:
            raise error.ParseError(
                _(b'%s "%s" changeset was not a candidate')
                % (self.verb, short(self.node)),
                hint=_(b'only use listed changesets'),
            )
        # and only one command per node
        if self.node in seen:
            raise error.ParseError(
                _(b'duplicated command for changeset %s') % short(self.node)
            )
###END###
def torule(self):
        """build a histedit rule line for an action

        by default lines are in the form:
        <hash> <rev> <summary>
        """
        ctx = self.repo[self.node]
        ui = self.repo.ui
        # We don't want color codes in the commit message template, so
        # disable the label() template function while we render it.
        with ui.configoverride(
            {(b'templatealias', b'label(l,x)'): b"x"}, b'histedit'
        ):
            summary = cmdutil.rendertemplate(
                ctx, ui.config(b'histedit', b'summary-template')
            )
        # Handle the fact that `''.splitlines() => []`
        summary = summary.splitlines()[0] if summary else b''
        line = b'%s %s %s' % (self.verb, ctx, summary)
        # trim to 75 columns by default so it's not stupidly wide in my editor
        # (the 5 more are left for verb)
        maxlen = self.repo.ui.configint(b'histedit', b'linelen')
        maxlen = max(maxlen, 22)  # avoid truncating hash
        return stringutil.ellipsis(line, maxlen)
###END###
def tostate(self):
        """Print an action in format used by histedit state files
        (the first line is a verb, the remainder is the second)
        """
        return b"%s\n%s" % (self.verb, hex(self.node))
###END###
def run(self):
        """Runs the action. The default behavior is simply apply the action's
        rulectx onto the current parentctx."""
        self.applychange()
        self.continuedirty()
        return self.continueclean()
###END###
def applychange(self):
        """Applies the changes from this action's rulectx onto the current
        parentctx, but does not commit them."""
        repo = self.repo
        rulectx = repo[self.node]
        with repo.ui.silent():
            hg.update(repo, self.state.parentctxnode, quietempty=True)
        stats = applychanges(repo.ui, repo, rulectx, {})
        repo.dirstate.setbranch(rulectx.branch())
        if stats.unresolvedcount:
            raise error.InterventionRequired(
                _(b'Fix up the change (%s %s)') % (self.verb, short(self.node)),
                hint=_(b'hg histedit --continue to resume'),
            )
###END###
def continuedirty(self):
        """Continues the action when changes have been applied to the working
        copy. The default behavior is to commit the dirty changes."""
        repo = self.repo
        rulectx = repo[self.node]

        editor = self.commiteditor()
        commit = commitfuncfor(repo, rulectx)
        if repo.ui.configbool(b'rewrite', b'update-timestamp'):
            date = dateutil.makedate()
        else:
            date = rulectx.date()
        commit(
            text=rulectx.description(),
            user=rulectx.user(),
            date=date,
            extra=rulectx.extra(),
            editor=editor,
        )
###END###
def commiteditor(self):
        """The editor to be used to edit the commit message."""
        return False
###END###
def continueclean(self):
        """Continues the action when the working copy is clean. The default
        behavior is to accept the current commit as the new version of the
        rulectx."""
        ctx = self.repo[b'.']
        if ctx.node() == self.state.parentctxnode:
            self.repo.ui.warn(
                _(b'%s: skipping changeset (no changes)\n') % short(self.node)
            )
            return ctx, [(self.node, tuple())]
        if ctx.node() == self.node:
            # Nothing changed
            return ctx, []
        return ctx, [(self.node, (ctx.node(),))]
###END###
def run(self):
        rulectx = self.repo[self.node]
        if rulectx.p1().node() == self.state.parentctxnode:
            self.repo.ui.debug(b'node %s unchanged\n' % short(self.node))
            return rulectx, []

        return super(pick, self).run()
###END###
def run(self):
        repo = self.repo
        rulectx = repo[self.node]
        hg.update(repo, self.state.parentctxnode, quietempty=True)
        applychanges(repo.ui, repo, rulectx, {})
        hint = _(b'to edit %s, `hg histedit --continue` after making changes')
        raise error.InterventionRequired(
            _(b'Editing (%s), commit as needed now to split the change')
            % short(self.node),
            hint=hint % short(self.node),
        )
###END###
def commiteditor(self):
        return cmdutil.getcommiteditor(edit=True, editform=b'histedit.edit')
###END###
def verify(self, prev, expected, seen):
        """Verifies semantic correctness of the fold rule"""
        super(fold, self).verify(prev, expected, seen)
        repo = self.repo
        if not prev:
            c = repo[self.node].p1()
        elif not prev.verb in (b'pick', b'base'):
            return
        else:
            c = repo[prev.node]
        if not c.mutable():
            raise error.ParseError(
                _(b"cannot fold into public change %s") % short(c.node())
            )
###END###
def continuedirty(self):
        repo = self.repo
        rulectx = repo[self.node]

        commit = commitfuncfor(repo, rulectx)
        commit(
            text=b'fold-temp-revision %s' % short(self.node),
            user=rulectx.user(),
            date=rulectx.date(),
            extra=rulectx.extra(),
        )
###END###
def continueclean(self):
        repo = self.repo
        ctx = repo[b'.']
        rulectx = repo[self.node]
        parentctxnode = self.state.parentctxnode
        if ctx.node() == parentctxnode:
            repo.ui.warn(_(b'%s: empty changeset\n') % short(self.node))
            return ctx, [(self.node, (parentctxnode,))]

        parentctx = repo[parentctxnode]
        newcommits = {
            c.node()
            for c in repo.set(b'(%d::. - %d)', parentctx.rev(), parentctx.rev())
        }
        if not newcommits:
            repo.ui.warn(
                _(
                    b'%s: cannot fold - working copy is not a '
                    b'descendant of previous commit %s\n'
                )
                % (short(self.node), short(parentctxnode))
            )
            return ctx, [(self.node, (ctx.node(),))]

        middlecommits = newcommits.copy()
        middlecommits.discard(ctx.node())

        return self.finishfold(
            repo.ui, repo, parentctx, rulectx, ctx.node(), middlecommits
        )
###END###
def skipprompt(self):
        """Returns true if the rule should skip the message editor.

        For example, 'fold' wants to show an editor, but 'rollup'
        doesn't want to.
        """
        return False
###END###
def mergedescs(self):
        """Returns true if the rule should merge messages of multiple changes.

        This exists mainly so that 'rollup' rules can be a subclass of
        'fold'.
        """
        return True
###END###
def firstdate(self):
        """Returns true if the rule should preserve the date of the first
        change.

        This exists mainly so that 'rollup' rules can be a subclass of
        'fold'.
        """
        return False
###END###
def finishfold(self, ui, repo, ctx, oldctx, newnode, internalchanges):
        mergemod.update(ctx.p1())
        ### prepare new commit data
        commitopts = {}
        commitopts[b'user'] = ctx.user()
        # commit message
        if not self.mergedescs():
            newmessage = ctx.description()
        else:
            newmessage = (
                b'\n***\n'.join(
                    [ctx.description()]
                    + [repo[r].description() for r in internalchanges]
                    + [oldctx.description()]
                )
                + b'\n'
            )
        commitopts[b'message'] = newmessage
        # date
        if self.firstdate():
            commitopts[b'date'] = ctx.date()
        else:
            commitopts[b'date'] = max(ctx.date(), oldctx.date())
        # if date is to be updated to current
        if ui.configbool(b'rewrite', b'update-timestamp'):
            commitopts[b'date'] = dateutil.makedate()

        extra = ctx.extra().copy()
        # histedit_source
        # note: ctx is likely a temporary commit but that the best we can do
        #       here. This is sufficient to solve issue3681 anyway.
        extra[b'histedit_source'] = b'%s,%s' % (ctx.hex(), oldctx.hex())
        commitopts[b'extra'] = extra
        phasemin = max(ctx.phase(), oldctx.phase())
        overrides = {(b'phases', b'new-commit'): phasemin}
        with repo.ui.configoverride(overrides, b'histedit'):
            n = collapse(
                repo,
                ctx,
                repo[newnode],
                commitopts,
                skipprompt=self.skipprompt(),
            )
        if n is None:
            return ctx, []
        mergemod.update(repo[n])
        replacements = [
            (oldctx.node(), (newnode,)),
            (ctx.node(), (n,)),
            (newnode, (n,)),
        ]
        for ich in internalchanges:
            replacements.append((ich, (n,)))
        return repo[n], replacements
###END###
def run(self):
        if self.repo[b'.'].node() != self.node:
            mergemod.clean_update(self.repo[self.node])
        return self.continueclean()
###END###
def continuedirty(self):
        abortdirty()
###END###
def continueclean(self):
        basectx = self.repo[b'.']
        return basectx, []
###END###
def _verifynodeconstraints(self, prev, expected, seen):
        # base can only be use with a node not in the edited set
        if self.node in expected:
            msg = _(b'%s "%s" changeset was an edited list candidate')
            raise error.ParseError(
                msg % (self.verb, short(self.node)),
                hint=_(b'base must only use unlisted changesets'),
            )
###END###
def skipprompt(self):
        return True
###END###
def mergedescs(self):
        return False
###END###
def skipprompt(self):
        return True
###END###
def firstdate(self):
        return True
###END###
def run(self):
        parentctx = self.repo[self.state.parentctxnode]
        return parentctx, [(self.node, tuple())]
###END###
def commiteditor(self):
        return cmdutil.getcommiteditor(edit=True, editform=b'histedit.mess')
###END###
def __init__(self, ui, ctx, pos, action=b'pick'):
        self.ui = ui
        self.ctx = ctx
        self.action = action
        self.origpos = pos
        self.pos = pos
        self.conflicts = []
###END###
def __bytes__(self):
        # Example display of several histeditrules:
        #
        #  #10 pick   316392:06a16c25c053   add option to skip tests
        #  #11 ^roll  316393:71313c964cc5   <RED>oops a fixup commit</RED>
        #  #12 pick   316394:ab31f3973b0d   include mfbt for mozilla-config.h
        #  #13 ^fold  316395:14ce5803f4c3   fix warnings
        #
        # The carets point to the changeset being folded into ("roll this
        # changeset into the changeset above").
        return b'%s%s' % (self.prefix, self.desc)
###END###
def prefix(self):
        # Some actions ('fold' and 'roll') combine a patch with a
        # previous one. Add a marker showing which patch they apply
        # to.
        action = ACTION_LABELS.get(self.action, self.action)

        h = self.ctx.hex()[0:12]
        r = self.ctx.rev()

        return b"#%s %s %d:%s   " % (
            (b'%d' % self.origpos).ljust(2),
            action.ljust(6),
            r,
            h,
        )
###END###
def desc(self):
        summary = cmdutil.rendertemplate(
            self.ctx, self.ui.config(b'histedit', b'summary-template')
        )
        if summary:
            return summary
        # This is split off from the prefix property so that we can
        # separately make the description for 'roll' red (since it
        # will get discarded).
        return self.ctx.description().splitlines()[0].strip()
###END###
def checkconflicts(self, other):
        if other.pos > self.pos and other.origpos <= self.origpos:
            if set(other.ctx.files()) & set(self.ctx.files()) != set():
                self.conflicts.append(other)
                return self.conflicts

        if other in self.conflicts:
            self.conflicts.remove(other)
        return self.conflicts
###END###
def __init__(self, lnode, rnode):
        self.lnode = lnode
        self.rnode = rnode
###END###
def __init__(self, path=None, transplantfile=None, opener=None):
        self.path = path
        self.transplantfile = transplantfile
        self.opener = opener

        if not opener:
            self.opener = vfsmod.vfs(self.path)
        self.transplants = {}
        self.dirty = False
        self.read()
###END###
def read(self):
        abspath = os.path.join(self.path, self.transplantfile)
        if self.transplantfile and os.path.exists(abspath):
            for line in self.opener.read(self.transplantfile).splitlines():
                lnode, rnode = map(bin, line.split(b':'))
                list = self.transplants.setdefault(rnode, [])
                list.append(transplantentry(lnode, rnode))
###END###
def write(self):
        if self.dirty and self.transplantfile:
            if not os.path.isdir(self.path):
                os.mkdir(self.path)
            fp = self.opener(self.transplantfile, b'w')
            for list in pycompat.itervalues(self.transplants):
                for t in list:
                    l, r = map(hex, (t.lnode, t.rnode))
                    fp.write(l + b':' + r + b'\n')
            fp.close()
        self.dirty = False
###END###
def get(self, rnode):
        return self.transplants.get(rnode) or []
###END###
def set(self, lnode, rnode):
        list = self.transplants.setdefault(rnode, [])
        list.append(transplantentry(lnode, rnode))
        self.dirty = True
###END###
def remove(self, transplant):
        list = self.transplants.get(transplant.rnode)
        if list:
            del list[list.index(transplant)]
            self.dirty = True
###END###
def __init__(self, ui, repo, opts):
        self.ui = ui
        self.repo = repo
        self.path = repo.vfs.join(b'transplant')
        self.opener = vfsmod.vfs(self.path)
        self.transplants = transplants(
            self.path, b'transplants', opener=self.opener
        )

        def getcommiteditor():
            editform = cmdutil.mergeeditform(repo[None], b'transplant')
            return cmdutil.getcommiteditor(
                editform=editform, **pycompat.strkwargs(opts)
            )

        self.getcommiteditor = getcommiteditor
###END###
def applied(self, repo, node, parent):
        """returns True if a node is already an ancestor of parent
        or is parent or has already been transplanted"""
        if hasnode(repo, parent):
            parentrev = repo.changelog.rev(parent)
        if hasnode(repo, node):
            rev = repo.changelog.rev(node)
            reachable = repo.changelog.ancestors(
                [parentrev], rev, inclusive=True
            )
            if rev in reachable:
                return True
        for t in self.transplants.get(node):
            # it might have been stripped
            if not hasnode(repo, t.lnode):
                self.transplants.remove(t)
                return False
            lnoderev = repo.changelog.rev(t.lnode)
            if lnoderev in repo.changelog.ancestors(
                [parentrev], lnoderev, inclusive=True
            ):
                return True
        return False
###END###
def apply(self, repo, source, revmap, merges, opts=None):
        '''apply the revisions in revmap one by one in revision order'''
        if opts is None:
            opts = {}
        revs = sorted(revmap)
        p1 = repo.dirstate.p1()
        pulls = []
        diffopts = patch.difffeatureopts(self.ui, opts)
        diffopts.git = True

        lock = tr = None
        try:
            lock = repo.lock()
            tr = repo.transaction(b'transplant')
            for rev in revs:
                node = revmap[rev]
                revstr = b'%d:%s' % (rev, short(node))

                if self.applied(repo, node, p1):
                    self.ui.warn(
                        _(b'skipping already applied revision %s\n') % revstr
                    )
                    continue

                parents = source.changelog.parents(node)
                if not (opts.get(b'filter') or opts.get(b'log')):
                    # If the changeset parent is the same as the
                    # wdir's parent, just pull it.
                    if parents[0] == p1:
                        pulls.append(node)
                        p1 = node
                        continue
                    if pulls:
                        if source != repo:
                            exchange.pull(repo, source.peer(), heads=pulls)
                        merge.update(repo[pulls[-1]])
                        p1 = repo.dirstate.p1()
                        pulls = []

                domerge = False
                if node in merges:
                    # pulling all the merge revs at once would mean we
                    # couldn't transplant after the latest even if
                    # transplants before them fail.
                    domerge = True
                    if not hasnode(repo, node):
                        exchange.pull(repo, source.peer(), heads=[node])

                skipmerge = False
                if parents[1] != repo.nullid:
                    if not opts.get(b'parent'):
                        self.ui.note(
                            _(b'skipping merge changeset %d:%s\n')
                            % (rev, short(node))
                        )
                        skipmerge = True
                    else:
                        parent = source.lookup(opts[b'parent'])
                        if parent not in parents:
                            raise error.Abort(
                                _(b'%s is not a parent of %s')
                                % (short(parent), short(node))
                            )
                else:
                    parent = parents[0]

                if skipmerge:
                    patchfile = None
                else:
                    fd, patchfile = pycompat.mkstemp(prefix=b'hg-transplant-')
                    fp = os.fdopen(fd, 'wb')
                    gen = patch.diff(source, parent, node, opts=diffopts)
                    for chunk in gen:
                        fp.write(chunk)
                    fp.close()

                del revmap[rev]
                if patchfile or domerge:
                    try:
                        try:
                            n = self.applyone(
                                repo,
                                node,
                                source.changelog.read(node),
                                patchfile,
                                merge=domerge,
                                log=opts.get(b'log'),
                                filter=opts.get(b'filter'),
                            )
                        except TransplantError:
                            # Do not rollback, it is up to the user to
                            # fix the merge or cancel everything
                            tr.close()
                            raise
                        if n and domerge:
                            self.ui.status(
                                _(b'%s merged at %s\n') % (revstr, short(n))
                            )
                        elif n:
                            self.ui.status(
                                _(b'%s transplanted to %s\n')
                                % (short(node), short(n))
                            )
                    finally:
                        if patchfile:
                            os.unlink(patchfile)
            tr.close()
            if pulls:
                exchange.pull(repo, source.peer(), heads=pulls)
                merge.update(repo[pulls[-1]])
        finally:
            self.saveseries(revmap, merges)
            self.transplants.write()
            if tr:
                tr.release()
            if lock:
                lock.release()
###END###
def filter(self, filter, node, changelog, patchfile):
        '''arbitrarily rewrite changeset before applying it'''

        self.ui.status(_(b'filtering %s\n') % patchfile)
        user, date, msg = (changelog[1], changelog[2], changelog[4])
        fd, headerfile = pycompat.mkstemp(prefix=b'hg-transplant-')
        fp = os.fdopen(fd, 'wb')
        fp.write(b"# HG changeset patch\n")
        fp.write(b"# User %s\n" % user)
        fp.write(b"# Date %d %d\n" % date)
        fp.write(msg + b'\n')
        fp.close()

        try:
            self.ui.system(
                b'%s %s %s'
                % (
                    filter,
                    procutil.shellquote(headerfile),
                    procutil.shellquote(patchfile),
                ),
                environ={
                    b'HGUSER': changelog[1],
                    b'HGREVISION': hex(node),
                },
                onerr=error.Abort,
                errprefix=_(b'filter failed'),
                blockedtag=b'transplant_filter',
            )
            user, date, msg = self.parselog(open(headerfile, b'rb'))[1:4]
        finally:
            os.unlink(headerfile)

        return (user, date, msg)
###END###
def applyone(
        self, repo, node, cl, patchfile, merge=False, log=False, filter=None
    ):
        '''apply the patch in patchfile to the repository as a transplant'''
        (manifest, user, (time, timezone), files, message) = cl[:5]
        date = b"%d %d" % (time, timezone)
        extra = {b'transplant_source': node}
        if filter:
            (user, date, message) = self.filter(filter, node, cl, patchfile)

        if log:
            # we don't translate messages inserted into commits
            message += b'\n(transplanted from %s)' % hex(node)

        self.ui.status(_(b'applying %s\n') % short(node))
        self.ui.note(b'%s %s\n%s\n' % (user, date, message))

        if not patchfile and not merge:
            raise error.Abort(_(b'can only omit patchfile if merging'))
        if patchfile:
            try:
                files = set()
                patch.patch(self.ui, repo, patchfile, files=files, eolmode=None)
                files = list(files)
            except Exception as inst:
                seriespath = os.path.join(self.path, b'series')
                if os.path.exists(seriespath):
                    os.unlink(seriespath)
                p1 = repo.dirstate.p1()
                p2 = node
                self.log(user, date, message, p1, p2, merge=merge)
                self.ui.write(stringutil.forcebytestr(inst) + b'\n')
                raise TransplantError(
                    _(
                        b'fix up the working directory and run '
                        b'hg transplant --continue'
                    )
                )
        else:
            files = None
        if merge:
            p1 = repo.dirstate.p1()
            repo.setparents(p1, node)
            m = match.always()
        else:
            m = match.exact(files)

        n = repo.commit(
            message,
            user,
            date,
            extra=extra,
            match=m,
            editor=self.getcommiteditor(),
        )
        if not n:
            self.ui.warn(_(b'skipping emptied changeset %s\n') % short(node))
            return None
        if not merge:
            self.transplants.set(n, node)

        return n
###END###
def canresume(self):
        return os.path.exists(os.path.join(self.path, b'journal'))
###END###
def resume(self, repo, source, opts):
        '''recover last transaction and apply remaining changesets'''
        if os.path.exists(os.path.join(self.path, b'journal')):
            n, node = self.recover(repo, source, opts)
            if n:
                self.ui.status(
                    _(b'%s transplanted as %s\n') % (short(node), short(n))
                )
            else:
                self.ui.status(
                    _(b'%s skipped due to empty diff\n') % (short(node),)
                )
        seriespath = os.path.join(self.path, b'series')
        if not os.path.exists(seriespath):
            self.transplants.write()
            return
        nodes, merges = self.readseries()
        revmap = {}
        for n in nodes:
            revmap[source.changelog.rev(n)] = n
        os.unlink(seriespath)

        self.apply(repo, source, revmap, merges, opts)
###END###
def recover(self, repo, source, opts):
        '''commit working directory using journal metadata'''
        node, user, date, message, parents = self.readlog()
        merge = False

        if not user or not date or not message or not parents[0]:
            raise error.Abort(_(b'transplant log file is corrupt'))

        parent = parents[0]
        if len(parents) > 1:
            if opts.get(b'parent'):
                parent = source.lookup(opts[b'parent'])
                if parent not in parents:
                    raise error.Abort(
                        _(b'%s is not a parent of %s')
                        % (short(parent), short(node))
                    )
            else:
                merge = True

        extra = {b'transplant_source': node}
        try:
            p1 = repo.dirstate.p1()
            if p1 != parent:
                raise error.Abort(
                    _(b'working directory not at transplant parent %s')
                    % hex(parent)
                )
            if merge:
                repo.setparents(p1, parents[1])
            st = repo.status()
            modified, added, removed, deleted = (
                st.modified,
                st.added,
                st.removed,
                st.deleted,
            )
            if merge or modified or added or removed or deleted:
                n = repo.commit(
                    message,
                    user,
                    date,
                    extra=extra,
                    editor=self.getcommiteditor(),
                )
                if not n:
                    raise error.Abort(_(b'commit failed'))
                if not merge:
                    self.transplants.set(n, node)
            else:
                n = None
            self.unlog()

            return n, node
        finally:
            # TODO: get rid of this meaningless try/finally enclosing.
            # this is kept only to reduce changes in a patch.
            pass
###END###
def stop(self, ui, repo):
        """logic to stop an interrupted transplant"""
        if self.canresume():
            startctx = repo[b'.']
            merge.clean_update(startctx)
            ui.status(_(b"stopped the interrupted transplant\n"))
            ui.status(
                _(b"working directory is now at %s\n") % startctx.hex()[:12]
            )
            self.unlog()
            return 0
###END###
def readseries(self):
        nodes = []
        merges = []
        cur = nodes
        for line in self.opener.read(b'series').splitlines():
            if line.startswith(b'# Merges'):
                cur = merges
                continue
            cur.append(bin(line))

        return (nodes, merges)
###END###
def saveseries(self, revmap, merges):
        if not revmap:
            return

        if not os.path.isdir(self.path):
            os.mkdir(self.path)
        series = self.opener(b'series', b'w')
        for rev in sorted(revmap):
            series.write(hex(revmap[rev]) + b'\n')
        if merges:
            series.write(b'# Merges\n')
            for m in merges:
                series.write(hex(m) + b'\n')
        series.close()
###END###
def parselog(self, fp):
        parents = []
        message = []
        node = self.repo.nullid
        inmsg = False
        user = None
        date = None
        for line in fp.read().splitlines():
            if inmsg:
                message.append(line)
            elif line.startswith(b'# User '):
                user = line[7:]
            elif line.startswith(b'# Date '):
                date = line[7:]
            elif line.startswith(b'# Node ID '):
                node = bin(line[10:])
            elif line.startswith(b'# Parent '):
                parents.append(bin(line[9:]))
            elif not line.startswith(b'# '):
                inmsg = True
                message.append(line)
        if None in (user, date):
            raise error.Abort(
                _(b"filter corrupted changeset (no user or date)")
            )
        return (node, user, date, b'\n'.join(message), parents)
###END###
def log(self, user, date, message, p1, p2, merge=False):
        '''journal changelog metadata for later recover'''

        if not os.path.isdir(self.path):
            os.mkdir(self.path)
        fp = self.opener(b'journal', b'w')
        fp.write(b'# User %s\n' % user)
        fp.write(b'# Date %s\n' % date)
        fp.write(b'# Node ID %s\n' % hex(p2))
        fp.write(b'# Parent ' + hex(p1) + b'\n')
        if merge:
            fp.write(b'# Parent ' + hex(p2) + b'\n')
        fp.write(message.rstrip() + b'\n')
        fp.close()
###END###
def readlog(self):
        return self.parselog(self.opener(b'journal'))
###END###
def unlog(self):
        '''remove changelog journal'''
        absdst = os.path.join(self.path, b'journal')
        if os.path.exists(absdst):
            os.unlink(absdst)
###END###
def transplantfilter(self, repo, source, root):
        def matchfn(node):
            if self.applied(repo, node, root):
                return False
            if source.changelog.parents(node)[1] != repo.nullid:
                return False
            extra = source.changelog.read(node)[5]
            cnode = extra.get(b'transplant_source')
            if cnode and self.applied(repo, cnode, root):
                return False
            return True

        return matchfn
###END###
def __init__(self, ui, root, data):
        self._decode = {
            b'LF': b'to-lf',
            b'CRLF': b'to-crlf',
            b'BIN': b'is-binary',
        }
        self._encode = {
            b'LF': b'to-lf',
            b'CRLF': b'to-crlf',
            b'BIN': b'is-binary',
        }

        self.cfg = config.config()
        # Our files should not be touched. The pattern must be
        # inserted first override a '** = native' pattern.
        self.cfg.set(b'patterns', b'.hg*', b'BIN', b'eol')
        # We can then parse the user's patterns.
        self.cfg.parse(b'.hgeol', data)

        isrepolf = self.cfg.get(b'repository', b'native') != b'CRLF'
        self._encode[b'NATIVE'] = isrepolf and b'to-lf' or b'to-crlf'
        iswdlf = ui.config(b'eol', b'native') in (b'LF', b'\n')
        self._decode[b'NATIVE'] = iswdlf and b'to-lf' or b'to-crlf'

        include = []
        exclude = []
        self.patterns = []
        for pattern, style in self.cfg.items(b'patterns'):
            key = style.upper()
            if key == b'BIN':
                exclude.append(pattern)
            else:
                include.append(pattern)
            m = match.match(root, b'', [pattern])
            self.patterns.append((pattern, key, m))
        # This will match the files for which we need to care
        # about inconsistent newlines.
        self.match = match.match(root, b'', [], include, exclude)
###END###
def copytoui(self, ui):
        newpatterns = {pattern for pattern, key, m in self.patterns}
        for section in (b'decode', b'encode'):
            for oldpattern, _filter in ui.configitems(section):
                if oldpattern not in newpatterns:
                    if ui.configsource(section, oldpattern) == b'eol':
                        ui.setconfig(section, oldpattern, b'!', b'eol')
        for pattern, key, m in self.patterns:
            try:
                ui.setconfig(b'decode', pattern, self._decode[key], b'eol')
                ui.setconfig(b'encode', pattern, self._encode[key], b'eol')
            except KeyError:
                ui.warn(
                    _(b"ignoring unknown EOL style '%s' from %s\n")
                    % (key, self.cfg.source(b'patterns', pattern))
                )
        # eol.only-consistent can be specified in ~/.hgrc or .hgeol
        for k, v in self.cfg.items(b'eol'):
            ui.setconfig(b'eol', k, v, b'eol')
###END###
def checkrev(self, repo, ctx, files):
        failed = []
        for f in files or ctx.files():
            if f not in ctx:
                continue
            for pattern, key, m in self.patterns:
                if not m(f):
                    continue
                target = self._encode[key]
                data = ctx[f].data()
                if (
                    target == b"to-lf"
                    and b"\r\n" in data
                    or target == b"to-crlf"
                    and singlelf.search(data)
                ):
                    failed.append((f, target, bytes(ctx)))
                break
        return failed
###END###
def loadeol(self, nodes):
            eol = parseeol(self.ui, self, nodes)
            if eol is None:
                return None
            eol.copytoui(self.ui)
            return eol.match
###END###
def _hgcleardirstate(self):
            self._eolmatch = self.loadeol([None])
            if not self._eolmatch:
                self._eolmatch = util.never
                return

            oldeol = None
            try:
                cachemtime = os.path.getmtime(self.vfs.join(b"eol.cache"))
            except OSError:
                cachemtime = 0
            else:
                olddata = self.vfs.read(b"eol.cache")
                if olddata:
                    oldeol = eolfile(self.ui, self.root, olddata)

            try:
                eolmtime = os.path.getmtime(self.wjoin(b".hgeol"))
            except OSError:
                eolmtime = 0

            if eolmtime >= cachemtime and eolmtime > 0:
                self.ui.debug(b"eol: detected change in .hgeol\n")

                hgeoldata = self.wvfs.read(b'.hgeol')
                neweol = eolfile(self.ui, self.root, hgeoldata)

                wlock = None
                try:
                    wlock = self.wlock()
                    for f in self.dirstate:
                        if self.dirstate[f] != b'n':
                            continue
                        if oldeol is not None:
                            if not oldeol.match(f) and not neweol.match(f):
                                continue
                            oldkey = None
                            for pattern, key, m in oldeol.patterns:
                                if m(f):
                                    oldkey = key
                                    break
                            newkey = None
                            for pattern, key, m in neweol.patterns:
                                if m(f):
                                    newkey = key
                                    break
                            if oldkey == newkey:
                                continue
                        # all normal files need to be looked at again since
                        # the new .hgeol file specify a different filter
                        self.dirstate.set_possibly_dirty(f)
                    # Write the cache to update mtime and cache .hgeol
                    with self.vfs(b"eol.cache", b"w") as f:
                        f.write(hgeoldata)
                except errormod.LockUnavailable:
                    # If we cannot lock the repository and clear the
                    # dirstate, then a commit might not see all files
                    # as modified. But if we cannot lock the
                    # repository, then we can also not make a commit,
                    # so ignore the error.
                    pass
                finally:
                    if wlock is not None:
                        wlock.release()
###END###
def commitctx(self, ctx, error=False, origctx=None):
            for f in sorted(ctx.added() + ctx.modified()):
                if not self._eolmatch(f):
                    continue
                fctx = ctx[f]
                if fctx is None:
                    continue
                data = fctx.data()
                if stringutil.binary(data):
                    # We should not abort here, since the user should
                    # be able to say "** = native" to automatically
                    # have all non-binary files taken care of.
                    continue
                if inconsistenteol(data):
                    raise errormod.Abort(
                        _(b"inconsistent newline style in %s\n") % f
                    )
            return super(eolrepo, self).commitctx(ctx, error, origctx)
###END###
def __init__(self, kind, repo):
        self.cache = {}
        self.potentialentries = {}
        self._kind = kind  # bookmarks or branches
        self._repo = repo
        self.loaded = False
###END###
def _load(self):
        """Read the remotenames file, store entries matching selected kind"""
        self.loaded = True
        repo = self._repo
        for node, rpath, rname in logexchange.readremotenamefile(
            repo, self._kind
        ):
            name = rpath + b'/' + rname
            self.potentialentries[name] = (node, rpath, name)
###END###
def _resolvedata(self, potentialentry):
        """Check that the node for potentialentry exists and return it"""
        if not potentialentry in self.potentialentries:
            return None
        node, remote, name = self.potentialentries[potentialentry]
        repo = self._repo
        binnode = bin(node)
        # if the node doesn't exist, skip it
        try:
            repo.changelog.rev(binnode)
        except LookupError:
            return None
        # Skip closed branches
        if self._kind == b'branches' and repo[binnode].closesbranch():
            return None
        return [binnode]
###END###
def __getitem__(self, key):
        if not self.loaded:
            self._load()
        val = self._fetchandcache(key)
        if val is not None:
            return val
        else:
            raise KeyError()
###END###
def __iter__(self):
        return iter(self.potentialentries)
###END###
def __len__(self):
        return len(self.potentialentries)
###END###
def __setitem__(self):
        raise NotImplementedError
###END###
def __delitem__(self):
        raise NotImplementedError
###END###
def _fetchandcache(self, key):
        if key in self.cache:
            return self.cache[key]
        val = self._resolvedata(key)
        if val is not None:
            self.cache[key] = val
            return val
        else:
            return None
###END###
def keys(self):
        """Get a list of bookmark or branch names"""
        if not self.loaded:
            self._load()
        return self.potentialentries.keys()
###END###
def iteritems(self):
        """Iterate over (name, node) tuples"""

        if not self.loaded:
            self._load()

        for k, vtup in pycompat.iteritems(self.potentialentries):
            yield (k, [bin(vtup[0])])
###END###
def __init__(self, repo, *args):
        self._repo = repo
        self.clearnames()
###END###
def clearnames(self):
        """Clear all remote names state"""
        self.bookmarks = lazyremotenamedict(b"bookmarks", self._repo)
        self.branches = lazyremotenamedict(b"branches", self._repo)
        self._invalidatecache()
###END###
def _invalidatecache(self):
        self._nodetobmarks = None
        self._nodetobranch = None
        self._hoisttonodes = None
        self._nodetohoists = None
###END###
def bmarktonodes(self):
        return self.bookmarks
###END###
def nodetobmarks(self):
        if not self._nodetobmarks:
            bmarktonodes = self.bmarktonodes()
            self._nodetobmarks = {}
            for name, node in pycompat.iteritems(bmarktonodes):
                self._nodetobmarks.setdefault(node[0], []).append(name)
        return self._nodetobmarks
###END###
def branchtonodes(self):
        return self.branches
###END###
def nodetobranch(self):
        if not self._nodetobranch:
            branchtonodes = self.branchtonodes()
            self._nodetobranch = {}
            for name, nodes in pycompat.iteritems(branchtonodes):
                for node in nodes:
                    self._nodetobranch.setdefault(node, []).append(name)
        return self._nodetobranch
###END###
def hoisttonodes(self, hoist):
        if not self._hoisttonodes:
            marktonodes = self.bmarktonodes()
            self._hoisttonodes = {}
            hoist += b'/'
            for name, node in pycompat.iteritems(marktonodes):
                if name.startswith(hoist):
                    name = name[len(hoist) :]
                    self._hoisttonodes[name] = node
        return self._hoisttonodes
###END###
def nodetohoists(self, hoist):
        if not self._nodetohoists:
            marktonodes = self.bmarktonodes()
            self._nodetohoists = {}
            hoist += b'/'
            for name, node in pycompat.iteritems(marktonodes):
                if name.startswith(hoist):
                    name = name[len(hoist) :]
                    self._nodetohoists.setdefault(node[0], []).append(name)
        return self._nodetohoists
###END###
def __init__(self, ui, repo, hooktype):
        self.ui = ui
        cfg = self.ui.config(b'notify', b'config')
        if cfg:
            self.ui.readconfig(cfg, sections=[b'usersubs', b'reposubs'])
        self.repo = repo
        self.stripcount = int(self.ui.config(b'notify', b'strip'))
        self.root = self.strip(self.repo.root)
        self.domain = self.ui.config(b'notify', b'domain')
        self.mbox = self.ui.config(b'notify', b'mbox')
        self.test = self.ui.configbool(b'notify', b'test')
        self.charsets = mail._charsets(self.ui)
        self.subs = self.subscribers()
        self.merge = self.ui.configbool(b'notify', b'merge')
        self.showfunc = self.ui.configbool(b'notify', b'showfunc')
        self.messageidseed = self.ui.config(b'notify', b'messageidseed')
        self.reply = self.ui.configbool(b'notify', b'reply-to-predecessor')

        if self.reply and not self.messageidseed:
            raise error.Abort(
                _(
                    b'notify.reply-to-predecessor used without '
                    b'notify.messageidseed'
                )
            )

        if self.showfunc is None:
            self.showfunc = self.ui.configbool(b'diff', b'showfunc')

        mapfile = None
        template = self.ui.config(b'notify', hooktype) or self.ui.config(
            b'notify', b'template'
        )
        if not template:
            mapfile = self.ui.config(b'notify', b'style')
        if not mapfile and not template:
            template = deftemplates.get(hooktype) or single_template
        spec = logcmdutil.templatespec(template, mapfile)
        self.t = logcmdutil.changesettemplater(self.ui, self.repo, spec)
###END###
def strip(self, path):
        '''strip leading slashes from local path, turn into web-safe path.'''

        path = util.pconvert(path)
        count = self.stripcount
        while count > 0:
            c = path.find(b'/')
            if c == -1:
                break
            path = path[c + 1 :]
            count -= 1
        return path
###END###
def fixmail(self, addr):
        '''try to clean up email addresses.'''

        addr = stringutil.email(addr.strip())
        if self.domain:
            a = addr.find(b'@localhost')
            if a != -1:
                addr = addr[:a]
            if b'@' not in addr:
                return addr + b'@' + self.domain
        return addr
###END###
def subscribers(self):
        '''return list of email addresses of subscribers to this repo.'''
        subs = set()
        for user, pats in self.ui.configitems(b'usersubs'):
            for pat in pats.split(b','):
                if b'#' in pat:
                    pat, revs = pat.split(b'#', 1)
                else:
                    revs = None
                if fnmatch.fnmatch(self.repo.root, pat.strip()):
                    subs.add((self.fixmail(user), revs))
        for pat, users in self.ui.configitems(b'reposubs'):
            if b'#' in pat:
                pat, revs = pat.split(b'#', 1)
            else:
                revs = None
            if fnmatch.fnmatch(self.repo.root, pat):
                for user in users.split(b','):
                    subs.add((self.fixmail(user), revs))
        return [
            (mail.addressencode(self.ui, s, self.charsets, self.test), r)
            for s, r in sorted(subs)
        ]
###END###
def node(self, ctx, **props):
        '''format one changeset, unless it is a suppressed merge.'''
        if not self.merge and len(ctx.parents()) > 1:
            return False
        self.t.show(
            ctx,
            changes=ctx.changeset(),
            baseurl=self.ui.config(b'web', b'baseurl'),
            root=self.repo.root,
            webroot=self.root,
            **props
        )
        return True
###END###
def skipsource(self, source):
        '''true if incoming changes from this source should be skipped.'''
        ok_sources = self.ui.config(b'notify', b'sources').split()
        return source not in ok_sources
###END###
def send(self, ctx, count, data):
        '''send message.'''

        # Select subscribers by revset
        subs = set()
        for sub, spec in self.subs:
            if spec is None:
                subs.add(sub)
                continue
            revs = self.repo.revs(b'%r and %d:', spec, ctx.rev())
            if len(revs):
                subs.add(sub)
                continue
        if len(subs) == 0:
            self.ui.debug(
                b'notify: no subscribers to selected repo and revset\n'
            )
            return

        try:
            msg = mail.parsebytes(data)
        except emailerrors.MessageParseError as inst:
            raise error.Abort(inst)

        # store sender and subject
        sender = msg['From']
        subject = msg['Subject']
        if sender is not None:
            sender = mail.headdecode(sender)
        if subject is not None:
            subject = mail.headdecode(subject)
        del msg['From'], msg['Subject']

        if not msg.is_multipart():
            # create fresh mime message from scratch
            # (multipart templates must take care of this themselves)
            headers = msg.items()
            payload = msg.get_payload(decode=pycompat.ispy3)
            # for notification prefer readability over data precision
            msg = mail.mimeencode(self.ui, payload, self.charsets, self.test)
            # reinstate custom headers
            for k, v in headers:
                msg[k] = v

        msg['Date'] = encoding.strfromlocal(
            dateutil.datestr(format=b"%a, %d %b %Y %H:%M:%S %1%2")
        )

        # try to make subject line exist and be useful
        if not subject:
            if count > 1:
                subject = _(b'%s: %d new changesets') % (self.root, count)
            else:
                s = ctx.description().lstrip().split(b'\n', 1)[0].rstrip()
                subject = b'%s: %s' % (self.root, s)
        maxsubject = int(self.ui.config(b'notify', b'maxsubject'))
        if maxsubject:
            subject = stringutil.ellipsis(subject, maxsubject)
        msg['Subject'] = mail.headencode(
            self.ui, subject, self.charsets, self.test
        )

        # try to make message have proper sender
        if not sender:
            sender = self.ui.config(b'email', b'from') or self.ui.username()
        if b'@' not in sender or b'@localhost' in sender:
            sender = self.fixmail(sender)
        msg['From'] = mail.addressencode(
            self.ui, sender, self.charsets, self.test
        )

        msg['X-Hg-Notification'] = 'changeset %s' % ctx
        if not msg['Message-Id']:
            msg['Message-Id'] = messageid(ctx, self.domain, self.messageidseed)
        if self.reply:
            unfi = self.repo.unfiltered()
            has_node = unfi.changelog.index.has_node
            predecessors = [
                unfi[ctx2]
                for ctx2 in obsutil.allpredecessors(unfi.obsstore, [ctx.node()])
                if ctx2 != ctx.node() and has_node(ctx2)
            ]
            if predecessors:
                # There is at least one predecessor, so which to pick?
                # Ideally, there is a unique root because changesets have
                # been evolved/rebased one step at a time. In this case,
                # just picking the oldest known changeset provides a stable
                # base. It doesn't help when changesets are folded. Any
                # better solution would require storing more information
                # in the repository.
                pred = min(predecessors, key=lambda ctx: ctx.rev())
                msg['In-Reply-To'] = messageid(
                    pred, self.domain, self.messageidseed
                )
        msg['To'] = ', '.join(sorted(subs))

        msgtext = msg.as_bytes() if pycompat.ispy3 else msg.as_string()
        if self.test:
            self.ui.write(msgtext)
            if not msgtext.endswith(b'\n'):
                self.ui.write(b'\n')
        else:
            self.ui.status(
                _(b'notify: sending %d subscribers %d changes\n')
                % (len(subs), count)
            )
            mail.sendmail(
                self.ui,
                emailutils.parseaddr(msg['From'])[1],
                subs,
                msgtext,
                mbox=self.mbox,
            )
###END###
def diff(self, ctx, ref=None):

        maxdiff = int(self.ui.config(b'notify', b'maxdiff'))
        prev = ctx.p1().node()
        if ref:
            ref = ref.node()
        else:
            ref = ctx.node()
        diffopts = patch.diffallopts(self.ui)
        diffopts.showfunc = self.showfunc
        chunks = patch.diff(self.repo, prev, ref, opts=diffopts)
        difflines = b''.join(chunks).splitlines()

        if self.ui.configbool(b'notify', b'diffstat'):
            maxdiffstat = int(self.ui.config(b'notify', b'maxdiffstat'))
            s = patch.diffstat(difflines)
            # s may be nil, don't include the header if it is
            if s:
                if maxdiffstat >= 0 and s.count(b"\n") > maxdiffstat + 1:
                    s = s.split(b"\n")
                    msg = _(b'\ndiffstat (truncated from %d to %d lines):\n\n')
                    self.ui.write(msg % (len(s) - 2, maxdiffstat))
                    self.ui.write(b"\n".join(s[:maxdiffstat] + s[-2:]))
                else:
                    self.ui.write(_(b'\ndiffstat:\n\n%s') % s)

        if maxdiff == 0:
            return
        elif maxdiff > 0 and len(difflines) > maxdiff:
            msg = _(b'\ndiffs (truncated from %d to %d lines):\n\n')
            self.ui.write(msg % (len(difflines), maxdiff))
            difflines = difflines[:maxdiff]
        elif difflines:
            self.ui.write(_(b'\ndiffs (%d lines):\n\n') % len(difflines))

        self.ui.write(b"\n".join(difflines))
###END###
def __init__(self, ui):
        self.ui = ui
        usermap = self.ui.config(b'bugzilla', b'usermap')
        if usermap:
            self.ui.readconfig(usermap, sections=[b'usermap'])
###END###
def map_committer(self, user):
        '''map name of committer to Bugzilla user name.'''
        for committer, bzuser in self.ui.configitems(b'usermap'):
            if committer.lower() == user.lower():
                return bzuser
        return user
###END###
def filter_real_bug_ids(self, bugs):
        '''remove bug IDs that do not exist in Bugzilla from bugs.'''
###END###
def filter_cset_known_bug_ids(self, node, bugs):
        '''remove bug IDs where node occurs in comment text from bugs.'''
###END###
def updatebug(self, bugid, newstate, text, committer):
        """update the specified bug. Add comment text and set new states.

        If possible add the comment as being from the committer of
        the changeset. Otherwise use the default Bugzilla user.
        """
###END###
def notify(self, bugs, committer):
        """Force sending of Bugzilla notification emails.

        Only required if the access method does not trigger notification
        emails automatically.
        """
###END###
def sql_buglist(ids):
        '''return SQL-friendly list of bug ids'''
        return b'(' + b','.join(map(str, ids)) + b')'
###END###
def __init__(self, ui):
        try:
            import MySQLdb as mysql

            bzmysql._MySQLdb = mysql
        except ImportError as err:
            raise error.Abort(
                _(b'python mysql support not available: %s') % err
            )

        bzaccess.__init__(self, ui)

        host = self.ui.config(b'bugzilla', b'host')
        user = self.ui.config(b'bugzilla', b'user')
        passwd = self.ui.config(b'bugzilla', b'password')
        db = self.ui.config(b'bugzilla', b'db')
        timeout = int(self.ui.config(b'bugzilla', b'timeout'))
        self.ui.note(
            _(b'connecting to %s:%s as %s, password %s\n')
            % (host, db, user, b'*' * len(passwd))
        )
        self.conn = bzmysql._MySQLdb.connect(
            host=host, user=user, passwd=passwd, db=db, connect_timeout=timeout
        )
        self.cursor = self.conn.cursor()
        self.longdesc_id = self.get_longdesc_id()
        self.user_ids = {}
        self.default_notify = b"cd %(bzdir)s && ./processmail %(id)s %(user)s"
###END###
def run(self, *args, **kwargs):
        '''run a query.'''
        self.ui.note(_(b'query: %s %s\n') % (args, kwargs))
        try:
            self.cursor.execute(*args, **kwargs)
        except bzmysql._MySQLdb.MySQLError:
            self.ui.note(_(b'failed query: %s %s\n') % (args, kwargs))
            raise
###END###
def get_longdesc_id(self):
        '''get identity of longdesc field'''
        self.run(b'select fieldid from fielddefs where name = "longdesc"')
        ids = self.cursor.fetchall()
        if len(ids) != 1:
            raise error.Abort(_(b'unknown database schema'))
        return ids[0][0]
###END###
def filter_real_bug_ids(self, bugs):
        '''filter not-existing bugs from set.'''
        self.run(
            b'select bug_id from bugs where bug_id in %s'
            % bzmysql.sql_buglist(bugs.keys())
        )
        existing = [id for (id,) in self.cursor.fetchall()]
        for id in bugs.keys():
            if id not in existing:
                self.ui.status(_(b'bug %d does not exist\n') % id)
                del bugs[id]
###END###
def filter_cset_known_bug_ids(self, node, bugs):
        '''filter bug ids that already refer to this changeset from set.'''
        self.run(
            '''select bug_id from longdescs where
                    bug_id in %s and thetext like "%%%s%%"'''
            % (bzmysql.sql_buglist(bugs.keys()), short(node))
        )
        for (id,) in self.cursor.fetchall():
            self.ui.status(
                _(b'bug %d already knows about changeset %s\n')
                % (id, short(node))
            )
            del bugs[id]
###END###
def notify(self, bugs, committer):
        '''tell bugzilla to send mail.'''
        self.ui.status(_(b'telling bugzilla to send mail:\n'))
        (user, userid) = self.get_bugzilla_user(committer)
        for id in bugs.keys():
            self.ui.status(_(b'  bug %s\n') % id)
            cmdfmt = self.ui.config(b'bugzilla', b'notify', self.default_notify)
            bzdir = self.ui.config(b'bugzilla', b'bzdir')
            try:
                # Backwards-compatible with old notify string, which
                # took one string. This will throw with a new format
                # string.
                cmd = cmdfmt % id
            except TypeError:
                cmd = cmdfmt % {b'bzdir': bzdir, b'id': id, b'user': user}
            self.ui.note(_(b'running notify command %s\n') % cmd)
            fp = procutil.popen(b'(%s) 2>&1' % cmd, b'rb')
            out = util.fromnativeeol(fp.read())
            ret = fp.close()
            if ret:
                self.ui.warn(out)
                raise error.Abort(
                    _(b'bugzilla notify command %s') % procutil.explainexit(ret)
                )
        self.ui.status(_(b'done\n'))
###END###
def get_user_id(self, user):
        '''look up numeric bugzilla user id.'''
        try:
            return self.user_ids[user]
        except KeyError:
            try:
                userid = int(user)
            except ValueError:
                self.ui.note(_(b'looking up user %s\n') % user)
                self.run(
                    '''select userid from profiles
                            where login_name like %s''',
                    user,
                )
                all = self.cursor.fetchall()
                if len(all) != 1:
                    raise KeyError(user)
                userid = int(all[0][0])
            self.user_ids[user] = userid
            return userid
###END###
def get_bugzilla_user(self, committer):
        """See if committer is a registered bugzilla user. Return
        bugzilla username and userid if so. If not, return default
        bugzilla username and userid."""
        user = self.map_committer(committer)
        try:
            userid = self.get_user_id(user)
        except KeyError:
            try:
                defaultuser = self.ui.config(b'bugzilla', b'bzuser')
                if not defaultuser:
                    raise error.Abort(
                        _(b'cannot find bugzilla user id for %s') % user
                    )
                userid = self.get_user_id(defaultuser)
                user = defaultuser
            except KeyError:
                raise error.Abort(
                    _(b'cannot find bugzilla user id for %s or %s')
                    % (user, defaultuser)
                )
        return (user, userid)
###END###
def updatebug(self, bugid, newstate, text, committer):
        """update bug state with comment text.

        Try adding comment as committer of changeset, otherwise as
        default bugzilla user."""
        if len(newstate) > 0:
            self.ui.warn(_(b"Bugzilla/MySQL cannot update bug state\n"))

        (user, userid) = self.get_bugzilla_user(committer)
        now = time.strftime('%Y-%m-%d %H:%M:%S')
        self.run(
            '''insert into longdescs
                    (bug_id, who, bug_when, thetext)
                    values (%s, %s, %s, %s)''',
            (bugid, userid, now, text),
        )
        self.run(
            '''insert into bugs_activity (bug_id, who, bug_when, fieldid)
                    values (%s, %s, %s, %s)''',
            (bugid, userid, now, self.longdesc_id),
        )
        self.conn.commit()
###END###
def __init__(self, ui):
        bzmysql.__init__(self, ui)
        self.default_notify = (
            b"cd %(bzdir)s && perl -T contrib/sendbugmail.pl %(id)s %(user)s"
        )
###END###
def __init__(self, ui):
        bzmysql_2_18.__init__(self, ui)
###END###
def get_longdesc_id(self):
        '''get identity of longdesc field'''
        self.run(b'select id from fielddefs where name = "longdesc"')
        ids = self.cursor.fetchall()
        if len(ids) != 1:
            raise error.Abort(_(b'unknown database schema'))
        return ids[0][0]
###END###
def send_cookies(self, connection):
        if self.cookies:
            for cookie in self.cookies:
                connection.putheader(b"Cookie", cookie)
###END###
def request(self, host, handler, request_body, verbose=0):
        self.verbose = verbose
        self.accept_gzip_encoding = False

        # issue XML-RPC request
        h = self.make_connection(host)
        if verbose:
            h.set_debuglevel(1)

        self.send_request(h, handler, request_body)
        self.send_host(h, host)
        self.send_cookies(h)
        self.send_user_agent(h)
        self.send_content(h, request_body)

        # Deal with differences between Python 2.6 and 2.7.
        # In the former h is a HTTP(S). In the latter it's a
        # HTTP(S)Connection. Luckily, the 2.6 implementation of
        # HTTP(S) has an underlying HTTP(S)Connection, so extract
        # that and use it.
        try:
            response = h.getresponse()
        except AttributeError:
            response = h._conn.getresponse()

        # Add any cookie definitions to our list.
        for header in response.msg.getallmatchingheaders(b"Set-Cookie"):
            val = header.split(b": ", 1)[1]
            cookie = val.split(b";", 1)[0]
            self.cookies.append(cookie)

        if response.status != 200:
            raise xmlrpclib.ProtocolError(
                host + handler,
                response.status,
                response.reason,
                response.msg.headers,
            )

        payload = response.read()
        parser, unmarshaller = self.getparser()
        parser.feed(payload)
        parser.close()

        return unmarshaller.close()
###END###
def __init__(self, use_datetime=0):
        if util.safehasattr(xmlrpclib.Transport, "__init__"):
            xmlrpclib.Transport.__init__(self, use_datetime)
###END###
def __init__(self, use_datetime=0):
        if util.safehasattr(xmlrpclib.Transport, "__init__"):
            xmlrpclib.SafeTransport.__init__(self, use_datetime)
###END###
def __init__(self, ui):
        bzaccess.__init__(self, ui)

        bzweb = self.ui.config(b'bugzilla', b'bzurl')
        bzweb = bzweb.rstrip(b"/") + b"/xmlrpc.cgi"

        user = self.ui.config(b'bugzilla', b'user')
        passwd = self.ui.config(b'bugzilla', b'password')

        self.fixstatus = self.ui.config(b'bugzilla', b'fixstatus')
        self.fixresolution = self.ui.config(b'bugzilla', b'fixresolution')

        self.bzproxy = xmlrpclib.ServerProxy(
            pycompat.strurl(bzweb), self.transport(bzweb)
        )
        ver = self.bzproxy.Bugzilla.version()[b'version'].split(b'.')
        self.bzvermajor = int(ver[0])
        self.bzverminor = int(ver[1])
        login = self.bzproxy.User.login(
            {b'login': user, b'password': passwd, b'restrict_login': True}
        )
        self.bztoken = login.get(b'token', b'')
###END###
def transport(self, uri):
        if util.urlreq.urlparse(uri, b"http")[0] == b"https":
            return cookiesafetransport()
        else:
            return cookietransport()
###END###
def get_bug_comments(self, id):
        """Return a string with all comment text for a bug."""
        c = self.bzproxy.Bug.comments(
            {b'ids': [id], b'include_fields': [b'text'], b'token': self.bztoken}
        )
        return b''.join(
            [t[b'text'] for t in c[b'bugs'][b'%d' % id][b'comments']]
        )
###END###
def filter_real_bug_ids(self, bugs):
        probe = self.bzproxy.Bug.get(
            {
                b'ids': sorted(bugs.keys()),
                b'include_fields': [],
                b'permissive': True,
                b'token': self.bztoken,
            }
        )
        for badbug in probe[b'faults']:
            id = badbug[b'id']
            self.ui.status(_(b'bug %d does not exist\n') % id)
            del bugs[id]
###END###
def filter_cset_known_bug_ids(self, node, bugs):
        for id in sorted(bugs.keys()):
            if self.get_bug_comments(id).find(short(node)) != -1:
                self.ui.status(
                    _(b'bug %d already knows about changeset %s\n')
                    % (id, short(node))
                )
                del bugs[id]
###END###
def updatebug(self, bugid, newstate, text, committer):
        args = {}
        if b'hours' in newstate:
            args[b'work_time'] = newstate[b'hours']

        if self.bzvermajor >= 4:
            args[b'ids'] = [bugid]
            args[b'comment'] = {b'body': text}
            if b'fix' in newstate:
                args[b'status'] = self.fixstatus
                args[b'resolution'] = self.fixresolution
            args[b'token'] = self.bztoken
            self.bzproxy.Bug.update(args)
        else:
            if b'fix' in newstate:
                self.ui.warn(
                    _(
                        b"Bugzilla/XMLRPC needs Bugzilla 4.0 or later "
                        b"to mark bugs fixed\n"
                    )
                )
            args[b'id'] = bugid
            args[b'comment'] = text
            self.bzproxy.Bug.add_comment(args)
###END###
def __init__(self, ui):
        bzxmlrpc.__init__(self, ui)

        self.bzemail = self.ui.config(b'bugzilla', b'bzemail')
        if not self.bzemail:
            raise error.Abort(_(b"configuration 'bzemail' missing"))
        mail.validateconfig(self.ui)
###END###
def makecommandline(self, fieldname, value):
        if self.bzvermajor >= 4:
            return b"@%s %s" % (fieldname, pycompat.bytestr(value))
        else:
            if fieldname == b"id":
                fieldname = b"bug_id"
            return b"@%s = %s" % (fieldname, pycompat.bytestr(value))
###END###
def send_bug_modify_email(self, bugid, commands, comment, committer):
        """send modification message to Bugzilla bug via email.

        The message format is documented in the Bugzilla email_in.pl
        specification. commands is a list of command lines, comment is the
        comment text.

        To stop users from crafting commit comments with
        Bugzilla commands, specify the bug ID via the message body, rather
        than the subject line, and leave a blank line after it.
        """
        user = self.map_committer(committer)
        matches = self.bzproxy.User.get(
            {b'match': [user], b'token': self.bztoken}
        )
        if not matches[b'users']:
            user = self.ui.config(b'bugzilla', b'user')
            matches = self.bzproxy.User.get(
                {b'match': [user], b'token': self.bztoken}
            )
            if not matches[b'users']:
                raise error.Abort(
                    _(b"default bugzilla user %s email not found") % user
                )
        user = matches[b'users'][0][b'email']
        commands.append(self.makecommandline(b"id", bugid))

        text = b"\n".join(commands) + b"\n\n" + comment

        _charsets = mail._charsets(self.ui)
        user = mail.addressencode(self.ui, user, _charsets)
        bzemail = mail.addressencode(self.ui, self.bzemail, _charsets)
        msg = mail.mimeencode(self.ui, text, _charsets)
        msg[b'From'] = user
        msg[b'To'] = bzemail
        msg[b'Subject'] = mail.headencode(
            self.ui, b"Bug modification", _charsets
        )
        sendmail = mail.connect(self.ui)
        sendmail(user, bzemail, msg.as_string())
###END###
def updatebug(self, bugid, newstate, text, committer):
        cmds = []
        if b'hours' in newstate:
            cmds.append(self.makecommandline(b"work_time", newstate[b'hours']))
        if b'fix' in newstate:
            cmds.append(self.makecommandline(b"bug_status", self.fixstatus))
            cmds.append(self.makecommandline(b"resolution", self.fixresolution))
        self.send_bug_modify_email(bugid, cmds, text, committer)
###END###
def __init__(self, ui):
        bzaccess.__init__(self, ui)
        bz = self.ui.config(b'bugzilla', b'bzurl')
        self.bzroot = b'/'.join([bz, b'rest'])
        self.apikey = self.ui.config(b'bugzilla', b'apikey')
        self.user = self.ui.config(b'bugzilla', b'user')
        self.passwd = self.ui.config(b'bugzilla', b'password')
        self.fixstatus = self.ui.config(b'bugzilla', b'fixstatus')
        self.fixresolution = self.ui.config(b'bugzilla', b'fixresolution')
###END###
def apiurl(self, targets, include_fields=None):
        url = b'/'.join([self.bzroot] + [pycompat.bytestr(t) for t in targets])
        qv = {}
        if self.apikey:
            qv[b'api_key'] = self.apikey
        elif self.user and self.passwd:
            qv[b'login'] = self.user
            qv[b'password'] = self.passwd
        if include_fields:
            qv[b'include_fields'] = include_fields
        if qv:
            url = b'%s?%s' % (url, util.urlreq.urlencode(qv))
        return url
###END###
def _fetch(self, burl):
        try:
            resp = url.open(self.ui, burl)
            return pycompat.json_loads(resp.read())
        except util.urlerr.httperror as inst:
            if inst.code == 401:
                raise error.Abort(_(b'authorization failed'))
            if inst.code == 404:
                raise NotFound()
            else:
                raise
###END###
def _submit(self, burl, data, method=b'POST'):
        data = json.dumps(data)
        if method == b'PUT':

            class putrequest(util.urlreq.request):
                def get_method(self):
                    return b'PUT'

            request_type = putrequest
        else:
            request_type = util.urlreq.request
        req = request_type(burl, data, {b'Content-Type': b'application/json'})
        try:
            resp = url.opener(self.ui).open(req)
            return pycompat.json_loads(resp.read())
        except util.urlerr.httperror as inst:
            if inst.code == 401:
                raise error.Abort(_(b'authorization failed'))
            if inst.code == 404:
                raise NotFound()
            else:
                raise
###END###
def filter_real_bug_ids(self, bugs):
        '''remove bug IDs that do not exist in Bugzilla from bugs.'''
        badbugs = set()
        for bugid in bugs:
            burl = self.apiurl((b'bug', bugid), include_fields=b'status')
            try:
                self._fetch(burl)
            except NotFound:
                badbugs.add(bugid)
        for bugid in badbugs:
            del bugs[bugid]
###END###
def filter_cset_known_bug_ids(self, node, bugs):
        '''remove bug IDs where node occurs in comment text from bugs.'''
        sn = short(node)
        for bugid in bugs.keys():
            burl = self.apiurl(
                (b'bug', bugid, b'comment'), include_fields=b'text'
            )
            result = self._fetch(burl)
            comments = result[b'bugs'][pycompat.bytestr(bugid)][b'comments']
            if any(sn in c[b'text'] for c in comments):
                self.ui.status(
                    _(b'bug %d already knows about changeset %s\n')
                    % (bugid, sn)
                )
                del bugs[bugid]
###END###
def updatebug(self, bugid, newstate, text, committer):
        """update the specified bug. Add comment text and set new states.

        If possible add the comment as being from the committer of
        the changeset. Otherwise use the default Bugzilla user.
        """
        bugmod = {}
        if b'hours' in newstate:
            bugmod[b'work_time'] = newstate[b'hours']
        if b'fix' in newstate:
            bugmod[b'status'] = self.fixstatus
            bugmod[b'resolution'] = self.fixresolution
        if bugmod:
            # if we have to change the bugs state do it here
            bugmod[b'comment'] = {
                b'comment': text,
                b'is_private': False,
                b'is_markdown': False,
            }
            burl = self.apiurl((b'bug', bugid))
            self._submit(burl, bugmod, method=b'PUT')
            self.ui.debug(b'updated bug %s\n' % bugid)
        else:
            burl = self.apiurl((b'bug', bugid, b'comment'))
            self._submit(
                burl,
                {
                    b'comment': text,
                    b'is_private': False,
                    b'is_markdown': False,
                },
            )
            self.ui.debug(b'added comment to bug %s\n' % bugid)
###END###
def notify(self, bugs, committer):
        """Force sending of Bugzilla notification emails.

        Only required if the access method does not trigger notification
        emails automatically.
        """
        pass
###END###
def __init__(self, ui, repo):
        self.ui = ui
        self.repo = repo

        bzversion = self.ui.config(b'bugzilla', b'version')
        try:
            bzclass = bugzilla._versions[bzversion]
        except KeyError:
            raise error.Abort(
                _(b'bugzilla version %s not supported') % bzversion
            )
        self.bzdriver = bzclass(self.ui)

        self.bug_re = re.compile(
            self.ui.config(b'bugzilla', b'regexp'), re.IGNORECASE
        )
        self.fix_re = re.compile(
            self.ui.config(b'bugzilla', b'fixregexp'), re.IGNORECASE
        )
        self.split_re = re.compile(br'\D+')
###END###
def find_bugs(self, ctx):
        """return bugs dictionary created from commit comment.

        Extract bug info from changeset comments. Filter out any that are
        not known to Bugzilla, and any that already have a reference to
        the given changeset in their comments.
        """
        start = 0
        bugs = {}
        bugmatch = self.bug_re.search(ctx.description(), start)
        fixmatch = self.fix_re.search(ctx.description(), start)
        while True:
            bugattribs = {}
            if not bugmatch and not fixmatch:
                break
            if not bugmatch:
                m = fixmatch
            elif not fixmatch:
                m = bugmatch
            else:
                if bugmatch.start() < fixmatch.start():
                    m = bugmatch
                else:
                    m = fixmatch
            start = m.end()
            if m is bugmatch:
                bugmatch = self.bug_re.search(ctx.description(), start)
                if b'fix' in bugattribs:
                    del bugattribs[b'fix']
            else:
                fixmatch = self.fix_re.search(ctx.description(), start)
                bugattribs[b'fix'] = None

            try:
                ids = m.group(b'ids')
            except IndexError:
                ids = m.group(1)
            try:
                hours = float(m.group(b'hours'))
                bugattribs[b'hours'] = hours
            except IndexError:
                pass
            except TypeError:
                pass
            except ValueError:
                self.ui.status(_(b"%s: invalid hours\n") % m.group(b'hours'))

            for id in self.split_re.split(ids):
                if not id:
                    continue
                bugs[int(id)] = bugattribs
        if bugs:
            self.bzdriver.filter_real_bug_ids(bugs)
        if bugs:
            self.bzdriver.filter_cset_known_bug_ids(ctx.node(), bugs)
        return bugs
###END###
def update(self, bugid, newstate, ctx):
        '''update bugzilla bug with reference to changeset.'''

        def webroot(root):
            """strip leading prefix of repo root and turn into
            url-safe path."""
            count = int(self.ui.config(b'bugzilla', b'strip'))
            root = util.pconvert(root)
            while count > 0:
                c = root.find(b'/')
                if c == -1:
                    break
                root = root[c + 1 :]
                count -= 1
            return root

        mapfile = None
        tmpl = self.ui.config(b'bugzilla', b'template')
        if not tmpl:
            mapfile = self.ui.config(b'bugzilla', b'style')
        if not mapfile and not tmpl:
            tmpl = _(
                b'changeset {node|short} in repo {root} refers '
                b'to bug {bug}.\ndetails:\n\t{desc|tabindent}'
            )
        spec = logcmdutil.templatespec(tmpl, mapfile)
        t = logcmdutil.changesettemplater(self.ui, self.repo, spec)
        self.ui.pushbuffer()
        t.show(
            ctx,
            changes=ctx.changeset(),
            bug=pycompat.bytestr(bugid),
            hgweb=self.ui.config(b'web', b'baseurl'),
            root=self.repo.root,
            webroot=webroot(self.repo.root),
        )
        data = self.ui.popbuffer()
        self.bzdriver.updatebug(
            bugid, newstate, data, stringutil.email(ctx.user())
        )
###END###
def notify(self, bugs, committer):
        '''ensure Bugzilla users are notified of bug change.'''
        self.bzdriver.notify(bugs, committer)
###END###
def get_method(self):
                    return b'PUT'
###END###
def lookup(self, key):
            try:
                _super = super(parentrevspecrepo, self)
                return _super.lookup(key)
            except error.RepoError:
                pass

            circ = key.find('^')
            tilde = key.find('~')
            if circ < 0 and tilde < 0:
                raise
            elif circ >= 0 and tilde >= 0:
                end = min(circ, tilde)
            else:
                end = max(circ, tilde)

            cl = self.changelog
            base = key[:end]
            try:
                node = _super.lookup(base)
            except error.RepoError:
                # eek - reraise the first error
                return _super.lookup(key)

            rev = cl.rev(node)
            suffix = key[end:]
            i = 0
            while i < len(suffix):
                # foo^N => Nth parent of foo
                # foo^0 == foo
                # foo^1 == foo^ == 1st parent of foo
                # foo^2 == 2nd parent of foo
                if suffix[i] == '^':
                    j = i + 1
                    p = cl.parentrevs(rev)
                    if j < len(suffix) and suffix[j].isdigit():
                        j += 1
                        n = int(suffix[i + 1:j])
                        if n > 2 or n == 2 and p[1] == -1:
                            raise
                    else:
                        n = 1
                    if n:
                        rev = p[n - 1]
                    i = j
                # foo~N => Nth first grandparent of foo
                # foo~0 = foo
                # foo~1 = foo^1 == foo^ == 1st parent of foo
                # foo~2 = foo^1^1 == foo^^ == 1st parent of 1st parent of foo
                elif suffix[i] == '~':
                    j = i + 1
                    while j < len(suffix) and suffix[j].isdigit():
                        j += 1
                    if j == i + 1:
                        raise
                    n = int(suffix[i + 1:j])
                    for k in xrange(n):
                        rev = cl.parentrevs(rev)[0]
                    i = j
                else:
                    raise
            return cl.node(rev)
###END###
def __init__(self, db, path, compression):
        self.nullid = sha1nodeconstants.nullid
        self._db = db
        self._path = path

        self._pathid = None

        # revnum -> node
        self._revtonode = {}
        # node -> revnum
        self._nodetorev = {}
        # node -> data structure
        self._revisions = {}

        self._revisioncache = util.lrucachedict(10)

        self._compengine = compression

        if compression == b'zstd':
            self._cctx = zstd.ZstdCompressor(level=3)
            self._dctx = zstd.ZstdDecompressor()
        else:
            self._cctx = None
            self._dctx = None

        self._refreshindex()
###END###
def _refreshindex(self):
        self._revtonode = {}
        self._nodetorev = {}
        self._revisions = {}

        res = list(
            self._db.execute(
                'SELECT id FROM filepath WHERE path=?', (self._path,)
            )
        )

        if not res:
            self._pathid = None
            return

        self._pathid = res[0][0]

        res = self._db.execute(
            'SELECT id, revnum, node, p1rev, p2rev, linkrev, flags '
            'FROM fileindex '
            'WHERE pathid=? '
            'ORDER BY revnum ASC',
            (self._pathid,),
        )

        for i, row in enumerate(res):
            rid, rev, node, p1rev, p2rev, linkrev, flags = row

            if i != rev:
                raise SQLiteStoreError(
                    _(b'sqlite database has inconsistent revision numbers')
                )

            if p1rev == nullrev:
                p1node = sha1nodeconstants.nullid
            else:
                p1node = self._revtonode[p1rev]

            if p2rev == nullrev:
                p2node = sha1nodeconstants.nullid
            else:
                p2node = self._revtonode[p2rev]

            entry = revisionentry(
                rid=rid,
                rev=rev,
                node=node,
                p1rev=p1rev,
                p2rev=p2rev,
                p1node=p1node,
                p2node=p2node,
                linkrev=linkrev,
                flags=flags,
            )

            self._revtonode[rev] = node
            self._nodetorev[node] = rev
            self._revisions[node] = entry
###END###
def __len__(self):
        return len(self._revisions)
###END###
def __iter__(self):
        return iter(pycompat.xrange(len(self._revisions)))
###END###
def hasnode(self, node):
        if node == sha1nodeconstants.nullid:
            return False

        return node in self._nodetorev
###END###
def revs(self, start=0, stop=None):
        return storageutil.iterrevs(
            len(self._revisions), start=start, stop=stop
        )
###END###
def parents(self, node):
        if node == sha1nodeconstants.nullid:
            return sha1nodeconstants.nullid, sha1nodeconstants.nullid

        if node not in self._revisions:
            raise error.LookupError(node, self._path, _(b'no node'))

        entry = self._revisions[node]
        return entry.p1node, entry.p2node
###END###
def parentrevs(self, rev):
        if rev == nullrev:
            return nullrev, nullrev

        if rev not in self._revtonode:
            raise IndexError(rev)

        entry = self._revisions[self._revtonode[rev]]
        return entry.p1rev, entry.p2rev
###END###
def rev(self, node):
        if node == sha1nodeconstants.nullid:
            return nullrev

        if node not in self._nodetorev:
            raise error.LookupError(node, self._path, _(b'no node'))

        return self._nodetorev[node]
###END###
def node(self, rev):
        if rev == nullrev:
            return sha1nodeconstants.nullid

        if rev not in self._revtonode:
            raise IndexError(rev)

        return self._revtonode[rev]
###END###
def lookup(self, node):
        return storageutil.fileidlookup(self, node, self._path)
###END###
def linkrev(self, rev):
        if rev == nullrev:
            return nullrev

        if rev not in self._revtonode:
            raise IndexError(rev)

        entry = self._revisions[self._revtonode[rev]]
        return entry.linkrev
###END###
def iscensored(self, rev):
        if rev == nullrev:
            return False

        if rev not in self._revtonode:
            raise IndexError(rev)

        return self._revisions[self._revtonode[rev]].flags & FLAG_CENSORED
###END###
def commonancestorsheads(self, node1, node2):
        rev1 = self.rev(node1)
        rev2 = self.rev(node2)

        ancestors = ancestor.commonancestorsheads(self.parentrevs, rev1, rev2)
        return pycompat.maplist(self.node, ancestors)
###END###
def descendants(self, revs):
        # TODO we could implement this using a recursive SQL query, which
        # might be faster.
        return dagop.descendantrevs(revs, self.revs, self.parentrevs)
###END###
def heads(self, start=None, stop=None):
        if start is None and stop is None:
            if not len(self):
                return [sha1nodeconstants.nullid]

        startrev = self.rev(start) if start is not None else nullrev
        stoprevs = {self.rev(n) for n in stop or []}

        revs = dagop.headrevssubset(
            self.revs, self.parentrevs, startrev=startrev, stoprevs=stoprevs
        )

        return [self.node(rev) for rev in revs]
###END###
def children(self, node):
        rev = self.rev(node)

        res = self._db.execute(
            'SELECT'
            '  node '
            '  FROM filedata '
            '  WHERE path=? AND (p1rev=? OR p2rev=?) '
            '  ORDER BY revnum ASC',
            (self._path, rev, rev),
        )

        return [row[0] for row in res]
###END###
def size(self, rev):
        if rev == nullrev:
            return 0

        if rev not in self._revtonode:
            raise IndexError(rev)

        node = self._revtonode[rev]

        if self.renamed(node):
            return len(self.read(node))

        return len(self.revision(node))
###END###
def revision(self, node, raw=False, _verifyhash=True):
        if node in (sha1nodeconstants.nullid, nullrev):
            return b''

        if isinstance(node, int):
            node = self.node(node)

        if node not in self._nodetorev:
            raise error.LookupError(node, self._path, _(b'no node'))

        if node in self._revisioncache:
            return self._revisioncache[node]

        # Because we have a fulltext revision cache, we are able to
        # short-circuit delta chain traversal and decompression as soon as
        # we encounter a revision in the cache.

        stoprids = {self._revisions[n].rid: n for n in self._revisioncache}

        if not stoprids:
            stoprids[-1] = None

        fulltext = resolvedeltachain(
            self._db,
            self._pathid,
            node,
            self._revisioncache,
            stoprids,
            zstddctx=self._dctx,
        )

        # Don't verify hashes if parent nodes were rewritten, as the hash
        # wouldn't verify.
        if self._revisions[node].flags & (FLAG_MISSING_P1 | FLAG_MISSING_P2):
            _verifyhash = False

        if _verifyhash:
            self._checkhash(fulltext, node)
            self._revisioncache[node] = fulltext

        return fulltext
###END###
def rawdata(self, *args, **kwargs):
        return self.revision(*args, **kwargs)
###END###
def read(self, node):
        return storageutil.filtermetadata(self.revision(node))
###END###
def renamed(self, node):
        return storageutil.filerevisioncopied(self, node)
###END###
def cmp(self, node, fulltext):
        return not storageutil.filedataequivalent(self, node, fulltext)
###END###
def emitrevisions(
        self,
        nodes,
        nodesorder=None,
        revisiondata=False,
        assumehaveparentrevisions=False,
        deltamode=repository.CG_DELTAMODE_STD,
        sidedata_helpers=None,
    ):
        if nodesorder not in (b'nodes', b'storage', b'linear', None):
            raise error.ProgrammingError(
                b'unhandled value for nodesorder: %s' % nodesorder
            )

        nodes = [n for n in nodes if n != sha1nodeconstants.nullid]

        if not nodes:
            return

        # TODO perform in a single query.
        res = self._db.execute(
            'SELECT revnum, deltaid FROM fileindex '
            'WHERE pathid=? '
            '    AND node in (%s)' % (','.join(['?'] * len(nodes))),
            tuple([self._pathid] + nodes),
        )

        deltabases = {}

        for rev, deltaid in res:
            res = self._db.execute(
                'SELECT revnum from fileindex WHERE pathid=? AND deltaid=?',
                (self._pathid, deltaid),
            )
            deltabases[rev] = res.fetchone()[0]

        # TODO define revdifffn so we can use delta from storage.
        for delta in storageutil.emitrevisions(
            self,
            nodes,
            nodesorder,
            sqliterevisiondelta,
            deltaparentfn=deltabases.__getitem__,
            revisiondata=revisiondata,
            assumehaveparentrevisions=assumehaveparentrevisions,
            deltamode=deltamode,
            sidedata_helpers=sidedata_helpers,
        ):

            yield delta
###END###
def add(self, filedata, meta, transaction, linkrev, p1, p2):
        if meta or filedata.startswith(b'\x01\n'):
            filedata = storageutil.packmeta(meta, filedata)

        rev = self.addrevision(filedata, transaction, linkrev, p1, p2)
        return self.node(rev)
###END###
def addrevision(
        self,
        revisiondata,
        transaction,
        linkrev,
        p1,
        p2,
        node=None,
        flags=0,
        cachedelta=None,
    ):
        if flags:
            raise SQLiteStoreError(_(b'flags not supported on revisions'))

        validatehash = node is not None
        node = node or storageutil.hashrevisionsha1(revisiondata, p1, p2)

        if validatehash:
            self._checkhash(revisiondata, node, p1, p2)

        rev = self._nodetorev.get(node)
        if rev is not None:
            return rev

        rev = self._addrawrevision(
            node, revisiondata, transaction, linkrev, p1, p2
        )

        self._revisioncache[node] = revisiondata
        return rev
###END###
def addgroup(
        self,
        deltas,
        linkmapper,
        transaction,
        addrevisioncb=None,
        duplicaterevisioncb=None,
        maybemissingparents=False,
    ):
        empty = True

        for (
            node,
            p1,
            p2,
            linknode,
            deltabase,
            delta,
            wireflags,
            sidedata,
        ) in deltas:
            storeflags = 0

            if wireflags & repository.REVISION_FLAG_CENSORED:
                storeflags |= FLAG_CENSORED

            if wireflags & ~repository.REVISION_FLAG_CENSORED:
                raise SQLiteStoreError(b'unhandled revision flag')

            if maybemissingparents:
                if p1 != sha1nodeconstants.nullid and not self.hasnode(p1):
                    p1 = sha1nodeconstants.nullid
                    storeflags |= FLAG_MISSING_P1

                if p2 != sha1nodeconstants.nullid and not self.hasnode(p2):
                    p2 = sha1nodeconstants.nullid
                    storeflags |= FLAG_MISSING_P2

            baserev = self.rev(deltabase)

            # If base is censored, delta must be full replacement in a single
            # patch operation.
            if baserev != nullrev and self.iscensored(baserev):
                hlen = struct.calcsize(b'>lll')
                oldlen = len(self.rawdata(deltabase, _verifyhash=False))
                newlen = len(delta) - hlen

                if delta[:hlen] != mdiff.replacediffheader(oldlen, newlen):
                    raise error.CensoredBaseError(self._path, deltabase)

            if not (storeflags & FLAG_CENSORED) and storageutil.deltaiscensored(
                delta, baserev, lambda x: len(self.rawdata(x))
            ):
                storeflags |= FLAG_CENSORED

            linkrev = linkmapper(linknode)

            if node in self._revisions:
                # Possibly reset parents to make them proper.
                entry = self._revisions[node]

                if (
                    entry.flags & FLAG_MISSING_P1
                    and p1 != sha1nodeconstants.nullid
                ):
                    entry.p1node = p1
                    entry.p1rev = self._nodetorev[p1]
                    entry.flags &= ~FLAG_MISSING_P1

                    self._db.execute(
                        'UPDATE fileindex SET p1rev=?, flags=? WHERE id=?',
                        (self._nodetorev[p1], entry.flags, entry.rid),
                    )

                if (
                    entry.flags & FLAG_MISSING_P2
                    and p2 != sha1nodeconstants.nullid
                ):
                    entry.p2node = p2
                    entry.p2rev = self._nodetorev[p2]
                    entry.flags &= ~FLAG_MISSING_P2

                    self._db.execute(
                        'UPDATE fileindex SET p2rev=?, flags=? WHERE id=?',
                        (self._nodetorev[p1], entry.flags, entry.rid),
                    )

                if duplicaterevisioncb:
                    duplicaterevisioncb(self, self.rev(node))
                empty = False
                continue

            if deltabase == sha1nodeconstants.nullid:
                text = mdiff.patch(b'', delta)
                storedelta = None
            else:
                text = None
                storedelta = (deltabase, delta)

            rev = self._addrawrevision(
                node,
                text,
                transaction,
                linkrev,
                p1,
                p2,
                storedelta=storedelta,
                flags=storeflags,
            )

            if addrevisioncb:
                addrevisioncb(self, rev)
            empty = False

        return not empty
###END###
def censorrevision(self, tr, censornode, tombstone=b''):
        tombstone = storageutil.packmeta({b'censored': tombstone}, b'')

        # This restriction is cargo culted from revlogs and makes no sense for
        # SQLite, since columns can be resized at will.
        if len(tombstone) > len(self.rawdata(censornode)):
            raise error.Abort(
                _(b'censor tombstone must be no longer than censored data')
            )

        # We need to replace the censored revision's data with the tombstone.
        # But replacing that data will have implications for delta chains that
        # reference it.
        #
        # While "better," more complex strategies are possible, we do something
        # simple: we find delta chain children of the censored revision and we
        # replace those incremental deltas with fulltexts of their corresponding
        # revision. Then we delete the now-unreferenced delta and original
        # revision and insert a replacement.

        # Find the delta to be censored.
        censoreddeltaid = self._db.execute(
            'SELECT deltaid FROM fileindex WHERE id=?',
            (self._revisions[censornode].rid,),
        ).fetchone()[0]

        # Find all its delta chain children.
        # TODO once we support storing deltas for !files, we'll need to look
        # for those delta chains too.
        rows = list(
            self._db.execute(
                'SELECT id, pathid, node FROM fileindex '
                'WHERE deltabaseid=? OR deltaid=?',
                (censoreddeltaid, censoreddeltaid),
            )
        )

        for row in rows:
            rid, pathid, node = row

            fulltext = resolvedeltachain(
                self._db, pathid, node, {}, {-1: None}, zstddctx=self._dctx
            )

            deltahash = hashutil.sha1(fulltext).digest()

            if self._compengine == b'zstd':
                deltablob = self._cctx.compress(fulltext)
                compression = COMPRESSION_ZSTD
            elif self._compengine == b'zlib':
                deltablob = zlib.compress(fulltext)
                compression = COMPRESSION_ZLIB
            elif self._compengine == b'none':
                deltablob = fulltext
                compression = COMPRESSION_NONE
            else:
                raise error.ProgrammingError(
                    b'unhandled compression engine: %s' % self._compengine
                )

            if len(deltablob) >= len(fulltext):
                deltablob = fulltext
                compression = COMPRESSION_NONE

            deltaid = insertdelta(self._db, compression, deltahash, deltablob)

            self._db.execute(
                'UPDATE fileindex SET deltaid=?, deltabaseid=NULL '
                'WHERE id=?',
                (deltaid, rid),
            )

        # Now create the tombstone delta and replace the delta on the censored
        # node.
        deltahash = hashutil.sha1(tombstone).digest()
        tombstonedeltaid = insertdelta(
            self._db, COMPRESSION_NONE, deltahash, tombstone
        )

        flags = self._revisions[censornode].flags
        flags |= FLAG_CENSORED

        self._db.execute(
            'UPDATE fileindex SET flags=?, deltaid=?, deltabaseid=NULL '
            'WHERE pathid=? AND node=?',
            (flags, tombstonedeltaid, self._pathid, censornode),
        )

        self._db.execute('DELETE FROM delta WHERE id=?', (censoreddeltaid,))

        self._refreshindex()
        self._revisioncache.clear()
###END###
def getstrippoint(self, minlink):
        return storageutil.resolvestripinfo(
            minlink,
            len(self) - 1,
            [self.rev(n) for n in self.heads()],
            self.linkrev,
            self.parentrevs,
        )
###END###
def strip(self, minlink, transaction):
        if not len(self):
            return

        rev, _ignored = self.getstrippoint(minlink)

        if rev == len(self):
            return

        for rev in self.revs(rev):
            self._db.execute(
                'DELETE FROM fileindex WHERE pathid=? AND node=?',
                (self._pathid, self.node(rev)),
            )

        # TODO how should we garbage collect data in delta table?

        self._refreshindex()
###END###
def files(self):
        return []
###END###
def sidedata(self, nodeorrev, _df=None):
        # Not supported for now
        return {}
###END###
def storageinfo(
        self,
        exclusivefiles=False,
        sharedfiles=False,
        revisionscount=False,
        trackedsize=False,
        storedsize=False,
    ):
        d = {}

        if exclusivefiles:
            d[b'exclusivefiles'] = []

        if sharedfiles:
            # TODO list sqlite file(s) here.
            d[b'sharedfiles'] = []

        if revisionscount:
            d[b'revisionscount'] = len(self)

        if trackedsize:
            d[b'trackedsize'] = sum(
                len(self.revision(node)) for node in self._nodetorev
            )

        if storedsize:
            # TODO implement this?
            d[b'storedsize'] = None

        return d
###END###
def verifyintegrity(self, state):
        state[b'skipread'] = set()

        for rev in self:
            node = self.node(rev)

            try:
                self.revision(node)
            except Exception as e:
                yield sqliteproblem(
                    error=_(b'unpacking %s: %s') % (short(node), e), node=node
                )

                state[b'skipread'].add(node)
###END###
def _checkhash(self, fulltext, node, p1=None, p2=None):
        if p1 is None and p2 is None:
            p1, p2 = self.parents(node)

        if node == storageutil.hashrevisionsha1(fulltext, p1, p2):
            return

        try:
            del self._revisioncache[node]
        except KeyError:
            pass

        if storageutil.iscensoredtext(fulltext):
            raise error.CensoredNodeError(self._path, node, fulltext)

        raise SQLiteStoreError(_(b'integrity check failed on %s') % self._path)
###END###
def _addrawrevision(
        self,
        node,
        revisiondata,
        transaction,
        linkrev,
        p1,
        p2,
        storedelta=None,
        flags=0,
    ):
        if self._pathid is None:
            res = self._db.execute(
                'INSERT INTO filepath (path) VALUES (?)', (self._path,)
            )
            self._pathid = res.lastrowid

        # For simplicity, always store a delta against p1.
        # TODO we need a lot more logic here to make behavior reasonable.

        if storedelta:
            deltabase, delta = storedelta

            if isinstance(deltabase, int):
                deltabase = self.node(deltabase)

        else:
            assert revisiondata is not None
            deltabase = p1

            if deltabase == sha1nodeconstants.nullid:
                delta = revisiondata
            else:
                delta = mdiff.textdiff(
                    self.revision(self.rev(deltabase)), revisiondata
                )

        # File index stores a pointer to its delta and the parent delta.
        # The parent delta is stored via a pointer to the fileindex PK.
        if deltabase == sha1nodeconstants.nullid:
            baseid = None
        else:
            baseid = self._revisions[deltabase].rid

        # Deltas are stored with a hash of their content. This allows
        # us to de-duplicate. The table is configured to ignore conflicts
        # and it is faster to just insert and silently noop than to look
        # first.
        deltahash = hashutil.sha1(delta).digest()

        if self._compengine == b'zstd':
            deltablob = self._cctx.compress(delta)
            compression = COMPRESSION_ZSTD
        elif self._compengine == b'zlib':
            deltablob = zlib.compress(delta)
            compression = COMPRESSION_ZLIB
        elif self._compengine == b'none':
            deltablob = delta
            compression = COMPRESSION_NONE
        else:
            raise error.ProgrammingError(
                b'unhandled compression engine: %s' % self._compengine
            )

        # Don't store compressed data if it isn't practical.
        if len(deltablob) >= len(delta):
            deltablob = delta
            compression = COMPRESSION_NONE

        deltaid = insertdelta(self._db, compression, deltahash, deltablob)

        rev = len(self)

        if p1 == sha1nodeconstants.nullid:
            p1rev = nullrev
        else:
            p1rev = self._nodetorev[p1]

        if p2 == sha1nodeconstants.nullid:
            p2rev = nullrev
        else:
            p2rev = self._nodetorev[p2]

        rid = self._db.execute(
            'INSERT INTO fileindex ('
            '    pathid, revnum, node, p1rev, p2rev, linkrev, flags, '
            '    deltaid, deltabaseid) '
            '    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)',
            (
                self._pathid,
                rev,
                node,
                p1rev,
                p2rev,
                linkrev,
                flags,
                deltaid,
                baseid,
            ),
        ).lastrowid

        entry = revisionentry(
            rid=rid,
            rev=rev,
            node=node,
            p1rev=p1rev,
            p2rev=p2rev,
            p1node=p1,
            p2node=p2,
            linkrev=linkrev,
            flags=flags,
        )

        self._nodetorev[node] = rev
        self._revtonode[rev] = node
        self._revisions[node] = entry

        return rev
###END###
def cancopy(self):
        return False
###END###
def transaction(self, *args, **kwargs):
        current = self.currenttransaction()

        tr = super(sqliterepository, self).transaction(*args, **kwargs)

        if current:
            return tr

        self._dbconn.execute('BEGIN TRANSACTION')

        def committransaction(_):
            self._dbconn.commit()

        tr.addfinalize(b'sqlitestore', committransaction)

        return tr
###END###
def _dbconn(self):
        # SQLite connections can only be used on the thread that created
        # them. In most cases, this "just works." However, hgweb uses
        # multiple threads.
        tid = threading.current_thread().ident

        if self._db:
            if self._db[0] == tid:
                return self._db[1]

        db = makedb(self.svfs.join(b'db.sqlite'))
        self._db = (tid, db)

        return db
###END###
def file(self, path):
        if path[0] == b'/':
            path = path[1:]

        if REQUIREMENT_ZSTD in self.requirements:
            compression = b'zstd'
        elif REQUIREMENT_ZLIB in self.requirements:
            compression = b'zlib'
        elif REQUIREMENT_NONE in self.requirements:
            compression = b'none'
        else:
            raise error.Abort(
                _(
                    b'unable to determine what compression engine '
                    b'to use for SQLite storage'
                )
            )

        return sqlitefilestore(self._dbconn, path, compression)
###END###
def __getitem__(name):
        def nullfunc(*args, **kwds):
            return

        return nullfunc
###END###
def __init__(self, repo):
        self._repo = repo
###END###
def data(self):
        return b''
###END###
def node(self):
        return self._repo.nullid
###END###
def __init__(self, basectx, memworkingcopy):
        self.basectx = basectx
        self.memworkingcopy = memworkingcopy
###END###
def getfile(self, path):
        """comply with mercurial.patch.filestore.getfile"""
        if path not in self.basectx:
            return None, None, None
        fctx = self.basectx[path]
        if path in self.memworkingcopy:
            content = self.memworkingcopy[path]
        else:
            content = fctx.data()
        mode = (fctx.islink(), fctx.isexec())
        copy = fctx.copysource()
        return content, mode, copy
###END###
def __init__(self, fctxs, path, ui=None, opts=None):
        """([fctx], ui or None) -> None

        fctxs should be linear, and sorted by topo order - oldest first.
        fctxs[0] will be considered as "immutable" and will not be changed.
        """
        self.fctxs = fctxs
        self.path = path
        self.ui = ui or nullui()
        self.opts = opts or {}

        # following fields are built from fctxs. they exist for perf reason
        self.contents = [f.data() for f in fctxs]
        self.contentlines = pycompat.maplist(mdiff.splitnewlines, self.contents)
        self.linelog = self._buildlinelog()
        if self.ui.debugflag:
            assert self._checkoutlinelog() == self.contents

        # following fields will be filled later
        self.chunkstats = [0, 0]  # [adopted, total : int]
        self.targetlines = []  # [str]
        self.fixups = []  # [(linelog rev, a1, a2, b1, b2)]
        self.finalcontents = []  # [str]
        self.ctxaffected = set()
###END###
def diffwith(self, targetfctx, fm=None):
        """calculate fixups needed by examining the differences between
        self.fctxs[-1] and targetfctx, chunk by chunk.

        targetfctx is the target state we move towards. we may or may not be
        able to get there because not all modified chunks can be amended into
        a non-public fctx unambiguously.

        call this only once, before apply().

        update self.fixups, self.chunkstats, and self.targetlines.
        """
        a = self.contents[-1]
        alines = self.contentlines[-1]
        b = targetfctx.data()
        blines = mdiff.splitnewlines(b)
        self.targetlines = blines

        self.linelog.annotate(self.linelog.maxrev)
        annotated = self.linelog.annotateresult  # [(linelog rev, linenum)]
        assert len(annotated) == len(alines)
        # add a dummy end line to make insertion at the end easier
        if annotated:
            dummyendline = (annotated[-1][0], annotated[-1][1] + 1)
            annotated.append(dummyendline)

        # analyse diff blocks
        for chunk in self._alldiffchunks(a, b, alines, blines):
            newfixups = self._analysediffchunk(chunk, annotated)
            self.chunkstats[0] += bool(newfixups)  # 1 or 0
            self.chunkstats[1] += 1
            self.fixups += newfixups
            if fm is not None:
                self._showchanges(fm, alines, blines, chunk, newfixups)
###END###
def apply(self):
        """apply self.fixups. update self.linelog, self.finalcontents.

        call this only once, before getfinalcontent(), after diffwith().
        """
        # the following is unnecessary, as it's done by "diffwith":
        #   self.linelog.annotate(self.linelog.maxrev)
        for rev, a1, a2, b1, b2 in reversed(self.fixups):
            blines = self.targetlines[b1:b2]
            if self.ui.debugflag:
                idx = (max(rev - 1, 0)) // 2
                self.ui.write(
                    _(b'%s: chunk %d:%d -> %d lines\n')
                    % (short(self.fctxs[idx].node()), a1, a2, len(blines))
                )
            self.linelog.replacelines(rev, a1, a2, b1, b2)
        if self.opts.get(b'edit_lines', False):
            self.finalcontents = self._checkoutlinelogwithedits()
        else:
            self.finalcontents = self._checkoutlinelog()
###END###
def getfinalcontent(self, fctx):
        """(fctx) -> str. get modified file content for a given filecontext"""
        idx = self.fctxs.index(fctx)
        return self.finalcontents[idx]
###END###
def _analysediffchunk(self, chunk, annotated):
        """analyse a different chunk and return new fixups found

        return [] if no lines from the chunk can be safely applied.

        the chunk (or lines) cannot be safely applied, if, for example:
          - the modified (deleted) lines belong to a public changeset
            (self.fctxs[0])
          - the chunk is a pure insertion and the adjacent lines (at most 2
            lines) belong to different non-public changesets, or do not belong
            to any non-public changesets.
          - the chunk is modifying lines from different changesets.
            in this case, if the number of lines deleted equals to the number
            of lines added, assume it's a simple 1:1 map (could be wrong).
            otherwise, give up.
          - the chunk is modifying lines from a single non-public changeset,
            but other revisions touch the area as well. i.e. the lines are
            not continuous as seen from the linelog.
        """
        a1, a2, b1, b2 = chunk
        # find involved indexes from annotate result
        involved = annotated[a1:a2]
        if not involved and annotated:  # a1 == a2 and a is not empty
            # pure insertion, check nearby lines. ignore lines belong
            # to the public (first) changeset (i.e. annotated[i][0] == 1)
            nearbylinenums = {a2, max(0, a1 - 1)}
            involved = [
                annotated[i] for i in nearbylinenums if annotated[i][0] != 1
            ]
        involvedrevs = list({r for r, l in involved})
        newfixups = []
        if len(involvedrevs) == 1 and self._iscontinuous(a1, a2 - 1, True):
            # chunk belongs to a single revision
            rev = involvedrevs[0]
            if rev > 1:
                fixuprev = rev + 1
                newfixups.append((fixuprev, a1, a2, b1, b2))
        elif a2 - a1 == b2 - b1 or b1 == b2:
            # 1:1 line mapping, or chunk was deleted
            for i in pycompat.xrange(a1, a2):
                rev, linenum = annotated[i]
                if rev > 1:
                    if b1 == b2:  # deletion, simply remove that single line
                        nb1 = nb2 = 0
                    else:  # 1:1 line mapping, change the corresponding rev
                        nb1 = b1 + i - a1
                        nb2 = nb1 + 1
                    fixuprev = rev + 1
                    newfixups.append((fixuprev, i, i + 1, nb1, nb2))
        return self._optimizefixups(newfixups)
###END###
def _alldiffchunks(a, b, alines, blines):
        """like mdiff.allblocks, but only care about differences"""
        blocks = mdiff.allblocks(a, b, lines1=alines, lines2=blines)
        for chunk, btype in blocks:
            if btype != b'!':
                continue
            yield chunk
###END###
def _buildlinelog(self):
        """calculate the initial linelog based on self.content{,line}s.
        this is similar to running a partial "annotate".
        """
        llog = linelog.linelog()
        a, alines = b'', []
        for i in pycompat.xrange(len(self.contents)):
            b, blines = self.contents[i], self.contentlines[i]
            llrev = i * 2 + 1
            chunks = self._alldiffchunks(a, b, alines, blines)
            for a1, a2, b1, b2 in reversed(list(chunks)):
                llog.replacelines(llrev, a1, a2, b1, b2)
            a, alines = b, blines
        return llog
###END###
def _checkoutlinelog(self):
        """() -> [str]. check out file contents from linelog"""
        contents = []
        for i in pycompat.xrange(len(self.contents)):
            rev = (i + 1) * 2
            self.linelog.annotate(rev)
            content = b''.join(map(self._getline, self.linelog.annotateresult))
            contents.append(content)
        return contents
###END###
def _checkoutlinelogwithedits(self):
        """() -> [str]. prompt all lines for edit"""
        alllines = self.linelog.getalllines()
        # header
        editortext = (
            _(
                b'HG: editing %s\nHG: "y" means the line to the right '
                b'exists in the changeset to the top\nHG:\n'
            )
            % self.fctxs[-1].path()
        )
        # [(idx, fctx)]. hide the dummy emptyfilecontext
        visiblefctxs = [
            (i, f)
            for i, f in enumerate(self.fctxs)
            if not isinstance(f, emptyfilecontext)
        ]
        for i, (j, f) in enumerate(visiblefctxs):
            editortext += _(b'HG: %s/%s %s %s\n') % (
                b'|' * i,
                b'-' * (len(visiblefctxs) - i + 1),
                short(f.node()),
                f.description().split(b'\n', 1)[0],
            )
        editortext += _(b'HG: %s\n') % (b'|' * len(visiblefctxs))
        # figure out the lifetime of a line, this is relatively inefficient,
        # but probably fine
        lineset = defaultdict(lambda: set())  # {(llrev, linenum): {llrev}}
        for i, f in visiblefctxs:
            self.linelog.annotate((i + 1) * 2)
            for l in self.linelog.annotateresult:
                lineset[l].add(i)
        # append lines
        for l in alllines:
            editortext += b'    %s : %s' % (
                b''.join(
                    [
                        (b'y' if i in lineset[l] else b' ')
                        for i, _f in visiblefctxs
                    ]
                ),
                self._getline(l),
            )
        # run editor
        editedtext = self.ui.edit(editortext, b'', action=b'absorb')
        if not editedtext:
            raise error.InputError(_(b'empty editor text'))
        # parse edited result
        contents = [b''] * len(self.fctxs)
        leftpadpos = 4
        colonpos = leftpadpos + len(visiblefctxs) + 1
        for l in mdiff.splitnewlines(editedtext):
            if l.startswith(b'HG:'):
                continue
            if l[colonpos - 1 : colonpos + 2] != b' : ':
                raise error.InputError(_(b'malformed line: %s') % l)
            linecontent = l[colonpos + 2 :]
            for i, ch in enumerate(
                pycompat.bytestr(l[leftpadpos : colonpos - 1])
            ):
                if ch == b'y':
                    contents[visiblefctxs[i][0]] += linecontent
        # chunkstats is hard to calculate if anything changes, therefore
        # set them to just a simple value (1, 1).
        if editedtext != editortext:
            self.chunkstats = [1, 1]
        return contents
###END###
def _getline(self, lineinfo):
        """((rev, linenum)) -> str. convert rev+line number to line content"""
        rev, linenum = lineinfo
        if rev & 1:  # odd: original line taken from fctxs
            return self.contentlines[rev // 2][linenum]
        else:  # even: fixup line from targetfctx
            return self.targetlines[linenum]
###END###
def _iscontinuous(self, a1, a2, closedinterval=False):
        """(a1, a2 : int) -> bool

        check if these lines are continuous. i.e. no other insertions or
        deletions (from other revisions) among these lines.

        closedinterval decides whether a2 should be included or not. i.e. is
        it [a1, a2), or [a1, a2] ?
        """
        if a1 >= a2:
            return True
        llog = self.linelog
        offset1 = llog.getoffset(a1)
        offset2 = llog.getoffset(a2) + int(closedinterval)
        linesinbetween = llog.getalllines(offset1, offset2)
        return len(linesinbetween) == a2 - a1 + int(closedinterval)
###END###
def _optimizefixups(self, fixups):
        """[(rev, a1, a2, b1, b2)] -> [(rev, a1, a2, b1, b2)].
        merge adjacent fixups to make them less fragmented.
        """
        result = []
        pcurrentchunk = [[-1, -1, -1, -1, -1]]

        def pushchunk():
            if pcurrentchunk[0][0] != -1:
                result.append(tuple(pcurrentchunk[0]))

        for i, chunk in enumerate(fixups):
            rev, a1, a2, b1, b2 = chunk
            lastrev = pcurrentchunk[0][0]
            lasta2 = pcurrentchunk[0][2]
            lastb2 = pcurrentchunk[0][4]
            if (
                a1 == lasta2
                and b1 == lastb2
                and rev == lastrev
                and self._iscontinuous(max(a1 - 1, 0), a1)
            ):
                # merge into currentchunk
                pcurrentchunk[0][2] = a2
                pcurrentchunk[0][4] = b2
            else:
                pushchunk()
                pcurrentchunk[0] = list(chunk)
        pushchunk()
        return result
###END###
def _showchanges(self, fm, alines, blines, chunk, fixups):
        def trim(line):
            if line.endswith(b'\n'):
                line = line[:-1]
            return line

        # this is not optimized for perf but _showchanges only gets executed
        # with an extra command-line flag.
        a1, a2, b1, b2 = chunk
        aidxs, bidxs = [0] * (a2 - a1), [0] * (b2 - b1)
        for idx, fa1, fa2, fb1, fb2 in fixups:
            for i in pycompat.xrange(fa1, fa2):
                aidxs[i - a1] = (max(idx, 1) - 1) // 2
            for i in pycompat.xrange(fb1, fb2):
                bidxs[i - b1] = (max(idx, 1) - 1) // 2

        fm.startitem()
        fm.write(
            b'hunk',
            b'        %s\n',
            b'@@ -%d,%d +%d,%d @@' % (a1, a2 - a1, b1, b2 - b1),
            label=b'diff.hunk',
        )
        fm.data(path=self.path, linetype=b'hunk')

        def writeline(idx, diffchar, line, linetype, linelabel):
            fm.startitem()
            node = b''
            if idx:
                ctx = self.fctxs[idx]
                fm.context(fctx=ctx)
                node = ctx.hex()
                self.ctxaffected.add(ctx.changectx())
            fm.write(b'node', b'%-7.7s ', node, label=b'absorb.node')
            fm.write(
                b'diffchar ' + linetype,
                b'%s%s\n',
                diffchar,
                line,
                label=linelabel,
            )
            fm.data(path=self.path, linetype=linetype)

        for i in pycompat.xrange(a1, a2):
            writeline(
                aidxs[i - a1],
                b'-',
                trim(alines[i]),
                b'deleted',
                b'diff.deleted',
            )
        for i in pycompat.xrange(b1, b2):
            writeline(
                bidxs[i - b1],
                b'+',
                trim(blines[i]),
                b'inserted',
                b'diff.inserted',
            )
###END###
def __init__(self, stack, ui=None, opts=None):
        """([ctx], ui or None) -> None

        stack: should be linear, and sorted by topo order - oldest first.
        all commits in stack are considered mutable.
        """
        assert stack
        self.ui = ui or nullui()
        self.opts = opts or {}
        self.stack = stack
        self.repo = stack[-1].repo().unfiltered()

        # following fields will be filled later
        self.paths = []  # [str]
        self.status = None  # ctx.status output
        self.fctxmap = {}  # {path: {ctx: fctx}}
        self.fixupmap = {}  # {path: filefixupstate}
        self.replacemap = {}  # {oldnode: newnode or None}
        self.finalnode = None  # head after all fixups
        self.ctxaffected = set()
###END###
def diffwith(self, targetctx, match=None, fm=None):
        """diff and prepare fixups. update self.fixupmap, self.paths"""
        # only care about modified files
        self.status = self.stack[-1].status(targetctx, match)
        self.paths = []
        # but if --edit-lines is used, the user may want to edit files
        # even if they are not modified
        editopt = self.opts.get(b'edit_lines')
        if not self.status.modified and editopt and match:
            interestingpaths = match.files()
        else:
            interestingpaths = self.status.modified
        # prepare the filefixupstate
        seenfctxs = set()
        # sorting is necessary to eliminate ambiguity for the "double move"
        # case: "hg cp A B; hg cp A C; hg rm A", then only "B" can affect "A".
        for path in sorted(interestingpaths):
            self.ui.debug(b'calculating fixups for %s\n' % path)
            targetfctx = targetctx[path]
            fctxs, ctx2fctx = getfilestack(self.stack, path, seenfctxs)
            # ignore symbolic links or binary, or unchanged files
            if any(
                f.islink() or stringutil.binary(f.data())
                for f in [targetfctx] + fctxs
                if not isinstance(f, emptyfilecontext)
            ):
                continue
            if targetfctx.data() == fctxs[-1].data() and not editopt:
                continue
            seenfctxs.update(fctxs[1:])
            self.fctxmap[path] = ctx2fctx
            fstate = filefixupstate(fctxs, path, ui=self.ui, opts=self.opts)
            if fm is not None:
                fm.startitem()
                fm.plain(b'showing changes for ')
                fm.write(b'path', b'%s\n', path, label=b'absorb.path')
                fm.data(linetype=b'path')
            fstate.diffwith(targetfctx, fm)
            self.fixupmap[path] = fstate
            self.paths.append(path)
            self.ctxaffected.update(fstate.ctxaffected)
###END###
def apply(self):
        """apply fixups to individual filefixupstates"""
        for path, state in pycompat.iteritems(self.fixupmap):
            if self.ui.debugflag:
                self.ui.write(_(b'applying fixups to %s\n') % path)
            state.apply()
###END###
def chunkstats(self):
        """-> {path: chunkstats}. collect chunkstats from filefixupstates"""
        return {
            path: state.chunkstats
            for path, state in pycompat.iteritems(self.fixupmap)
        }
###END###
def commit(self):
        """commit changes. update self.finalnode, self.replacemap"""
        with self.repo.transaction(b'absorb') as tr:
            self._commitstack()
            self._movebookmarks(tr)
            if self.repo[b'.'].node() in self.replacemap:
                self._moveworkingdirectoryparent()
            self._cleanupoldcommits()
        return self.finalnode
###END###
def printchunkstats(self):
        """print things like '1 of 2 chunk(s) applied'"""
        ui = self.ui
        chunkstats = self.chunkstats
        if ui.verbose:
            # chunkstats for each file
            for path, stat in pycompat.iteritems(chunkstats):
                if stat[0]:
                    ui.write(
                        _(b'%s: %d of %d chunk(s) applied\n')
                        % (path, stat[0], stat[1])
                    )
        elif not ui.quiet:
            # a summary for all files
            stats = chunkstats.values()
            applied, total = (sum(s[i] for s in stats) for i in (0, 1))
            ui.write(_(b'%d of %d chunk(s) applied\n') % (applied, total))
###END###
def _commitstack(self):
        """make new commits. update self.finalnode, self.replacemap.
        it is splitted from "commit" to avoid too much indentation.
        """
        # last node (20-char) committed by us
        lastcommitted = None
        # p1 which overrides the parent of the next commit, "None" means use
        # the original parent unchanged
        nextp1 = None
        for ctx in self.stack:
            memworkingcopy = self._getnewfilecontents(ctx)
            if not memworkingcopy and not lastcommitted:
                # nothing changed, nothing commited
                nextp1 = ctx
                continue
            willbecomenoop = ctx.files() and self._willbecomenoop(
                memworkingcopy, ctx, nextp1
            )
            if self.skip_empty_successor and willbecomenoop:
                # changeset is no longer necessary
                self.replacemap[ctx.node()] = None
                msg = _(b'became empty and was dropped')
            else:
                # changeset needs re-commit
                nodestr = self._commitsingle(memworkingcopy, ctx, p1=nextp1)
                lastcommitted = self.repo[nodestr]
                nextp1 = lastcommitted
                self.replacemap[ctx.node()] = lastcommitted.node()
                if memworkingcopy:
                    if willbecomenoop:
                        msg = _(b'%d file(s) changed, became empty as %s')
                    else:
                        msg = _(b'%d file(s) changed, became %s')
                    msg = msg % (
                        len(memworkingcopy),
                        self._ctx2str(lastcommitted),
                    )
                else:
                    msg = _(b'became %s') % self._ctx2str(lastcommitted)
            if self.ui.verbose and msg:
                self.ui.write(_(b'%s: %s\n') % (self._ctx2str(ctx), msg))
        self.finalnode = lastcommitted and lastcommitted.node()
###END###
def _ctx2str(self, ctx):
        if self.ui.debugflag:
            return b'%d:%s' % (ctx.rev(), ctx.hex())
        else:
            return b'%d:%s' % (ctx.rev(), short(ctx.node()))
###END###
def _getnewfilecontents(self, ctx):
        """(ctx) -> {path: str}

        fetch file contents from filefixupstates.
        return the working copy overrides - files different from ctx.
        """
        result = {}
        for path in self.paths:
            ctx2fctx = self.fctxmap[path]  # {ctx: fctx}
            if ctx not in ctx2fctx:
                continue
            fctx = ctx2fctx[ctx]
            content = fctx.data()
            newcontent = self.fixupmap[path].getfinalcontent(fctx)
            if content != newcontent:
                result[fctx.path()] = newcontent
        return result
###END###
def _movebookmarks(self, tr):
        repo = self.repo
        needupdate = [
            (name, self.replacemap[hsh])
            for name, hsh in pycompat.iteritems(repo._bookmarks)
            if hsh in self.replacemap
        ]
        changes = []
        for name, hsh in needupdate:
            if hsh:
                changes.append((name, hsh))
                if self.ui.verbose:
                    self.ui.write(
                        _(b'moving bookmark %s to %s\n') % (name, hex(hsh))
                    )
            else:
                changes.append((name, None))
                if self.ui.verbose:
                    self.ui.write(_(b'deleting bookmark %s\n') % name)
        repo._bookmarks.applychanges(repo, tr, changes)
###END###
def _moveworkingdirectoryparent(self):
        if not self.finalnode:
            # Find the latest not-{obsoleted,stripped} parent.
            revs = self.repo.revs(b'max(::. - %ln)', self.replacemap.keys())
            ctx = self.repo[revs.first()]
            self.finalnode = ctx.node()
        else:
            ctx = self.repo[self.finalnode]

        dirstate = self.repo.dirstate
        # dirstate.rebuild invalidates fsmonitorstate, causing "hg status" to
        # be slow. in absorb's case, no need to invalidate fsmonitorstate.
        noop = lambda: 0
        restore = noop
        if util.safehasattr(dirstate, '_fsmonitorstate'):
            bak = dirstate._fsmonitorstate.invalidate

            def restore():
                dirstate._fsmonitorstate.invalidate = bak

            dirstate._fsmonitorstate.invalidate = noop
        try:
            with dirstate.parentchange():
                dirstate.rebuild(ctx.node(), ctx.manifest(), self.paths)
        finally:
            restore()
###END###
def _willbecomenoop(memworkingcopy, ctx, pctx=None):
        """({path: content}, ctx, ctx) -> bool. test if a commit will be noop

        if it will become an empty commit (does not change anything, after the
        memworkingcopy overrides), return True. otherwise return False.
        """
        if not pctx:
            parents = ctx.parents()
            if len(parents) != 1:
                return False
            pctx = parents[0]
        if ctx.branch() != pctx.branch():
            return False
        if ctx.extra().get(b'close'):
            return False
        # ctx changes more files (not a subset of memworkingcopy)
        if not set(ctx.files()).issubset(set(memworkingcopy)):
            return False
        for path, content in pycompat.iteritems(memworkingcopy):
            if path not in pctx or path not in ctx:
                return False
            fctx = ctx[path]
            pfctx = pctx[path]
            if pfctx.flags() != fctx.flags():
                return False
            if pfctx.data() != content:
                return False
        return True
###END###
def _commitsingle(self, memworkingcopy, ctx, p1=None):
        """(ctx, {path: content}, node) -> node. make a single commit

        the commit is a clone from ctx, with a (optionally) different p1, and
        different file contents replaced by memworkingcopy.
        """
        parents = p1 and (p1, self.repo.nullid)
        extra = ctx.extra()
        if self._useobsolete and self.ui.configbool(b'absorb', b'add-noise'):
            extra[b'absorb_source'] = ctx.hex()

        desc = rewriteutil.update_hash_refs(
            ctx.repo(),
            ctx.description(),
            {
                oldnode: [newnode]
                for oldnode, newnode in self.replacemap.items()
            },
        )
        mctx = overlaycontext(
            memworkingcopy, ctx, parents, extra=extra, desc=desc
        )
        return mctx.commit()
###END###
def _useobsolete(self):
        """() -> bool"""
        return obsolete.isenabled(self.repo, obsolete.createmarkersopt)
###END###
def _cleanupoldcommits(self):
        replacements = {
            k: ([v] if v is not None else [])
            for k, v in pycompat.iteritems(self.replacemap)
        }
        if replacements:
            scmutil.cleanupnodes(
                self.repo, replacements, operation=b'absorb', fixphase=True
            )
###END###
def skip_empty_successor(self):
        return rewriteutil.skip_empty_successor(self.ui, b'absorb')
###END###
def __init__(self, repo, ui, inmemory=False, dryrun=False, opts=None):
        if opts is None:
            opts = {}

        # prepared: whether we have rebasestate prepared or not. Currently it
        # decides whether "self.repo" is unfiltered or not.
        # The rebasestate has explicit hash to hash instructions not depending
        # on visibility. If rebasestate exists (in-memory or on-disk), use
        # unfiltered repo to avoid visibility issues.
        # Before knowing rebasestate (i.e. when starting a new rebase (not
        # --continue or --abort)), the original repo should be used so
        # visibility-dependent revsets are correct.
        self.prepared = False
        self.resume = False
        self._repo = repo

        self.ui = ui
        self.opts = opts
        self.originalwd = None
        self.external = nullrev
        # Mapping between the old revision id and either what is the new rebased
        # revision or what needs to be done with the old revision. The state
        # dict will be what contains most of the rebase progress state.
        self.state = {}
        self.activebookmark = None
        self.destmap = {}
        self.skipped = set()

        self.collapsef = opts.get('collapse', False)
        self.collapsemsg = cmdutil.logmessage(ui, pycompat.byteskwargs(opts))
        self.date = opts.get('date', None)

        e = opts.get('extrafn')  # internal, used by e.g. hgsubversion
        self.extrafns = [_savegraft]
        if e:
            self.extrafns = [e]

        self.backupf = ui.configbool(b'rewrite', b'backup-bundle')
        self.keepf = opts.get('keep', False)
        self.keepbranchesf = opts.get('keepbranches', False)
        self.skipemptysuccessorf = rewriteutil.skip_empty_successor(
            repo.ui, b'rebase'
        )
        self.obsolete_with_successor_in_destination = {}
        self.obsolete_with_successor_in_rebase_set = set()
        self.inmemory = inmemory
        self.dryrun = dryrun
        self.stateobj = statemod.cmdstate(repo, b'rebasestate')
###END###
def repo(self):
        if self.prepared:
            return self._repo.unfiltered()
        else:
            return self._repo
###END###
def storestatus(self, tr=None):
        """Store the current status to allow recovery"""
        if tr:
            tr.addfilegenerator(
                b'rebasestate',
                (b'rebasestate',),
                self._writestatus,
                location=b'plain',
            )
        else:
            with self.repo.vfs(b"rebasestate", b"w") as f:
                self._writestatus(f)
###END###
def _writestatus(self, f):
        repo = self.repo
        assert repo.filtername is None
        f.write(repo[self.originalwd].hex() + b'\n')
        # was "dest". we now write dest per src root below.
        f.write(b'\n')
        f.write(repo[self.external].hex() + b'\n')
        f.write(b'%d\n' % int(self.collapsef))
        f.write(b'%d\n' % int(self.keepf))
        f.write(b'%d\n' % int(self.keepbranchesf))
        f.write(b'%s\n' % (self.activebookmark or b''))
        destmap = self.destmap
        for d, v in pycompat.iteritems(self.state):
            oldrev = repo[d].hex()
            if v >= 0:
                newrev = repo[v].hex()
            else:
                newrev = b"%d" % v
            destnode = repo[destmap[d]].hex()
            f.write(b"%s:%s:%s\n" % (oldrev, newrev, destnode))
        repo.ui.debug(b'rebase status stored\n')
###END###
def restorestatus(self):
        """Restore a previously stored status"""
        if not self.stateobj.exists():
            cmdutil.wrongtooltocontinue(self.repo, _(b'rebase'))

        data = self._read()
        self.repo.ui.debug(b'rebase status resumed\n')

        self.originalwd = data[b'originalwd']
        self.destmap = data[b'destmap']
        self.state = data[b'state']
        self.skipped = data[b'skipped']
        self.collapsef = data[b'collapse']
        self.keepf = data[b'keep']
        self.keepbranchesf = data[b'keepbranches']
        self.external = data[b'external']
        self.activebookmark = data[b'activebookmark']
###END###
def _read(self):
        self.prepared = True
        repo = self.repo
        assert repo.filtername is None
        data = {
            b'keepbranches': None,
            b'collapse': None,
            b'activebookmark': None,
            b'external': nullrev,
            b'keep': None,
            b'originalwd': None,
        }
        legacydest = None
        state = {}
        destmap = {}

        if True:
            f = repo.vfs(b"rebasestate")
            for i, l in enumerate(f.read().splitlines()):
                if i == 0:
                    data[b'originalwd'] = repo[l].rev()
                elif i == 1:
                    # this line should be empty in newer version. but legacy
                    # clients may still use it
                    if l:
                        legacydest = repo[l].rev()
                elif i == 2:
                    data[b'external'] = repo[l].rev()
                elif i == 3:
                    data[b'collapse'] = bool(int(l))
                elif i == 4:
                    data[b'keep'] = bool(int(l))
                elif i == 5:
                    data[b'keepbranches'] = bool(int(l))
                elif i == 6 and not (len(l) == 81 and b':' in l):
                    # line 6 is a recent addition, so for backwards
                    # compatibility check that the line doesn't look like the
                    # oldrev:newrev lines
                    data[b'activebookmark'] = l
                else:
                    args = l.split(b':')
                    oldrev = repo[args[0]].rev()
                    newrev = args[1]
                    if newrev in legacystates:
                        continue
                    if len(args) > 2:
                        destrev = repo[args[2]].rev()
                    else:
                        destrev = legacydest
                    destmap[oldrev] = destrev
                    if newrev == revtodostr:
                        state[oldrev] = revtodo
                        # Legacy compat special case
                    else:
                        state[oldrev] = repo[newrev].rev()

        if data[b'keepbranches'] is None:
            raise error.Abort(_(b'.hg/rebasestate is incomplete'))

        data[b'destmap'] = destmap
        data[b'state'] = state
        skipped = set()
        # recompute the set of skipped revs
        if not data[b'collapse']:
            seen = set(destmap.values())
            for old, new in sorted(state.items()):
                if new != revtodo and new in seen:
                    skipped.add(old)
                seen.add(new)
        data[b'skipped'] = skipped
        repo.ui.debug(
            b'computed skipped revs: %s\n'
            % (b' '.join(b'%d' % r for r in sorted(skipped)) or b'')
        )

        return data
###END###
def _handleskippingobsolete(self):
        """Compute structures necessary for skipping obsolete revisions"""
        if self.keepf:
            return
        if not self.ui.configbool(b'experimental', b'rebaseskipobsolete'):
            return
        obsoleteset = {r for r in self.state if self.repo[r].obsolete()}
        (
            self.obsolete_with_successor_in_destination,
            self.obsolete_with_successor_in_rebase_set,
        ) = _compute_obsolete_sets(self.repo, obsoleteset, self.destmap)
        skippedset = set(self.obsolete_with_successor_in_destination)
        skippedset.update(self.obsolete_with_successor_in_rebase_set)
        _checkobsrebase(self.repo, self.ui, obsoleteset, skippedset)
        if obsolete.isenabled(self.repo, obsolete.allowdivergenceopt):
            self.obsolete_with_successor_in_rebase_set = set()
        else:
            for rev in self.repo.revs(
                b'descendants(%ld) and not %ld',
                self.obsolete_with_successor_in_rebase_set,
                self.obsolete_with_successor_in_rebase_set,
            ):
                self.state.pop(rev, None)
                self.destmap.pop(rev, None)
###END###
def _prepareabortorcontinue(
        self, isabort, backup=True, suppwarns=False, dryrun=False, confirm=False
    ):
        self.resume = True
        try:
            self.restorestatus()
            # Calculate self.obsolete_* sets
            self._handleskippingobsolete()
            self.collapsemsg = restorecollapsemsg(self.repo, isabort)
        except error.RepoLookupError:
            if isabort:
                clearstatus(self.repo)
                clearcollapsemsg(self.repo)
                self.repo.ui.warn(
                    _(
                        b'rebase aborted (no revision is removed,'
                        b' only broken state is cleared)\n'
                    )
                )
                return 0
            else:
                msg = _(b'cannot continue inconsistent rebase')
                hint = _(b'use "hg rebase --abort" to clear broken state')
                raise error.Abort(msg, hint=hint)

        if isabort:
            backup = backup and self.backupf
            return self._abort(
                backup=backup,
                suppwarns=suppwarns,
                dryrun=dryrun,
                confirm=confirm,
            )
###END###
def _preparenewrebase(self, destmap):
        if not destmap:
            return _nothingtorebase()

        result = buildstate(self.repo, destmap, self.collapsef)

        if not result:
            # Empty state built, nothing to rebase
            self.ui.status(_(b'nothing to rebase\n'))
            return _nothingtorebase()

        (self.originalwd, self.destmap, self.state) = result
        if self.collapsef:
            dests = set(self.destmap.values())
            if len(dests) != 1:
                raise error.InputError(
                    _(b'--collapse does not work with multiple destinations')
                )
            destrev = next(iter(dests))
            destancestors = self.repo.changelog.ancestors(
                [destrev], inclusive=True
            )
            self.external = externalparent(self.repo, self.state, destancestors)

        for destrev in sorted(set(destmap.values())):
            dest = self.repo[destrev]
            if dest.closesbranch() and not self.keepbranchesf:
                self.ui.status(_(b'reopening closed branch head %s\n') % dest)

        # Calculate self.obsolete_* sets
        self._handleskippingobsolete()

        if not self.keepf:
            rebaseset = set(destmap.keys())
            rebaseset -= set(self.obsolete_with_successor_in_destination)
            rebaseset -= self.obsolete_with_successor_in_rebase_set
            # We have our own divergence-checking in the rebase extension
            overrides = {}
            if obsolete.isenabled(self.repo, obsolete.createmarkersopt):
                overrides = {
                    (b'experimental', b'evolution.allowdivergence'): b'true'
                }
            try:
                with self.ui.configoverride(overrides):
                    rewriteutil.precheck(self.repo, rebaseset, action=b'rebase')
            except error.Abort as e:
                if e.hint is None:
                    e.hint = _(b'use --keep to keep original changesets')
                raise e

        self.prepared = True
###END###
def _assignworkingcopy(self):
        if self.inmemory:
            from mercurial.context import overlayworkingctx

            self.wctx = overlayworkingctx(self.repo)
            self.repo.ui.debug(b"rebasing in memory\n")
        else:
            self.wctx = self.repo[None]
            self.repo.ui.debug(b"rebasing on disk\n")
        self.repo.ui.log(
            b"rebase",
            b"using in-memory rebase: %r\n",
            self.inmemory,
            rebase_imm_used=self.inmemory,
        )
###END###
def _performrebase(self, tr):
        self._assignworkingcopy()
        repo, ui = self.repo, self.ui
        if self.keepbranchesf:
            # insert _savebranch at the start of extrafns so if
            # there's a user-provided extrafn it can clobber branch if
            # desired
            self.extrafns.insert(0, _savebranch)
            if self.collapsef:
                branches = set()
                for rev in self.state:
                    branches.add(repo[rev].branch())
                    if len(branches) > 1:
                        raise error.InputError(
                            _(b'cannot collapse multiple named branches')
                        )

        # Keep track of the active bookmarks in order to reset them later
        self.activebookmark = self.activebookmark or repo._activebookmark
        if self.activebookmark:
            bookmarks.deactivate(repo)

        # Store the state before we begin so users can run 'hg rebase --abort'
        # if we fail before the transaction closes.
        self.storestatus()
        if tr:
            # When using single transaction, store state when transaction
            # commits.
            self.storestatus(tr)

        cands = [k for k, v in pycompat.iteritems(self.state) if v == revtodo]
        p = repo.ui.makeprogress(
            _(b"rebasing"), unit=_(b'changesets'), total=len(cands)
        )

        def progress(ctx):
            p.increment(item=(b"%d:%s" % (ctx.rev(), ctx)))

        for subset in sortsource(self.destmap):
            sortedrevs = self.repo.revs(b'sort(%ld, -topo)', subset)
            for rev in sortedrevs:
                self._rebasenode(tr, rev, progress)
        p.complete()
        ui.note(_(b'rebase merging completed\n'))
###END###
def _concludenode(self, rev, editor, commitmsg=None):
        """Commit the wd changes with parents p1 and p2.

        Reuse commit info from rev but also store useful information in extra.
        Return node of committed revision."""
        repo = self.repo
        ctx = repo[rev]
        if commitmsg is None:
            commitmsg = ctx.description()

        # Skip replacement if collapsing, as that degenerates to p1 for all
        # nodes.
        if not self.collapsef:
            cl = repo.changelog
            commitmsg = rewriteutil.update_hash_refs(
                repo,
                commitmsg,
                {
                    cl.node(oldrev): [cl.node(newrev)]
                    for oldrev, newrev in self.state.items()
                    if newrev != revtodo
                },
            )

        date = self.date
        if date is None:
            date = ctx.date()
        extra = {b'rebase_source': ctx.hex()}
        for c in self.extrafns:
            c(ctx, extra)
        destphase = max(ctx.phase(), phases.draft)
        overrides = {
            (b'phases', b'new-commit'): destphase,
            (b'ui', b'allowemptycommit'): not self.skipemptysuccessorf,
        }
        with repo.ui.configoverride(overrides, b'rebase'):
            if self.inmemory:
                newnode = commitmemorynode(
                    repo,
                    wctx=self.wctx,
                    extra=extra,
                    commitmsg=commitmsg,
                    editor=editor,
                    user=ctx.user(),
                    date=date,
                )
            else:
                newnode = commitnode(
                    repo,
                    extra=extra,
                    commitmsg=commitmsg,
                    editor=editor,
                    user=ctx.user(),
                    date=date,
                )

            return newnode
###END###
def _rebasenode(self, tr, rev, progressfn):
        repo, ui, opts = self.repo, self.ui, self.opts
        ctx = repo[rev]
        desc = _ctxdesc(ctx)
        if self.state[rev] == rev:
            ui.status(_(b'already rebased %s\n') % desc)
        elif rev in self.obsolete_with_successor_in_rebase_set:
            msg = (
                _(
                    b'note: not rebasing %s and its descendants as '
                    b'this would cause divergence\n'
                )
                % desc
            )
            repo.ui.status(msg)
            self.skipped.add(rev)
        elif rev in self.obsolete_with_successor_in_destination:
            succ = self.obsolete_with_successor_in_destination[rev]
            if succ is None:
                msg = _(b'note: not rebasing %s, it has no successor\n') % desc
            else:
                succdesc = _ctxdesc(repo[succ])
                msg = _(
                    b'note: not rebasing %s, already in destination as %s\n'
                ) % (desc, succdesc)
            repo.ui.status(msg)
            # Make clearrebased aware state[rev] is not a true successor
            self.skipped.add(rev)
            # Record rev as moved to its desired destination in self.state.
            # This helps bookmark and working parent movement.
            dest = max(
                adjustdest(repo, rev, self.destmap, self.state, self.skipped)
            )
            self.state[rev] = dest
        elif self.state[rev] == revtodo:
            ui.status(_(b'rebasing %s\n') % desc)
            progressfn(ctx)
            p1, p2, base = defineparents(
                repo,
                rev,
                self.destmap,
                self.state,
                self.skipped,
                self.obsolete_with_successor_in_destination,
            )
            if self.resume and self.wctx.p1().rev() == p1:
                repo.ui.debug(b'resuming interrupted rebase\n')
                self.resume = False
            else:
                overrides = {(b'ui', b'forcemerge'): opts.get('tool', b'')}
                with ui.configoverride(overrides, b'rebase'):
                    try:
                        rebasenode(
                            repo,
                            rev,
                            p1,
                            p2,
                            base,
                            self.collapsef,
                            wctx=self.wctx,
                        )
                    except error.InMemoryMergeConflictsError:
                        if self.dryrun:
                            raise error.ConflictResolutionRequired(b'rebase')
                        if self.collapsef:
                            # TODO: Make the overlayworkingctx reflected
                            # in the working copy here instead of re-raising
                            # so the entire rebase operation is retried.
                            raise
                        ui.status(
                            _(
                                b"hit merge conflicts; rebasing that "
                                b"commit again in the working copy\n"
                            )
                        )
                        try:
                            cmdutil.bailifchanged(repo)
                        except error.Abort:
                            clearstatus(repo)
                            clearcollapsemsg(repo)
                            raise
                        self.inmemory = False
                        self._assignworkingcopy()
                        mergemod.update(repo[p1], wc=self.wctx)
                        rebasenode(
                            repo,
                            rev,
                            p1,
                            p2,
                            base,
                            self.collapsef,
                            wctx=self.wctx,
                        )
            if not self.collapsef:
                merging = p2 != nullrev
                editform = cmdutil.mergeeditform(merging, b'rebase')
                editor = cmdutil.getcommiteditor(editform=editform, **opts)
                # We need to set parents again here just in case we're continuing
                # a rebase started with an old hg version (before 9c9cfecd4600),
                # because those old versions would have left us with two dirstate
                # parents, and we don't want to create a merge commit here (unless
                # we're rebasing a merge commit).
                self.wctx.setparents(repo[p1].node(), repo[p2].node())
                newnode = self._concludenode(rev, editor)
            else:
                # Skip commit if we are collapsing
                newnode = None
            # Update the state
            if newnode is not None:
                self.state[rev] = repo[newnode].rev()
                ui.debug(b'rebased as %s\n' % short(newnode))
                if repo[newnode].isempty():
                    ui.warn(
                        _(
                            b'note: created empty successor for %s, its '
                            b'destination already has all its changes\n'
                        )
                        % desc
                    )
            else:
                if not self.collapsef:
                    ui.warn(
                        _(
                            b'note: not rebasing %s, its destination already '
                            b'has all its changes\n'
                        )
                        % desc
                    )
                    self.skipped.add(rev)
                self.state[rev] = p1
                ui.debug(b'next revision set to %d\n' % p1)
        else:
            ui.status(
                _(b'already rebased %s as %s\n') % (desc, repo[self.state[rev]])
            )
        if not tr:
            # When not using single transaction, store state after each
            # commit is completely done. On InterventionRequired, we thus
            # won't store the status. Instead, we'll hit the "len(parents) == 2"
            # case and realize that the commit was in progress.
            self.storestatus()
###END###
def _finishrebase(self):
        repo, ui, opts = self.repo, self.ui, self.opts
        fm = ui.formatter(b'rebase', pycompat.byteskwargs(opts))
        fm.startitem()
        if self.collapsef:
            p1, p2, _base = defineparents(
                repo,
                min(self.state),
                self.destmap,
                self.state,
                self.skipped,
                self.obsolete_with_successor_in_destination,
            )
            editopt = opts.get('edit')
            editform = b'rebase.collapse'
            if self.collapsemsg:
                commitmsg = self.collapsemsg
            else:
                commitmsg = b'Collapsed revision'
                for rebased in sorted(self.state):
                    if rebased not in self.skipped:
                        commitmsg += b'\n* %s' % repo[rebased].description()
                editopt = True
            editor = cmdutil.getcommiteditor(edit=editopt, editform=editform)
            revtoreuse = max(self.state)

            self.wctx.setparents(repo[p1].node(), repo[self.external].node())
            newnode = self._concludenode(
                revtoreuse, editor, commitmsg=commitmsg
            )

            if newnode is not None:
                newrev = repo[newnode].rev()
                for oldrev in self.state:
                    self.state[oldrev] = newrev

        if b'qtip' in repo.tags():
            updatemq(repo, self.state, self.skipped, **opts)

        # restore original working directory
        # (we do this before stripping)
        newwd = self.state.get(self.originalwd, self.originalwd)
        if newwd < 0:
            # original directory is a parent of rebase set root or ignored
            newwd = self.originalwd
        if newwd not in [c.rev() for c in repo[None].parents()]:
            ui.note(_(b"update back to initial working directory parent\n"))
            mergemod.update(repo[newwd])

        collapsedas = None
        if self.collapsef and not self.keepf:
            collapsedas = newnode
        clearrebased(
            ui,
            repo,
            self.destmap,
            self.state,
            self.skipped,
            collapsedas,
            self.keepf,
            fm=fm,
            backup=self.backupf,
        )

        clearstatus(repo)
        clearcollapsemsg(repo)

        ui.note(_(b"rebase completed\n"))
        util.unlinkpath(repo.sjoin(b'undo'), ignoremissing=True)
        if self.skipped:
            skippedlen = len(self.skipped)
            ui.note(_(b"%d revisions have been skipped\n") % skippedlen)
        fm.end()

        if (
            self.activebookmark
            and self.activebookmark in repo._bookmarks
            and repo[b'.'].node() == repo._bookmarks[self.activebookmark]
        ):
            bookmarks.activate(repo, self.activebookmark)
###END###
def _abort(self, backup=True, suppwarns=False, dryrun=False, confirm=False):
        '''Restore the repository to its original state.'''

        repo = self.repo
        try:
            # If the first commits in the rebased set get skipped during the
            # rebase, their values within the state mapping will be the dest
            # rev id. The rebased list must must not contain the dest rev
            # (issue4896)
            rebased = [
                s
                for r, s in self.state.items()
                if s >= 0 and s != r and s != self.destmap[r]
            ]
            immutable = [d for d in rebased if not repo[d].mutable()]
            cleanup = True
            if immutable:
                repo.ui.warn(
                    _(b"warning: can't clean up public changesets %s\n")
                    % b', '.join(bytes(repo[r]) for r in immutable),
                    hint=_(b"see 'hg help phases' for details"),
                )
                cleanup = False

            descendants = set()
            if rebased:
                descendants = set(repo.changelog.descendants(rebased))
            if descendants - set(rebased):
                repo.ui.warn(
                    _(
                        b"warning: new changesets detected on "
                        b"destination branch, can't strip\n"
                    )
                )
                cleanup = False

            if cleanup:
                if rebased:
                    strippoints = [
                        c.node() for c in repo.set(b'roots(%ld)', rebased)
                    ]

                updateifonnodes = set(rebased)
                updateifonnodes.update(self.destmap.values())

                if not dryrun and not confirm:
                    updateifonnodes.add(self.originalwd)

                shouldupdate = repo[b'.'].rev() in updateifonnodes

                # Update away from the rebase if necessary
                if shouldupdate:
                    mergemod.clean_update(repo[self.originalwd])

                # Strip from the first rebased revision
                if rebased:
                    repair.strip(repo.ui, repo, strippoints, backup=backup)

            if self.activebookmark and self.activebookmark in repo._bookmarks:
                bookmarks.activate(repo, self.activebookmark)

        finally:
            clearstatus(repo)
            clearcollapsemsg(repo)
            if not suppwarns:
                repo.ui.warn(_(b'rebase aborted\n'))
        return 0
###END###
def __init__(self, **opts):
        opts = pycompat.byteskwargs(opts)
        for k, v in pycompat.iteritems(self.defaults):
            setattr(self, k, opts.get(k, v))
###END###
def shortstr(self):
        """represent opts in a short string, suitable for a directory name"""
        result = b''
        if not self.followrename:
            result += b'r0'
        if not self.followmerge:
            result += b'm0'
        if self.diffopts is not None:
            assert isinstance(self.diffopts, mdiff.diffopts)
            diffopthash = hashdiffopts(self.diffopts)
            if diffopthash != _defaultdiffopthash:
                result += b'i' + diffopthash
        return result or b'default'
###END###
def __init__(self, repo, path, linelogpath, revmappath, opts):
        self.repo = repo
        self.ui = repo.ui
        self.path = path
        self.opts = opts
        self.linelogpath = linelogpath
        self.revmappath = revmappath
        self._linelog = None
        self._revmap = None
        self._node2path = {}
###END###
def linelog(self):
        if self._linelog is None:
            if os.path.exists(self.linelogpath):
                with open(self.linelogpath, b'rb') as f:
                    try:
                        self._linelog = linelogmod.linelog.fromdata(f.read())
                    except linelogmod.LineLogError:
                        self._linelog = linelogmod.linelog()
            else:
                self._linelog = linelogmod.linelog()
        return self._linelog
###END###
def revmap(self):
        if self._revmap is None:
            self._revmap = revmapmod.revmap(self.revmappath)
        return self._revmap
###END###
def close(self):
        if self._revmap is not None:
            self._revmap.flush()
            self._revmap = None
        if self._linelog is not None:
            with open(self.linelogpath, b'wb') as f:
                f.write(self._linelog.encode())
            self._linelog = None
###END###
def rebuild(self):
        """delete linelog and revmap, useful for rebuilding"""
        self.close()
        self._node2path.clear()
        _unlinkpaths([self.revmappath, self.linelogpath])
###END###
def lastnode(self):
        """return last node in revmap, or None if revmap is empty"""
        if self._revmap is None:
            # fast path, read revmap without loading its full content
            return revmapmod.getlastnode(self.revmappath)
        else:
            return self._revmap.rev2hsh(self._revmap.maxrev)
###END###
def isuptodate(self, master, strict=True):
        """return True if the revmap / linelog is up-to-date, or the file
        does not exist in the master revision. False otherwise.

        it tries to be fast and could return false negatives, because of the
        use of linkrev instead of introrev.

        useful for both server and client to decide whether to update
        fastannotate cache or not.

        if strict is True, even if fctx exists in the revmap, but is not the
        last node, isuptodate will return False. it's good for performance - no
        expensive check was done.

        if strict is False, if fctx exists in the revmap, this function may
        return True. this is useful for the client to skip downloading the
        cache if the client's master is behind the server's.
        """
        lastnode = self.lastnode
        try:
            f = self._resolvefctx(master, resolverev=True)
            # choose linkrev instead of introrev as the check is meant to be
            # *fast*.
            linknode = self.repo.changelog.node(f.linkrev())
            if not strict and lastnode and linknode != lastnode:
                # check if f.node() is in the revmap. note: this loads the
                # revmap and can be slow.
                return self.revmap.hsh2rev(linknode) is not None
            # avoid resolving old manifest, or slow adjustlinkrev to be fast,
            # false negatives are acceptable in this case.
            return linknode == lastnode
        except LookupError:
            # master does not have the file, or the revmap is ahead
            return True
###END###
def annotate(self, rev, master=None, showpath=False, showlines=False):
        """incrementally update the cache so it includes revisions in the main
        branch till 'master'. and run annotate on 'rev', which may or may not be
        included in the main branch.

        if master is None, do not update linelog.

        the first value returned is the annotate result, it is [(node, linenum)]
        by default. [(node, linenum, path)] if showpath is True.

        if showlines is True, a second value will be returned, it is a list of
        corresponding line contents.
        """

        # the fast path test requires commit hash, convert rev number to hash,
        # so it may hit the fast path. note: in the "fctx" mode, the "annotate"
        # command could give us a revision number even if the user passes a
        # commit hash.
        if isinstance(rev, int):
            rev = hex(self.repo.changelog.node(rev))

        # fast path: if rev is in the main branch already
        directly, revfctx = self.canannotatedirectly(rev)
        if directly:
            if self.ui.debugflag:
                self.ui.debug(
                    b'fastannotate: %s: using fast path '
                    b'(resolved fctx: %s)\n'
                    % (
                        self.path,
                        stringutil.pprint(util.safehasattr(revfctx, b'node')),
                    )
                )
            return self.annotatedirectly(revfctx, showpath, showlines)

        # resolve master
        masterfctx = None
        if master:
            try:
                masterfctx = self._resolvefctx(
                    master, resolverev=True, adjustctx=True
                )
            except LookupError:  # master does not have the file
                pass
            else:
                if masterfctx in self.revmap:  # no need to update linelog
                    masterfctx = None

        #                  ... - @ <- rev (can be an arbitrary changeset,
        #                 /                not necessarily a descendant
        #      master -> o                 of master)
        #                |
        #     a merge -> o         'o': new changesets in the main branch
        #                |\        '#': revisions in the main branch that
        #                o *            exist in linelog / revmap
        #                | .       '*': changesets in side branches, or
        # last master -> # .            descendants of master
        #                | .
        #                # *       joint: '#', and is a parent of a '*'
        #                |/
        #     a joint -> # ^^^^ --- side branches
        #                |
        #                ^ --- main branch (in linelog)

        # these DFSes are similar to the traditional annotate algorithm.
        # we cannot really reuse the code for perf reason.

        # 1st DFS calculates merges, joint points, and needed.
        # "needed" is a simple reference counting dict to free items in
        # "hist", reducing its memory usage otherwise could be huge.
        initvisit = [revfctx]
        if masterfctx:
            if masterfctx.rev() is None:
                raise error.Abort(
                    _(b'cannot update linelog to wdir()'),
                    hint=_(b'set fastannotate.mainbranch'),
                )
            initvisit.append(masterfctx)
        visit = initvisit[:]
        pcache = {}
        needed = {revfctx: 1}
        hist = {}  # {fctx: ([(llrev or fctx, linenum)], text)}
        while visit:
            f = visit.pop()
            if f in pcache or f in hist:
                continue
            if f in self.revmap:  # in the old main branch, it's a joint
                llrev = self.revmap.hsh2rev(f.node())
                self.linelog.annotate(llrev)
                result = self.linelog.annotateresult
                hist[f] = (result, f.data())
                continue
            pl = self._parentfunc(f)
            pcache[f] = pl
            for p in pl:
                needed[p] = needed.get(p, 0) + 1
                if p not in pcache:
                    visit.append(p)

        # 2nd (simple) DFS calculates new changesets in the main branch
        # ('o' nodes in # the above graph), so we know when to update linelog.
        newmainbranch = set()
        f = masterfctx
        while f and f not in self.revmap:
            newmainbranch.add(f)
            pl = pcache[f]
            if pl:
                f = pl[0]
            else:
                f = None
                break

        # f, if present, is the position where the last build stopped at, and
        # should be the "master" last time. check to see if we can continue
        # building the linelog incrementally. (we cannot if diverged)
        if masterfctx is not None:
            self._checklastmasterhead(f)

        if self.ui.debugflag:
            if newmainbranch:
                self.ui.debug(
                    b'fastannotate: %s: %d new changesets in the main'
                    b' branch\n' % (self.path, len(newmainbranch))
                )
            elif not hist:  # no joints, no updates
                self.ui.debug(
                    b'fastannotate: %s: linelog cannot help in '
                    b'annotating this revision\n' % self.path
                )

        # prepare annotateresult so we can update linelog incrementally
        self.linelog.annotate(self.linelog.maxrev)

        # 3rd DFS does the actual annotate
        visit = initvisit[:]
        progress = self.ui.makeprogress(
            b'building cache', total=len(newmainbranch)
        )
        while visit:
            f = visit[-1]
            if f in hist:
                visit.pop()
                continue

            ready = True
            pl = pcache[f]
            for p in pl:
                if p not in hist:
                    ready = False
                    visit.append(p)
            if not ready:
                continue

            visit.pop()
            blocks = None  # mdiff blocks, used for appending linelog
            ismainbranch = f in newmainbranch
            # curr is the same as the traditional annotate algorithm,
            # if we only care about linear history (do not follow merge),
            # then curr is not actually used.
            assert f not in hist
            curr = _decorate(f)
            for i, p in enumerate(pl):
                bs = list(self._diffblocks(hist[p][1], curr[1]))
                if i == 0 and ismainbranch:
                    blocks = bs
                curr = _pair(hist[p], curr, bs)
                if needed[p] == 1:
                    del hist[p]
                    del needed[p]
                else:
                    needed[p] -= 1

            hist[f] = curr
            del pcache[f]

            if ismainbranch:  # need to write to linelog
                progress.increment()
                bannotated = None
                if len(pl) == 2 and self.opts.followmerge:  # merge
                    bannotated = curr[0]
                if blocks is None:  # no parents, add an empty one
                    blocks = list(self._diffblocks(b'', curr[1]))
                self._appendrev(f, blocks, bannotated)
            elif showpath:  # not append linelog, but we need to record path
                self._node2path[f.node()] = f.path()

        progress.complete()

        result = [
            ((self.revmap.rev2hsh(fr) if isinstance(fr, int) else fr.node()), l)
            for fr, l in hist[revfctx][0]
        ]  # [(node, linenumber)]
        return self._refineannotateresult(result, revfctx, showpath, showlines)
###END###
def canannotatedirectly(self, rev):
        """(str) -> bool, fctx or node.
        return (True, f) if we can annotate without updating the linelog, pass
        f to annotatedirectly.
        return (False, f) if we need extra calculation. f is the fctx resolved
        from rev.
        """
        result = True
        f = None
        if not isinstance(rev, int) and rev is not None:
            hsh = {20: bytes, 40: bin}.get(len(rev), lambda x: None)(rev)
            if hsh is not None and (hsh, self.path) in self.revmap:
                f = hsh
        if f is None:
            adjustctx = b'linkrev' if self._perfhack else True
            f = self._resolvefctx(rev, adjustctx=adjustctx, resolverev=True)
            result = f in self.revmap
            if not result and self._perfhack:
                # redo the resolution without perfhack - as we are going to
                # do write operations, we need a correct fctx.
                f = self._resolvefctx(rev, adjustctx=True, resolverev=True)
        return result, f
###END###
def annotatealllines(self, rev, showpath=False, showlines=False):
        """(rev : str) -> [(node : str, linenum : int, path : str)]

        the result has the same format with annotate, but include all (including
        deleted) lines up to rev. call this after calling annotate(rev, ...) for
        better performance and accuracy.
        """
        revfctx = self._resolvefctx(rev, resolverev=True, adjustctx=True)

        # find a chain from rev to anything in the mainbranch
        if revfctx not in self.revmap:
            chain = [revfctx]
            a = b''
            while True:
                f = chain[-1]
                pl = self._parentfunc(f)
                if not pl:
                    break
                if pl[0] in self.revmap:
                    a = pl[0].data()
                    break
                chain.append(pl[0])

            # both self.linelog and self.revmap is backed by filesystem. now
            # we want to modify them but do not want to write changes back to
            # files. so we create in-memory objects and copy them. it's like
            # a "fork".
            linelog = linelogmod.linelog()
            linelog.copyfrom(self.linelog)
            linelog.annotate(linelog.maxrev)
            revmap = revmapmod.revmap()
            revmap.copyfrom(self.revmap)

            for f in reversed(chain):
                b = f.data()
                blocks = list(self._diffblocks(a, b))
                self._doappendrev(linelog, revmap, f, blocks)
                a = b
        else:
            # fastpath: use existing linelog, revmap as we don't write to them
            linelog = self.linelog
            revmap = self.revmap

        lines = linelog.getalllines()
        hsh = revfctx.node()
        llrev = revmap.hsh2rev(hsh)
        result = [(revmap.rev2hsh(r), l) for r, l in lines if r <= llrev]
        # cannot use _refineannotateresult since we need custom logic for
        # resolving line contents
        if showpath:
            result = self._addpathtoresult(result, revmap)
        if showlines:
            linecontents = self._resolvelines(result, revmap, linelog)
            result = (result, linecontents)
        return result
###END###
def _resolvelines(self, annotateresult, revmap, linelog):
        """(annotateresult) -> [line]. designed for annotatealllines.
        this is probably the most inefficient code in the whole fastannotate
        directory. but we have made a decision that the linelog does not
        store line contents. so getting them requires random accesses to
        the revlog data, since they can be many, it can be very slow.
        """
        # [llrev]
        revs = [revmap.hsh2rev(l[0]) for l in annotateresult]
        result = [None] * len(annotateresult)
        # {(rev, linenum): [lineindex]}
        key2idxs = collections.defaultdict(list)
        for i in pycompat.xrange(len(result)):
            key2idxs[(revs[i], annotateresult[i][1])].append(i)
        while key2idxs:
            # find an unresolved line and its linelog rev to annotate
            hsh = None
            try:
                for (rev, _linenum), idxs in pycompat.iteritems(key2idxs):
                    if revmap.rev2flag(rev) & revmapmod.sidebranchflag:
                        continue
                    hsh = annotateresult[idxs[0]][0]
                    break
            except StopIteration:  # no more unresolved lines
                return result
            if hsh is None:
                # the remaining key2idxs are not in main branch, resolving them
                # using the hard way...
                revlines = {}
                for (rev, linenum), idxs in pycompat.iteritems(key2idxs):
                    if rev not in revlines:
                        hsh = annotateresult[idxs[0]][0]
                        if self.ui.debugflag:
                            self.ui.debug(
                                b'fastannotate: reading %s line #%d '
                                b'to resolve lines %r\n'
                                % (short(hsh), linenum, idxs)
                            )
                        fctx = self._resolvefctx(hsh, revmap.rev2path(rev))
                        lines = mdiff.splitnewlines(fctx.data())
                        revlines[rev] = lines
                    for idx in idxs:
                        result[idx] = revlines[rev][linenum]
                assert all(x is not None for x in result)
                return result

            # run the annotate and the lines should match to the file content
            self.ui.debug(
                b'fastannotate: annotate %s to resolve lines\n' % short(hsh)
            )
            linelog.annotate(rev)
            fctx = self._resolvefctx(hsh, revmap.rev2path(rev))
            annotated = linelog.annotateresult
            lines = mdiff.splitnewlines(fctx.data())
            if len(lines) != len(annotated):
                raise faerror.CorruptedFileError(b'unexpected annotated lines')
            # resolve lines from the annotate result
            for i, line in enumerate(lines):
                k = annotated[i]
                if k in key2idxs:
                    for idx in key2idxs[k]:
                        result[idx] = line
                    del key2idxs[k]
        return result
###END###
def annotatedirectly(self, f, showpath, showlines):
        """like annotate, but when we know that f is in linelog.
        f can be either a 20-char str (node) or a fctx. this is for perf - in
        the best case, the user provides a node and we don't need to read the
        filelog or construct any filecontext.
        """
        if isinstance(f, bytes):
            hsh = f
        else:
            hsh = f.node()
        llrev = self.revmap.hsh2rev(hsh)
        if not llrev:
            raise faerror.CorruptedFileError(b'%s is not in revmap' % hex(hsh))
        if (self.revmap.rev2flag(llrev) & revmapmod.sidebranchflag) != 0:
            raise faerror.CorruptedFileError(
                b'%s is not in revmap mainbranch' % hex(hsh)
            )
        self.linelog.annotate(llrev)
        result = [
            (self.revmap.rev2hsh(r), l) for r, l in self.linelog.annotateresult
        ]
        return self._refineannotateresult(result, f, showpath, showlines)
###END###
def _refineannotateresult(self, result, f, showpath, showlines):
        """add the missing path or line contents, they can be expensive.
        f could be either node or fctx.
        """
        if showpath:
            result = self._addpathtoresult(result)
        if showlines:
            if isinstance(f, bytes):  # f: node or fctx
                llrev = self.revmap.hsh2rev(f)
                fctx = self._resolvefctx(f, self.revmap.rev2path(llrev))
            else:
                fctx = f
            lines = mdiff.splitnewlines(fctx.data())
            if len(lines) != len(result):  # linelog is probably corrupted
                raise faerror.CorruptedFileError()
            result = (result, lines)
        return result
###END###
def _appendrev(self, fctx, blocks, bannotated=None):
        self._doappendrev(self.linelog, self.revmap, fctx, blocks, bannotated)
###END###
def _diffblocks(self, a, b):
        return mdiff.allblocks(a, b, self.opts.diffopts)
###END###
def _doappendrev(linelog, revmap, fctx, blocks, bannotated=None):
        """append a revision to linelog and revmap"""

        def getllrev(f):
            """(fctx) -> int"""
            # f should not be a linelog revision
            if isinstance(f, int):
                raise error.ProgrammingError(b'f should not be an int')
            # f is a fctx, allocate linelog rev on demand
            hsh = f.node()
            rev = revmap.hsh2rev(hsh)
            if rev is None:
                rev = revmap.append(hsh, sidebranch=True, path=f.path())
            return rev

        # append sidebranch revisions to revmap
        siderevs = []
        siderevmap = {}  # node: int
        if bannotated is not None:
            for (a1, a2, b1, b2), op in blocks:
                if op != b'=':
                    # f could be either linelong rev, or fctx.
                    siderevs += [
                        f
                        for f, l in bannotated[b1:b2]
                        if not isinstance(f, int)
                    ]
        siderevs = set(siderevs)
        if fctx in siderevs:  # mainnode must be appended seperately
            siderevs.remove(fctx)
        for f in siderevs:
            siderevmap[f] = getllrev(f)

        # the changeset in the main branch, could be a merge
        llrev = revmap.append(fctx.node(), path=fctx.path())
        siderevmap[fctx] = llrev

        for (a1, a2, b1, b2), op in reversed(blocks):
            if op == b'=':
                continue
            if bannotated is None:
                linelog.replacelines(llrev, a1, a2, b1, b2)
            else:
                blines = [
                    ((r if isinstance(r, int) else siderevmap[r]), l)
                    for r, l in bannotated[b1:b2]
                ]
                linelog.replacelines_vec(llrev, a1, a2, blines)
###END###
def _addpathtoresult(self, annotateresult, revmap=None):
        """(revmap, [(node, linenum)]) -> [(node, linenum, path)]"""
        if revmap is None:
            revmap = self.revmap

        def _getpath(nodeid):
            path = self._node2path.get(nodeid)
            if path is None:
                path = revmap.rev2path(revmap.hsh2rev(nodeid))
                self._node2path[nodeid] = path
            return path

        return [(n, l, _getpath(n)) for n, l in annotateresult]
###END###
def _checklastmasterhead(self, fctx):
        """check if fctx is the master's head last time, raise if not"""
        if fctx is None:
            llrev = 0
        else:
            llrev = self.revmap.hsh2rev(fctx.node())
            if not llrev:
                raise faerror.CannotReuseError()
        if self.linelog.maxrev != llrev:
            raise faerror.CannotReuseError()
###END###
def _parentfunc(self):
        """-> (fctx) -> [fctx]"""
        followrename = self.opts.followrename
        followmerge = self.opts.followmerge

        def parents(f):
            pl = _parents(f, follow=followrename)
            if not followmerge:
                pl = pl[:1]
            return pl

        return parents
###END###
def _perfhack(self):
        return self.ui.configbool(b'fastannotate', b'perfhack')
###END###
def _resolvefctx(self, rev, path=None, **kwds):
        return resolvefctx(self.repo, rev, (path or self.path), **kwds)
###END###
def __init__(self, repo, path, opts=defaultopts):
        # different options use different directories
        self._vfspath = os.path.join(
            b'fastannotate', opts.shortstr, encodedir(path)
        )
        self._repo = repo
###END###
def dirname(self):
        return os.path.dirname(self._repo.vfs.join(self._vfspath))
###END###
def linelogpath(self):
        return self._repo.vfs.join(self._vfspath + b'.l')
###END###
def lock(self):
        return lockmod.lock(self._repo.vfs, self._vfspath + b'.lock')
###END###
def revmappath(self):
        return self._repo.vfs.join(self._vfspath + b'.m')
###END###
def __init__(self, path=None):
        """create or load the revmap, optionally associate to a file

        if path is None, the revmap is entirely in-memory. the caller is
        responsible for locking. concurrent writes to a same file is unsafe.
        the caller needs to make sure one file is associated to at most one
        revmap object at a time."""
        self.path = path
        self._rev2hsh = [None]
        self._rev2flag = [None]
        self._hsh2rev = {}
        # since rename does not happen frequently, do not store path for every
        # revision. self._renamerevs can be used for bisecting.
        self._renamerevs = [0]
        self._renamepaths = [b'']
        self._lastmaxrev = -1
        if path:
            if os.path.exists(path):
                self._load()
            else:
                # write the header so "append" can do incremental updates
                self.flush()
###END###
def copyfrom(self, rhs):
        """copy the map data from another revmap. do not affect self.path"""
        self._rev2hsh = rhs._rev2hsh[:]
        self._rev2flag = rhs._rev2flag[:]
        self._hsh2rev = rhs._hsh2rev.copy()
        self._renamerevs = rhs._renamerevs[:]
        self._renamepaths = rhs._renamepaths[:]
        self._lastmaxrev = -1
###END###
def maxrev(self):
        """return max linelog revision number"""
        return len(self._rev2hsh) - 1
###END###
def append(self, hsh, sidebranch=False, path=None, flush=False):
        """add a binary hg hash and return the mapped linelog revision.
        if flush is True, incrementally update the file.
        """
        if hsh in self._hsh2rev:
            raise error.CorruptedFileError(
                b'%r is in revmap already' % hex(hsh)
            )
        if len(hsh) != _hshlen:
            raise hgerror.ProgrammingError(
                b'hsh must be %d-char long' % _hshlen
            )
        idx = len(self._rev2hsh)
        flag = 0
        if sidebranch:
            flag |= sidebranchflag
        if path is not None and path != self._renamepaths[-1]:
            flag |= renameflag
            self._renamerevs.append(idx)
            self._renamepaths.append(path)
        self._rev2hsh.append(hsh)
        self._rev2flag.append(flag)
        self._hsh2rev[hsh] = idx
        if flush:
            self.flush()
        return idx
###END###
def rev2hsh(self, rev):
        """convert linelog revision to hg hash. return None if not found."""
        if rev > self.maxrev or rev < 0:
            return None
        return self._rev2hsh[rev]
###END###
def rev2flag(self, rev):
        """get the flag (uint8) for a given linelog revision.
        return None if revision does not exist.
        """
        if rev > self.maxrev or rev < 0:
            return None
        return self._rev2flag[rev]
###END###
def rev2path(self, rev):
        """get the path for a given linelog revision.
        return None if revision does not exist.
        """
        if rev > self.maxrev or rev < 0:
            return None
        idx = bisect.bisect_right(self._renamerevs, rev) - 1
        return self._renamepaths[idx]
###END###
def hsh2rev(self, hsh):
        """convert hg hash to linelog revision. return None if not found."""
        return self._hsh2rev.get(hsh)
###END###
def clear(self, flush=False):
        """make the map empty. if flush is True, write to disk"""
        # rev 0 is reserved, real rev starts from 1
        self._rev2hsh = [None]
        self._rev2flag = [None]
        self._hsh2rev = {}
        self._rev2path = [b'']
        self._lastmaxrev = -1
        if flush:
            self.flush()
###END###
def flush(self):
        """write the state down to the file"""
        if not self.path:
            return
        if self._lastmaxrev == -1:  # write the entire file
            with open(self.path, b'wb') as f:
                f.write(self.HEADER)
                for i in pycompat.xrange(1, len(self._rev2hsh)):
                    self._writerev(i, f)
        else:  # append incrementally
            with open(self.path, b'ab') as f:
                for i in pycompat.xrange(
                    self._lastmaxrev + 1, len(self._rev2hsh)
                ):
                    self._writerev(i, f)
        self._lastmaxrev = self.maxrev
###END###
def _load(self):
        """load state from file"""
        if not self.path:
            return
        # use local variables in a loop. CPython uses LOAD_FAST for them,
        # which is faster than both LOAD_CONST and LOAD_GLOBAL.
        flaglen = 1
        hshlen = _hshlen
        with open(self.path, b'rb') as f:
            if f.read(len(self.HEADER)) != self.HEADER:
                raise error.CorruptedFileError()
            self.clear(flush=False)
            while True:
                buf = f.read(flaglen)
                if not buf:
                    break
                flag = ord(buf)
                rev = len(self._rev2hsh)
                if flag & renameflag:
                    path = self._readcstr(f)
                    self._renamerevs.append(rev)
                    self._renamepaths.append(path)
                hsh = f.read(hshlen)
                if len(hsh) != hshlen:
                    raise error.CorruptedFileError()
                self._hsh2rev[hsh] = rev
                self._rev2flag.append(flag)
                self._rev2hsh.append(hsh)
        self._lastmaxrev = self.maxrev
###END###
def _writerev(self, rev, f):
        """append a revision data to file"""
        flag = self._rev2flag[rev]
        hsh = self._rev2hsh[rev]
        f.write(struct.pack(b'B', flag))
        if flag & renameflag:
            path = self.rev2path(rev)
            if path is None:
                raise error.CorruptedFileError(b'cannot find path for %s' % rev)
            f.write(path + b'\0')
        f.write(hsh)
###END###
def _readcstr(f):
        """read a C-language-like '\0'-terminated string"""
        buf = b''
        while True:
            ch = f.read(1)
            if not ch:  # unexpected eof
                raise error.CorruptedFileError()
            if ch == b'\0':
                break
            buf += ch
        return buf
###END###
def __contains__(self, f):
        """(fctx or (node, path)) -> bool.
        test if (node, path) is in the map, and is not in a side branch.
        f can be either a tuple of (node, path), or a fctx.
        """
        if isinstance(f, tuple):  # f: (node, path)
            hsh, path = f
        else:  # f: fctx
            hsh, path = f.node(), f.path()
        rev = self.hsh2rev(hsh)
        if rev is None:
            return False
        if path is not None and path != self.rev2path(rev):
            return False
        return (self.rev2flag(rev) & sidebranchflag) == 0
###END###
def getannotate(self, path, lastnode=None):
            if not self.capable(b'getannotate'):
                ui.warn(_(b'remote peer cannot provide annotate cache\n'))
                yield None, None
            else:
                args = {b'path': path, b'lastnode': lastnode or b''}
                f = wireprotov1peer.future()
                yield args, f
                yield _parseresponse(f.value)
###END###
def prefetchfastannotate(self, paths, peer=None):
            master = _getmaster(self.ui)
            needupdatepaths = []
            lastnodemap = {}
            try:
                for path in _filterfetchpaths(self, paths):
                    with context.annotatecontext(self, path) as actx:
                        if not actx.isuptodate(master, strict=False):
                            needupdatepaths.append(path)
                            lastnodemap[path] = actx.lastnode
                if needupdatepaths:
                    clientfetch(self, needupdatepaths, lastnodemap, peer)
            except Exception as ex:
                # could be directory not writable or so, not fatal
                self.ui.debug(b'fastannotate: prefetch failed: %r\n' % ex)
###END###
def __init__(self, ui, repotype, path, revs=None):
        super(bzr_source, self).__init__(ui, repotype, path, revs=revs)

        if not os.path.exists(os.path.join(path, b'.bzr')):
            raise common.NoRepo(
                _(b'%s does not look like a Bazaar repository') % path
            )

        try:
            # access breezy stuff
            bzrdir
        except NameError:
            raise common.NoRepo(_(b'Bazaar modules could not be loaded'))

        path = util.abspath(path)
        self._checkrepotype(path)
        try:
            bzr_dir = bzrdir.BzrDir.open(path.decode())
            self.sourcerepo = bzr_dir.open_repository()
        except errors.NoRepositoryPresent:
            raise common.NoRepo(
                _(b'%s does not look like a Bazaar repository') % path
            )
        self._parentids = {}
        self._saverev = ui.configbool(b'convert', b'bzr.saverev')
###END###
def _checkrepotype(self, path):
        # Lightweight checkouts detection is informational but probably
        # fragile at API level. It should not terminate the conversion.
        try:
            dir = bzrdir.BzrDir.open_containing(path.decode())[0]
            try:
                tree = dir.open_workingtree(recommend_upgrade=False)
                branch = tree.branch
            except (errors.NoWorkingTree, errors.NotLocalUrl):
                tree = None
                branch = dir.open_branch()
            if (
                tree is not None
                and tree.controldir.root_transport.base
                != branch.controldir.root_transport.base
            ):
                self.ui.warn(
                    _(
                        b'warning: lightweight checkouts may cause '
                        b'conversion failures, try with a regular '
                        b'branch instead.\n'
                    )
                )
        except Exception:
            self.ui.note(_(b'bzr source type could not be determined\n'))
###END###
def before(self):
        """Before the conversion begins, acquire a read lock
        for all the operations that might need it. Fortunately
        read locks don't block other reads or writes to the
        repository, so this shouldn't have any impact on the usage of
        the source repository.

        The alternative would be locking on every operation that
        needs locks (there are currently two: getting the file and
        getting the parent map) and releasing immediately after,
        but this approach can take even 40% longer."""
        self.sourcerepo.lock_read()
###END###
def after(self):
        self.sourcerepo.unlock()
###END###
def _bzrbranches(self):
        return self.sourcerepo.find_branches(using=True)
###END###
def getheads(self):
        if not self.revs:
            # Set using=True to avoid nested repositories (see issue3254)
            heads = sorted([b.last_revision() for b in self._bzrbranches()])
        else:
            revid = None
            for branch in self._bzrbranches():
                try:
                    revspec = self.revs[0].decode()
                    r = revisionspec.RevisionSpec.from_string(revspec)
                    info = r.in_history(branch)
                except errors.BzrError:
                    pass
                revid = info.rev_id
            if revid is None:
                raise error.Abort(
                    _(b'%s is not a valid revision') % self.revs[0]
                )
            heads = [revid]
        # Empty repositories return 'null:', which cannot be retrieved
        heads = [h for h in heads if h != b'null:']
        return heads
###END###
def getfile(self, name, rev):
        name = name.decode()
        revtree = self.sourcerepo.revision_tree(rev)

        try:
            kind = revtree.kind(name)
        except breezy.errors.NoSuchFile:
            return None, None
        if kind not in supportedkinds:
            # the file is not available anymore - was deleted
            return None, None
        mode = self._modecache[(name.encode(), rev)]
        if kind == 'symlink':
            target = revtree.get_symlink_target(name)
            if target is None:
                raise error.Abort(
                    _(b'%s.%s symlink has no target') % (name, rev)
                )
            return target.encode(), mode
        else:
            sio = revtree.get_file(name)
            return sio.read(), mode
###END###
def getchanges(self, version, full):
        if full:
            raise error.Abort(_(b"convert from cvs does not support --full"))
        self._modecache = {}
        self._revtree = self.sourcerepo.revision_tree(version)
        # get the parentids from the cache
        parentids = self._parentids.pop(version)
        # only diff against first parent id
        prevtree = self.sourcerepo.revision_tree(parentids[0])
        files, changes = self._gettreechanges(self._revtree, prevtree)
        return files, changes, set()
###END###
def getcommit(self, version):
        rev = self.sourcerepo.get_revision(version)
        # populate parent id cache
        if not rev.parent_ids:
            parents = []
            self._parentids[version] = (revision.NULL_REVISION,)
        else:
            parents = self._filterghosts(rev.parent_ids)
            self._parentids[version] = parents

        branch = rev.properties.get('branch-nick', 'default')
        if branch == 'trunk':
            branch = 'default'
        return common.commit(
            parents=parents,
            date=b'%d %d' % (rev.timestamp, -rev.timezone),
            author=self.recode(rev.committer),
            desc=self.recode(rev.message),
            branch=branch.encode('utf8'),
            rev=version,
            saverev=self._saverev,
        )
###END###
def gettags(self):
        bytetags = {}
        for branch in self._bzrbranches():
            if not branch.supports_tags():
                return {}
            tagdict = branch.tags.get_tag_dict()
            for name, rev in pycompat.iteritems(tagdict):
                bytetags[self.recode(name)] = rev
        return bytetags
###END###
def getchangedfiles(self, rev, i):
        self._modecache = {}
        curtree = self.sourcerepo.revision_tree(rev)
        if i is not None:
            parentid = self._parentids[rev][i]
        else:
            # no parent id, get the empty revision
            parentid = revision.NULL_REVISION

        prevtree = self.sourcerepo.revision_tree(parentid)
        changes = [e[0] for e in self._gettreechanges(curtree, prevtree)[0]]
        return changes
###END###
def _gettreechanges(self, current, origin):
        revid = current._revision_id
        changes = []
        renames = {}
        seen = set()

        # Fall back to the deprecated attribute for legacy installations.
        try:
            inventory = origin.root_inventory
        except AttributeError:
            inventory = origin.inventory

        # Process the entries by reverse lexicographic name order to
        # handle nested renames correctly, most specific first.

        def key(c):
            return c.path[0] or c.path[1] or ""

        curchanges = sorted(
            current.iter_changes(origin),
            key=key,
            reverse=True,
        )
        for change in curchanges:
            paths = change.path
            kind = change.kind
            executable = change.executable
            if paths[0] == u'' or paths[1] == u'':
                # ignore changes to tree root
                continue

            # bazaar tracks directories, mercurial does not, so
            # we have to rename the directory contents
            if kind[1] == 'directory':
                if kind[0] not in (None, 'directory'):
                    # Replacing 'something' with a directory, record it
                    # so it can be removed.
                    changes.append((self.recode(paths[0]), revid))

                if kind[0] == 'directory' and None not in paths:
                    renaming = paths[0] != paths[1]
                    # neither an add nor an delete - a move
                    # rename all directory contents manually
                    subdir = inventory.path2id(paths[0])
                    # get all child-entries of the directory
                    for name, entry in inventory.iter_entries(subdir):
                        # hg does not track directory renames
                        if entry.kind == 'directory':
                            continue
                        frompath = self.recode(paths[0] + '/' + name)
                        if frompath in seen:
                            # Already handled by a more specific change entry
                            # This is important when you have:
                            # a => b
                            # a/c => a/c
                            # Here a/c must not be renamed into b/c
                            continue
                        seen.add(frompath)
                        if not renaming:
                            continue
                        topath = self.recode(paths[1] + '/' + name)
                        # register the files as changed
                        changes.append((frompath, revid))
                        changes.append((topath, revid))
                        # add to mode cache
                        mode = (
                            (entry.executable and b'x')
                            or (entry.kind == 'symlink' and b's')
                            or b''
                        )
                        self._modecache[(topath, revid)] = mode
                        # register the change as move
                        renames[topath] = frompath

                # no further changes, go to the next change
                continue

            # we got unicode paths, need to convert them
            path, topath = paths
            if path is not None:
                path = self.recode(path)
            if topath is not None:
                topath = self.recode(topath)
            seen.add(path or topath)

            if topath is None:
                # file deleted
                changes.append((path, revid))
                continue

            # renamed
            if path and path != topath:
                renames[topath] = path
                changes.append((path, revid))

            # populate the mode cache
            kind, executable = [e[1] for e in (kind, executable)]
            mode = (executable and b'x') or (kind == 'symlink' and b'l') or b''
            self._modecache[(topath, revid)] = mode
            changes.append((topath, revid))

        return changes, renames
###END###
def _filterghosts(self, ids):
        """Filters out ghost revisions which hg does not support, see
        <http://bazaar-vcs.org/GhostRevision>
        """
        parentmap = self.sourcerepo.get_parent_map(ids)
        parents = tuple([parent for parent in ids if parent in parentmap])
        return parents
###END###
def __init__(self, ui, source, filecount):
        self.ui = ui
        self.source = source
        self.progress = ui.makeprogress(
            _(b'getting files'), unit=_(b'files'), total=filecount
        )
###END###
def getfile(self, file, rev):
        self.progress.increment(item=file)
        return self.source.getfile(file, rev)
###END###
def targetfilebelongstosource(self, targetfilename):
        return self.source.targetfilebelongstosource(targetfilename)
###END###
def lookuprev(self, rev):
        return self.source.lookuprev(rev)
###END###
def close(self):
        self.progress.complete()
###END###
def __init__(self, ui, source, dest, revmapfile, opts):

        self.source = source
        self.dest = dest
        self.ui = ui
        self.opts = opts
        self.commitcache = {}
        self.authors = {}
        self.authorfile = None

        # Record converted revisions persistently: maps source revision
        # ID to target revision ID (both strings).  (This is how
        # incremental conversions work.)
        self.map = mapfile(ui, revmapfile)

        # Read first the dst author map if any
        authorfile = self.dest.authorfile()
        if authorfile and os.path.exists(authorfile):
            self.readauthormap(authorfile)
        # Extend/Override with new author map if necessary
        if opts.get(b'authormap'):
            self.readauthormap(opts.get(b'authormap'))
            self.authorfile = self.dest.authorfile()

        self.splicemap = self.parsesplicemap(opts.get(b'splicemap'))
        self.branchmap = mapfile(ui, opts.get(b'branchmap'))
###END###
def parsesplicemap(self, path):
        """check and validate the splicemap format and
        return a child/parents dictionary.
        Format checking has two parts.
        1. generic format which is same across all source types
        2. specific format checking which may be different for
           different source type.  This logic is implemented in
           checkrevformat function in source files like
           hg.py, subversion.py etc.
        """

        if not path:
            return {}
        m = {}
        try:
            fp = open(path, b'rb')
            for i, line in enumerate(util.iterfile(fp)):
                line = line.splitlines()[0].rstrip()
                if not line:
                    # Ignore blank lines
                    continue
                # split line
                lex = common.shlexer(data=line, whitespace=b',')
                line = list(lex)
                # check number of parents
                if not (2 <= len(line) <= 3):
                    raise error.Abort(
                        _(
                            b'syntax error in %s(%d): child parent1'
                            b'[,parent2] expected'
                        )
                        % (path, i + 1)
                    )
                for part in line:
                    self.source.checkrevformat(part)
                child, p1, p2 = line[0], line[1:2], line[2:]
                if p1 == p2:
                    m[child] = p1
                else:
                    m[child] = p1 + p2
        # if file does not exist or error reading, exit
        except IOError:
            raise error.Abort(
                _(b'splicemap file not found or error reading %s:') % path
            )
        return m
###END###
def walktree(self, heads):
        """Return a mapping that identifies the uncommitted parents of every
        uncommitted changeset."""
        visit = list(heads)
        known = set()
        parents = {}
        numcommits = self.source.numcommits()
        progress = self.ui.makeprogress(
            _(b'scanning'), unit=_(b'revisions'), total=numcommits
        )
        while visit:
            n = visit.pop(0)
            if n in known:
                continue
            if n in self.map:
                m = self.map[n]
                if m == SKIPREV or self.dest.hascommitfrommap(m):
                    continue
            known.add(n)
            progress.update(len(known))
            commit = self.cachecommit(n)
            parents[n] = []
            for p in commit.parents:
                parents[n].append(p)
                visit.append(p)
        progress.complete()

        return parents
###END###
def mergesplicemap(self, parents, splicemap):
        """A splicemap redefines child/parent relationships. Check the
        map contains valid revision identifiers and merge the new
        links in the source graph.
        """
        for c in sorted(splicemap):
            if c not in parents:
                if not self.dest.hascommitforsplicemap(self.map.get(c, c)):
                    # Could be in source but not converted during this run
                    self.ui.warn(
                        _(
                            b'splice map revision %s is not being '
                            b'converted, ignoring\n'
                        )
                        % c
                    )
                continue
            pc = []
            for p in splicemap[c]:
                # We do not have to wait for nodes already in dest.
                if self.dest.hascommitforsplicemap(self.map.get(p, p)):
                    continue
                # Parent is not in dest and not being converted, not good
                if p not in parents:
                    raise error.Abort(_(b'unknown splice map parent: %s') % p)
                pc.append(p)
            parents[c] = pc
###END###
def toposort(self, parents, sortmode):
        """Return an ordering such that every uncommitted changeset is
        preceded by all its uncommitted ancestors."""

        def mapchildren(parents):
            """Return a (children, roots) tuple where 'children' maps parent
            revision identifiers to children ones, and 'roots' is the list of
            revisions without parents. 'parents' must be a mapping of revision
            identifier to its parents ones.
            """
            visit = collections.deque(sorted(parents))
            seen = set()
            children = {}
            roots = []

            while visit:
                n = visit.popleft()
                if n in seen:
                    continue
                seen.add(n)
                # Ensure that nodes without parents are present in the
                # 'children' mapping.
                children.setdefault(n, [])
                hasparent = False
                for p in parents[n]:
                    if p not in self.map:
                        visit.append(p)
                        hasparent = True
                    children.setdefault(p, []).append(n)
                if not hasparent:
                    roots.append(n)

            return children, roots

        # Sort functions are supposed to take a list of revisions which
        # can be converted immediately and pick one

        def makebranchsorter():
            """If the previously converted revision has a child in the
            eligible revisions list, pick it. Return the list head
            otherwise. Branch sort attempts to minimize branch
            switching, which is harmful for Mercurial backend
            compression.
            """
            prev = [None]

            def picknext(nodes):
                next = nodes[0]
                for n in nodes:
                    if prev[0] in parents[n]:
                        next = n
                        break
                prev[0] = next
                return next

            return picknext

        def makesourcesorter():
            """Source specific sort."""
            keyfn = lambda n: self.commitcache[n].sortkey

            def picknext(nodes):
                return sorted(nodes, key=keyfn)[0]

            return picknext

        def makeclosesorter():
            """Close order sort."""
            keyfn = lambda n: (
                b'close' not in self.commitcache[n].extra,
                self.commitcache[n].sortkey,
            )

            def picknext(nodes):
                return sorted(nodes, key=keyfn)[0]

            return picknext

        def makedatesorter():
            """Sort revisions by date."""
            dates = {}

            def getdate(n):
                if n not in dates:
                    dates[n] = dateutil.parsedate(self.commitcache[n].date)
                return dates[n]

            def picknext(nodes):
                return min([(getdate(n), n) for n in nodes])[1]

            return picknext

        if sortmode == b'branchsort':
            picknext = makebranchsorter()
        elif sortmode == b'datesort':
            picknext = makedatesorter()
        elif sortmode == b'sourcesort':
            picknext = makesourcesorter()
        elif sortmode == b'closesort':
            picknext = makeclosesorter()
        else:
            raise error.Abort(_(b'unknown sort mode: %s') % sortmode)

        children, actives = mapchildren(parents)

        s = []
        pendings = {}
        while actives:
            n = picknext(actives)
            actives.remove(n)
            s.append(n)

            # Update dependents list
            for c in children.get(n, []):
                if c not in pendings:
                    pendings[c] = [p for p in parents[c] if p not in self.map]
                try:
                    pendings[c].remove(n)
                except ValueError:
                    raise error.Abort(
                        _(b'cycle detected between %s and %s')
                        % (recode(c), recode(n))
                    )
                if not pendings[c]:
                    # Parents are converted, node is eligible
                    actives.insert(0, c)
                    pendings[c] = None

        if len(s) != len(parents):
            raise error.Abort(_(b"not all revisions were sorted"))

        return s
###END###
def writeauthormap(self):
        authorfile = self.authorfile
        if authorfile:
            self.ui.status(_(b'writing author map file %s\n') % authorfile)
            ofile = open(authorfile, b'wb+')
            for author in self.authors:
                ofile.write(
                    util.tonativeeol(
                        b"%s=%s\n" % (author, self.authors[author])
                    )
                )
            ofile.close()
###END###
def readauthormap(self, authorfile):
        self.authors = readauthormap(self.ui, authorfile, self.authors)
###END###
def cachecommit(self, rev):
        commit = self.source.getcommit(rev)
        commit.author = self.authors.get(commit.author, commit.author)
        commit.branch = mapbranch(commit.branch, self.branchmap)
        self.commitcache[rev] = commit
        return commit
###END###
def copy(self, rev):
        commit = self.commitcache[rev]
        full = self.opts.get(b'full')
        changes = self.source.getchanges(rev, full)
        if isinstance(changes, bytes):
            if changes == SKIPREV:
                dest = SKIPREV
            else:
                dest = self.map[changes]
            self.map[rev] = dest
            return
        files, copies, cleanp2 = changes
        pbranches = []
        if commit.parents:
            for prev in commit.parents:
                if prev not in self.commitcache:
                    self.cachecommit(prev)
                pbranches.append(
                    (self.map[prev], self.commitcache[prev].branch)
                )
        self.dest.setbranch(commit.branch, pbranches)
        try:
            parents = self.splicemap[rev]
            self.ui.status(
                _(b'spliced in %s as parents of %s\n')
                % (_(b' and ').join(parents), rev)
            )
            parents = [self.map.get(p, p) for p in parents]
        except KeyError:
            parents = [b[0] for b in pbranches]
            parents.extend(
                self.map[x] for x in commit.optparents if x in self.map
            )
        if len(pbranches) != 2:
            cleanp2 = set()
        if len(parents) < 3:
            source = progresssource(self.ui, self.source, len(files))
        else:
            # For an octopus merge, we end up traversing the list of
            # changed files N-1 times. This tweak to the number of
            # files makes it so the progress bar doesn't overflow
            # itself.
            source = progresssource(
                self.ui, self.source, len(files) * (len(parents) - 1)
            )
        newnode = self.dest.putcommit(
            files, copies, parents, commit, source, self.map, full, cleanp2
        )
        source.close()
        self.source.converted(rev, newnode)
        self.map[rev] = newnode
###END###
def convert(self, sortmode):
        try:
            self.source.before()
            self.dest.before()
            self.source.setrevmap(self.map)
            self.ui.status(_(b"scanning source...\n"))
            heads = self.source.getheads()
            parents = self.walktree(heads)
            self.mergesplicemap(parents, self.splicemap)
            self.ui.status(_(b"sorting...\n"))
            t = self.toposort(parents, sortmode)
            num = len(t)
            c = None

            self.ui.status(_(b"converting...\n"))
            progress = self.ui.makeprogress(
                _(b'converting'), unit=_(b'revisions'), total=len(t)
            )
            for i, c in enumerate(t):
                num -= 1
                desc = self.commitcache[c].desc
                if b"\n" in desc:
                    desc = desc.splitlines()[0]
                # convert log message to local encoding without using
                # tolocal() because the encoding.encoding convert()
                # uses is 'utf-8'
                self.ui.status(b"%d %s\n" % (num, recode(desc)))
                self.ui.note(_(b"source: %s\n") % recode(c))
                progress.update(i)
                self.copy(c)
            progress.complete()

            if not self.ui.configbool(b'convert', b'skiptags'):
                tags = self.source.gettags()
                ctags = {}
                for k in tags:
                    v = tags[k]
                    if self.map.get(v, SKIPREV) != SKIPREV:
                        ctags[k] = self.map[v]

                if c and ctags:
                    nrev, tagsparent = self.dest.puttags(ctags)
                    if nrev and tagsparent:
                        # write another hash correspondence to override the
                        # previous one so we don't end up with extra tag heads
                        tagsparents = [
                            e
                            for e in pycompat.iteritems(self.map)
                            if e[1] == tagsparent
                        ]
                        if tagsparents:
                            self.map[tagsparents[0][0]] = nrev

            bookmarks = self.source.getbookmarks()
            cbookmarks = {}
            for k in bookmarks:
                v = bookmarks[k]
                if self.map.get(v, SKIPREV) != SKIPREV:
                    cbookmarks[k] = self.map[v]

            if c and cbookmarks:
                self.dest.putbookmarks(cbookmarks)

            self.writeauthormap()
        finally:
            self.cleanup()
###END###
def cleanup(self):
        try:
            self.dest.after()
        finally:
            self.source.after()
        self.map.close()
###END###
def __init__(self, ui, path=None):
        self.ui = ui
        self.include = {}
        self.exclude = {}
        self.rename = {}
        self.targetprefixes = None
        if path:
            if self.parse(path):
                raise error.Abort(_(b'errors in filemap'))
###END###
def parse(self, path):
        errs = 0

        def check(name, mapping, listname):
            if not name:
                self.ui.warn(
                    _(b'%s:%d: path to %s is missing\n')
                    % (lex.infile, lex.lineno, listname)
                )
                return 1
            if name in mapping:
                self.ui.warn(
                    _(b'%s:%d: %r already in %s list\n')
                    % (lex.infile, lex.lineno, name, listname)
                )
                return 1
            if name.startswith(b'/') or name.endswith(b'/') or b'//' in name:
                self.ui.warn(
                    _(b'%s:%d: superfluous / in %s %r\n')
                    % (lex.infile, lex.lineno, listname, pycompat.bytestr(name))
                )
                return 1
            return 0

        lex = common.shlexer(
            filepath=path, wordchars=b'!@#$%^&*()-=+[]{}|;:,./<>?'
        )
        cmd = lex.get_token()
        while cmd:
            if cmd == b'include':
                name = normalize(lex.get_token())
                errs += check(name, self.exclude, b'exclude')
                self.include[name] = name
            elif cmd == b'exclude':
                name = normalize(lex.get_token())
                errs += check(name, self.include, b'include')
                errs += check(name, self.rename, b'rename')
                self.exclude[name] = name
            elif cmd == b'rename':
                src = normalize(lex.get_token())
                dest = normalize(lex.get_token())
                errs += check(src, self.exclude, b'exclude')
                self.rename[src] = dest
            elif cmd == b'source':
                errs += self.parse(normalize(lex.get_token()))
            else:
                self.ui.warn(
                    _(b'%s:%d: unknown directive %r\n')
                    % (lex.infile, lex.lineno, pycompat.bytestr(cmd))
                )
                errs += 1
            cmd = lex.get_token()
        return errs
###END###
def lookup(self, name, mapping):
        name = normalize(name)
        for pre, suf in rpairs(name):
            try:
                return mapping[pre], pre, suf
            except KeyError:
                pass
        return b'', name, b''
###END###
def istargetfile(self, filename):
        """Return true if the given target filename is covered as a destination
        of the filemap. This is useful for identifying what parts of the target
        repo belong to the source repo and what parts don't."""
        if self.targetprefixes is None:
            self.targetprefixes = set()
            for before, after in pycompat.iteritems(self.rename):
                self.targetprefixes.add(after)

        # If "." is a target, then all target files are considered from the
        # source.
        if not self.targetprefixes or b'.' in self.targetprefixes:
            return True

        filename = normalize(filename)
        for pre, suf in rpairs(filename):
            # This check is imperfect since it doesn't account for the
            # include/exclude list, but it should work in filemaps that don't
            # apply include/exclude to the same source directories they are
            # renaming.
            if pre in self.targetprefixes:
                return True
        return False
###END###
def __call__(self, name):
        if self.include:
            inc = self.lookup(name, self.include)[0]
        else:
            inc = name
        if self.exclude:
            exc = self.lookup(name, self.exclude)[0]
        else:
            exc = b''
        if (not self.include and exc) or (len(inc) <= len(exc)):
            return None
        newpre, pre, suf = self.lookup(name, self.rename)
        if newpre:
            if newpre == b'.':
                return suf
            if suf:
                if newpre.endswith(b'/'):
                    return newpre + suf
                return newpre + b'/' + suf
            return newpre
        return name
###END###
def active(self):
        return bool(self.include or self.exclude or self.rename)
###END###
def __init__(self, ui, baseconverter, filemap):
        super(filemap_source, self).__init__(ui, baseconverter.repotype)
        self.base = baseconverter
        self.filemapper = filemapper(ui, filemap)
        self.commits = {}
        # if a revision rev has parent p in the original revision graph, then
        # rev will have parent self.parentmap[p] in the restricted graph.
        self.parentmap = {}
        # self.wantedancestors[rev] is the set of all ancestors of rev that
        # are in the restricted graph.
        self.wantedancestors = {}
        self.convertedorder = None
        self._rebuilt = False
        self.origparents = {}
        self.children = {}
        self.seenchildren = {}
        # experimental config: convert.ignoreancestorcheck
        self.ignoreancestorcheck = self.ui.configbool(
            b'convert', b'ignoreancestorcheck'
        )
###END###
def before(self):
        self.base.before()
###END###
def after(self):
        self.base.after()
###END###
def setrevmap(self, revmap):
        # rebuild our state to make things restartable
        #
        # To avoid calling getcommit for every revision that has already
        # been converted, we rebuild only the parentmap, delaying the
        # rebuild of wantedancestors until we need it (i.e. until a
        # merge).
        #
        # We assume the order argument lists the revisions in
        # topological order, so that we can infer which revisions were
        # wanted by previous runs.
        self._rebuilt = not revmap
        seen = {SKIPREV: SKIPREV}
        dummyset = set()
        converted = []
        for rev in revmap.order:
            mapped = revmap[rev]
            wanted = mapped not in seen
            if wanted:
                seen[mapped] = rev
                self.parentmap[rev] = rev
            else:
                self.parentmap[rev] = seen[mapped]
            self.wantedancestors[rev] = dummyset
            arg = seen[mapped]
            if arg == SKIPREV:
                arg = None
            converted.append((rev, wanted, arg))
        self.convertedorder = converted
        return self.base.setrevmap(revmap)
###END###
def rebuild(self):
        if self._rebuilt:
            return True
        self._rebuilt = True
        self.parentmap.clear()
        self.wantedancestors.clear()
        self.seenchildren.clear()
        for rev, wanted, arg in self.convertedorder:
            if rev not in self.origparents:
                try:
                    self.origparents[rev] = self.getcommit(rev).parents
                except error.RepoLookupError:
                    self.ui.debug(b"unknown revmap source: %s\n" % rev)
                    continue
            if arg is not None:
                self.children[arg] = self.children.get(arg, 0) + 1

        for rev, wanted, arg in self.convertedorder:
            try:
                parents = self.origparents[rev]
            except KeyError:
                continue  # unknown revmap source
            if wanted:
                self.mark_wanted(rev, parents)
            else:
                self.mark_not_wanted(rev, arg)
            self._discard(arg, *parents)

        return True
###END###
def getheads(self):
        return self.base.getheads()
###END###
def getcommit(self, rev):
        # We want to save a reference to the commit objects to be able
        # to rewrite their parents later on.
        c = self.commits[rev] = self.base.getcommit(rev)
        for p in c.parents:
            self.children[p] = self.children.get(p, 0) + 1
        return c
###END###
def numcommits(self):
        return self.base.numcommits()
###END###
def _cachedcommit(self, rev):
        if rev in self.commits:
            return self.commits[rev]
        return self.base.getcommit(rev)
###END###
def _discard(self, *revs):
        for r in revs:
            if r is None:
                continue
            self.seenchildren[r] = self.seenchildren.get(r, 0) + 1
            if self.seenchildren[r] == self.children[r]:
                self.wantedancestors.pop(r, None)
                self.parentmap.pop(r, None)
                del self.seenchildren[r]
                if self._rebuilt:
                    del self.children[r]
###END###
def wanted(self, rev, i):
        # Return True if we're directly interested in rev.
        #
        # i is an index selecting one of the parents of rev (if rev
        # has no parents, i is None).  getchangedfiles will give us
        # the list of files that are different in rev and in the parent
        # indicated by i.  If we're interested in any of these files,
        # we're interested in rev.
        try:
            files = self.base.getchangedfiles(rev, i)
        except NotImplementedError:
            raise error.Abort(_(b"source repository doesn't support --filemap"))
        for f in files:
            if self.filemapper(f):
                return True

        # The include directive is documented to include nothing else (though
        # valid branch closes are included).
        if self.filemapper.include:
            return False

        # Allow empty commits in the source revision through.  The getchanges()
        # method doesn't even bother calling this if it determines that the
        # close marker is significant (i.e. all of the branch ancestors weren't
        # eliminated).  Therefore if there *is* a close marker, getchanges()
        # doesn't consider it significant, and this revision should be dropped.
        return not files and b'close' not in self.commits[rev].extra
###END###
def mark_not_wanted(self, rev, p):
        # Mark rev as not interesting and update data structures.

        if p is None:
            # A root revision. Use SKIPREV to indicate that it doesn't
            # map to any revision in the restricted graph.  Put SKIPREV
            # in the set of wanted ancestors to simplify code elsewhere
            self.parentmap[rev] = SKIPREV
            self.wantedancestors[rev] = {SKIPREV}
            return

        # Reuse the data from our parent.
        self.parentmap[rev] = self.parentmap[p]
        self.wantedancestors[rev] = self.wantedancestors[p]
###END###
def mark_wanted(self, rev, parents):
        # Mark rev ss wanted and update data structures.

        # rev will be in the restricted graph, so children of rev in
        # the original graph should still have rev as a parent in the
        # restricted graph.
        self.parentmap[rev] = rev

        # The set of wanted ancestors of rev is the union of the sets
        # of wanted ancestors of its parents. Plus rev itself.
        wrev = set()
        for p in parents:
            if p in self.wantedancestors:
                wrev.update(self.wantedancestors[p])
            else:
                self.ui.warn(
                    _(b'warning: %s parent %s is missing\n') % (rev, p)
                )
        wrev.add(rev)
        self.wantedancestors[rev] = wrev
###END###
def getchanges(self, rev, full):
        parents = self.commits[rev].parents
        if len(parents) > 1 and not self.ignoreancestorcheck:
            self.rebuild()

        # To decide whether we're interested in rev we:
        #
        # - calculate what parents rev will have if it turns out we're
        #   interested in it.  If it's going to have more than 1 parent,
        #   we're interested in it.
        #
        # - otherwise, we'll compare it with the single parent we found.
        #   If any of the files we're interested in is different in the
        #   the two revisions, we're interested in rev.

        # A parent p is interesting if its mapped version (self.parentmap[p]):
        # - is not SKIPREV
        # - is still not in the list of parents (we don't want duplicates)
        # - is not an ancestor of the mapped versions of the other parents or
        #   there is no parent in the same branch than the current revision.
        mparents = []
        knownparents = set()
        branch = self.commits[rev].branch
        hasbranchparent = False
        for i, p1 in enumerate(parents):
            mp1 = self.parentmap[p1]
            if mp1 == SKIPREV or mp1 in knownparents:
                continue

            isancestor = not self.ignoreancestorcheck and any(
                p2
                for p2 in parents
                if p1 != p2
                and mp1 != self.parentmap[p2]
                and mp1 in self.wantedancestors[p2]
            )
            if not isancestor and not hasbranchparent and len(parents) > 1:
                # This could be expensive, avoid unnecessary calls.
                if self._cachedcommit(p1).branch == branch:
                    hasbranchparent = True
            mparents.append((p1, mp1, i, isancestor))
            knownparents.add(mp1)
        # Discard parents ancestors of other parents if there is a
        # non-ancestor one on the same branch than current revision.
        if hasbranchparent:
            mparents = [p for p in mparents if not p[3]]
        wp = None
        if mparents:
            wp = max(p[2] for p in mparents)
            mparents = [p[1] for p in mparents]
        elif parents:
            wp = 0

        self.origparents[rev] = parents

        closed = False
        if b'close' in self.commits[rev].extra:
            # A branch closing revision is only useful if one of its
            # parents belong to the branch being closed
            pbranches = [self._cachedcommit(p).branch for p in mparents]
            if branch in pbranches:
                closed = True

        if len(mparents) < 2 and not closed and not self.wanted(rev, wp):
            # We don't want this revision.
            # Update our state and tell the convert process to map this
            # revision to the same revision its parent as mapped to.
            p = None
            if parents:
                p = parents[wp]
            self.mark_not_wanted(rev, p)
            self.convertedorder.append((rev, False, p))
            self._discard(*parents)
            return self.parentmap[rev]

        # We want this revision.
        # Rewrite the parents of the commit object
        self.commits[rev].parents = mparents
        self.mark_wanted(rev, parents)
        self.convertedorder.append((rev, True, None))
        self._discard(*parents)

        # Get the real changes and do the filtering/mapping. To be
        # able to get the files later on in getfile, we hide the
        # original filename in the rev part of the return value.
        changes, copies, cleanp2 = self.base.getchanges(rev, full)
        files = {}
        ncleanp2 = set(cleanp2)
        for f, r in changes:
            newf = self.filemapper(f)
            if newf and (newf != f or newf not in files):
                files[newf] = (f, r)
                if newf != f:
                    ncleanp2.discard(f)
        files = sorted(files.items())

        ncopies = {}
        for c in copies:
            newc = self.filemapper(c)
            if newc:
                newsource = self.filemapper(copies[c])
                if newsource:
                    ncopies[newc] = newsource

        return files, ncopies, ncleanp2
###END###
def targetfilebelongstosource(self, targetfilename):
        return self.filemapper.istargetfile(targetfilename)
###END###
def getfile(self, name, rev):
        realname, realrev = rev
        return self.base.getfile(realname, realrev)
###END###
def gettags(self):
        return self.base.gettags()
###END###
def hasnativeorder(self):
        return self.base.hasnativeorder()
###END###
def lookuprev(self, rev):
        return self.base.lookuprev(rev)
###END###
def getbookmarks(self):
        return self.base.getbookmarks()
###END###
def converted(self, rev, sinkrev):
        self.base.converted(rev, sinkrev)
###END###
def __init__(self, path, node, url):
        self.path = path
        self.node = node
        self.url = url
###END###
def hgsub(self):
        return b"%s = [git]%s" % (self.path, self.url)
###END###
def hgsubstate(self):
        return b"%s %s" % (self.node, self.path)
###END###
def _gitcmd(self, cmd, *args, **kwargs):
        return cmd(b'--git-dir=%s' % self.path, *args, **kwargs)
###END###
def gitrun0(self, *args, **kwargs):
        return self._gitcmd(self.run0, *args, **kwargs)
###END###
def gitrun(self, *args, **kwargs):
        return self._gitcmd(self.run, *args, **kwargs)
###END###
def gitrunlines0(self, *args, **kwargs):
        return self._gitcmd(self.runlines0, *args, **kwargs)
###END###
def gitrunlines(self, *args, **kwargs):
        return self._gitcmd(self.runlines, *args, **kwargs)
###END###
def gitpipe(self, *args, **kwargs):
        return self._gitcmd(self._run3, *args, **kwargs)
###END###
def __init__(self, ui, repotype, path, revs=None):
        super(convert_git, self).__init__(ui, repotype, path, revs=revs)
        common.commandline.__init__(self, ui, b'git')

        # Pass an absolute path to git to prevent from ever being interpreted
        # as a URL
        path = util.abspath(path)

        if os.path.isdir(path + b"/.git"):
            path += b"/.git"
        if not os.path.exists(path + b"/objects"):
            raise common.NoRepo(
                _(b"%s does not look like a Git repository") % path
            )

        # The default value (50) is based on the default for 'git diff'.
        similarity = ui.configint(b'convert', b'git.similarity')
        if similarity < 0 or similarity > 100:
            raise error.Abort(_(b'similarity must be between 0 and 100'))
        if similarity > 0:
            self.simopt = [b'-C%d%%' % similarity]
            findcopiesharder = ui.configbool(
                b'convert', b'git.findcopiesharder'
            )
            if findcopiesharder:
                self.simopt.append(b'--find-copies-harder')

            renamelimit = ui.configint(b'convert', b'git.renamelimit')
            self.simopt.append(b'-l%d' % renamelimit)
        else:
            self.simopt = []

        common.checktool(b'git', b'git')

        self.path = path
        self.submodules = []

        self.catfilepipe = self.gitpipe(b'cat-file', b'--batch')

        self.copyextrakeys = self.ui.configlist(b'convert', b'git.extrakeys')
        banned = set(self.copyextrakeys) & bannedextrakeys
        if banned:
            raise error.Abort(
                _(b'copying of extra key is forbidden: %s')
                % _(b', ').join(sorted(banned))
            )

        committeractions = self.ui.configlist(
            b'convert', b'git.committeractions'
        )

        messagedifferent = None
        messagealways = None
        for a in committeractions:
            if a.startswith((b'messagedifferent', b'messagealways')):
                k = a
                v = None
                if b'=' in a:
                    k, v = a.split(b'=', 1)

                if k == b'messagedifferent':
                    messagedifferent = v or b'committer:'
                elif k == b'messagealways':
                    messagealways = v or b'committer:'

        if messagedifferent and messagealways:
            raise error.Abort(
                _(
                    b'committeractions cannot define both '
                    b'messagedifferent and messagealways'
                )
            )

        dropcommitter = b'dropcommitter' in committeractions
        replaceauthor = b'replaceauthor' in committeractions

        if dropcommitter and replaceauthor:
            raise error.Abort(
                _(
                    b'committeractions cannot define both '
                    b'dropcommitter and replaceauthor'
                )
            )

        if dropcommitter and messagealways:
            raise error.Abort(
                _(
                    b'committeractions cannot define both '
                    b'dropcommitter and messagealways'
                )
            )

        if not messagedifferent and not messagealways:
            messagedifferent = b'committer:'

        self.committeractions = {
            b'dropcommitter': dropcommitter,
            b'replaceauthor': replaceauthor,
            b'messagedifferent': messagedifferent,
            b'messagealways': messagealways,
        }
###END###
def after(self):
        for f in self.catfilepipe:
            f.close()
###END###
def getheads(self):
        if not self.revs:
            output, status = self.gitrun(
                b'rev-parse', b'--branches', b'--remotes'
            )
            heads = output.splitlines()
            if status:
                raise error.Abort(_(b'cannot retrieve git heads'))
        else:
            heads = []
            for rev in self.revs:
                rawhead, ret = self.gitrun(b'rev-parse', b'--verify', rev)
                heads.append(rawhead[:-1])
                if ret:
                    raise error.Abort(_(b'cannot retrieve git head "%s"') % rev)
        return heads
###END###
def catfile(self, rev, ftype):
        if rev == sha1nodeconstants.nullhex:
            raise IOError
        self.catfilepipe[0].write(rev + b'\n')
        self.catfilepipe[0].flush()
        info = self.catfilepipe[1].readline().split()
        if info[1] != ftype:
            raise error.Abort(
                _(b'cannot read %r object at %s')
                % (pycompat.bytestr(ftype), rev)
            )
        size = int(info[2])
        data = self.catfilepipe[1].read(size)
        if len(data) < size:
            raise error.Abort(
                _(b'cannot read %r object at %s: unexpected size')
                % (ftype, rev)
            )
        # read the trailing newline
        self.catfilepipe[1].read(1)
        return data
###END###
def getfile(self, name, rev):
        if rev == sha1nodeconstants.nullhex:
            return None, None
        if name == b'.hgsub':
            data = b'\n'.join([m.hgsub() for m in self.submoditer()])
            mode = b''
        elif name == b'.hgsubstate':
            data = b'\n'.join([m.hgsubstate() for m in self.submoditer()])
            mode = b''
        else:
            data = self.catfile(rev, b"blob")
            mode = self.modecache[(name, rev)]
        return data, mode
###END###
def submoditer(self):
        null = sha1nodeconstants.nullhex
        for m in sorted(self.submodules, key=lambda p: p.path):
            if m.node != null:
                yield m
###END###
def parsegitmodules(self, content):
        """Parse the formatted .gitmodules file, example file format:
        [submodule "sub"]\n
        \tpath = sub\n
        \turl = git://giturl\n
        """
        self.submodules = []
        c = config.config()
        # Each item in .gitmodules starts with whitespace that cant be parsed
        c.parse(
            b'.gitmodules',
            b'\n'.join(line.strip() for line in content.split(b'\n')),
        )
        for sec in c.sections():
            # turn the config object into a real dict
            s = dict(c.items(sec))
            if b'url' in s and b'path' in s:
                self.submodules.append(submodule(s[b'path'], b'', s[b'url']))
###END###
def retrievegitmodules(self, version):
        modules, ret = self.gitrun(
            b'show', b'%s:%s' % (version, b'.gitmodules')
        )
        if ret:
            # This can happen if a file is in the repo that has permissions
            # 160000, but there is no .gitmodules file.
            self.ui.warn(
                _(b"warning: cannot read submodules config file in %s\n")
                % version
            )
            return

        try:
            self.parsegitmodules(modules)
        except error.ParseError:
            self.ui.warn(
                _(b"warning: unable to parse .gitmodules in %s\n") % version
            )
            return

        for m in self.submodules:
            node, ret = self.gitrun(b'rev-parse', b'%s:%s' % (version, m.path))
            if ret:
                continue
            m.node = node.strip()
###END###
def getchanges(self, version, full):
        if full:
            raise error.Abort(_(b"convert from git does not support --full"))
        self.modecache = {}
        cmd = (
            [b'diff-tree', b'-z', b'--root', b'-m', b'-r']
            + self.simopt
            + [version]
        )
        output, status = self.gitrun(*cmd)
        if status:
            raise error.Abort(_(b'cannot read changes in %s') % version)
        changes = []
        copies = {}
        seen = set()
        entry = None
        subexists = [False]
        subdeleted = [False]
        difftree = output.split(b'\x00')
        lcount = len(difftree)
        i = 0

        skipsubmodules = self.ui.configbool(b'convert', b'git.skipsubmodules')

        def add(entry, f, isdest):
            seen.add(f)
            h = entry[3]
            p = entry[1] == b"100755"
            s = entry[1] == b"120000"
            renamesource = not isdest and entry[4][0] == b'R'

            if f == b'.gitmodules':
                if skipsubmodules:
                    return

                subexists[0] = True
                if entry[4] == b'D' or renamesource:
                    subdeleted[0] = True
                    changes.append((b'.hgsub', sha1nodeconstants.nullhex))
                else:
                    changes.append((b'.hgsub', b''))
            elif entry[1] == b'160000' or entry[0] == b':160000':
                if not skipsubmodules:
                    subexists[0] = True
            else:
                if renamesource:
                    h = sha1nodeconstants.nullhex
                self.modecache[(f, h)] = (p and b"x") or (s and b"l") or b""
                changes.append((f, h))

        while i < lcount:
            l = difftree[i]
            i += 1
            if not entry:
                if not l.startswith(b':'):
                    continue
                entry = tuple(pycompat.bytestr(p) for p in l.split())
                continue
            f = l
            if entry[4][0] == b'C':
                copysrc = f
                copydest = difftree[i]
                i += 1
                f = copydest
                copies[copydest] = copysrc
            if f not in seen:
                add(entry, f, False)
            # A file can be copied multiple times, or modified and copied
            # simultaneously. So f can be repeated even if fdest isn't.
            if entry[4][0] == b'R':
                # rename: next line is the destination
                fdest = difftree[i]
                i += 1
                if fdest not in seen:
                    add(entry, fdest, True)
                    # .gitmodules isn't imported at all, so it being copied to
                    # and fro doesn't really make sense
                    if f != b'.gitmodules' and fdest != b'.gitmodules':
                        copies[fdest] = f
            entry = None

        if subexists[0]:
            if subdeleted[0]:
                changes.append((b'.hgsubstate', sha1nodeconstants.nullhex))
            else:
                self.retrievegitmodules(version)
                changes.append((b'.hgsubstate', b''))
        return (changes, copies, set())
###END###
def getcommit(self, version):
        c = self.catfile(version, b"commit")  # read the commit hash
        end = c.find(b"\n\n")
        message = c[end + 2 :]
        message = self.recode(message)
        l = c[:end].splitlines()
        parents = []
        author = committer = None
        extra = {}
        for e in l[1:]:
            n, v = e.split(b" ", 1)
            if n == b"author":
                p = v.split()
                tm, tz = p[-2:]
                author = b" ".join(p[:-2])
                if author[0] == b"<":
                    author = author[1:-1]
                author = self.recode(author)
            if n == b"committer":
                p = v.split()
                tm, tz = p[-2:]
                committer = b" ".join(p[:-2])
                if committer[0] == b"<":
                    committer = committer[1:-1]
                committer = self.recode(committer)
            if n == b"parent":
                parents.append(v)
            if n in self.copyextrakeys:
                extra[n] = v

        if self.committeractions[b'dropcommitter']:
            committer = None
        elif self.committeractions[b'replaceauthor']:
            author = committer

        if committer:
            messagealways = self.committeractions[b'messagealways']
            messagedifferent = self.committeractions[b'messagedifferent']
            if messagealways:
                message += b'\n%s %s\n' % (messagealways, committer)
            elif messagedifferent and author != committer:
                message += b'\n%s %s\n' % (messagedifferent, committer)

        tzs, tzh, tzm = tz[-5:-4] + b"1", tz[-4:-2], tz[-2:]
        tz = -int(tzs) * (int(tzh) * 3600 + int(tzm))
        date = tm + b" " + (b"%d" % tz)
        saverev = self.ui.configbool(b'convert', b'git.saverev')

        c = common.commit(
            parents=parents,
            date=date,
            author=author,
            desc=message,
            rev=version,
            extra=extra,
            saverev=saverev,
        )
        return c
###END###
def numcommits(self):
        output, ret = self.gitrunlines(b'rev-list', b'--all')
        if ret:
            raise error.Abort(
                _(b'cannot retrieve number of commits in %s') % self.path
            )
        return len(output)
###END###
def gettags(self):
        tags = {}
        alltags = {}
        output, status = self.gitrunlines(b'ls-remote', b'--tags', self.path)

        if status:
            raise error.Abort(_(b'cannot read tags from %s') % self.path)
        prefix = b'refs/tags/'

        # Build complete list of tags, both annotated and bare ones
        for line in output:
            line = line.strip()
            if line.startswith(b"error:") or line.startswith(b"fatal:"):
                raise error.Abort(_(b'cannot read tags from %s') % self.path)
            node, tag = line.split(None, 1)
            if not tag.startswith(prefix):
                continue
            alltags[tag[len(prefix) :]] = node

        # Filter out tag objects for annotated tag refs
        for tag in alltags:
            if tag.endswith(b'^{}'):
                tags[tag[:-3]] = alltags[tag]
            else:
                if tag + b'^{}' in alltags:
                    continue
                else:
                    tags[tag] = alltags[tag]

        return tags
###END###
def getchangedfiles(self, version, i):
        changes = []
        if i is None:
            output, status = self.gitrunlines(
                b'diff-tree', b'--root', b'-m', b'-r', version
            )
            if status:
                raise error.Abort(_(b'cannot read changes in %s') % version)
            for l in output:
                if b"\t" not in l:
                    continue
                m, f = l[:-1].split(b"\t")
                changes.append(f)
        else:
            output, status = self.gitrunlines(
                b'diff-tree',
                b'--name-only',
                b'--root',
                b'-r',
                version,
                b'%s^%d' % (version, i + 1),
                b'--',
            )
            if status:
                raise error.Abort(_(b'cannot read changes in %s') % version)
            changes = [f.rstrip(b'\n') for f in output]

        return changes
###END###
def getbookmarks(self):
        bookmarks = {}

        # Handle local and remote branches
        remoteprefix = self.ui.config(b'convert', b'git.remoteprefix')
        reftypes = [
            # (git prefix, hg prefix)
            (b'refs/remotes/origin/', remoteprefix + b'/'),
            (b'refs/heads/', b''),
        ]

        exclude = {
            b'refs/remotes/origin/HEAD',
        }

        try:
            output, status = self.gitrunlines(b'show-ref')
            for line in output:
                line = line.strip()
                rev, name = line.split(None, 1)
                # Process each type of branch
                for gitprefix, hgprefix in reftypes:
                    if not name.startswith(gitprefix) or name in exclude:
                        continue
                    name = b'%s%s' % (hgprefix, name[len(gitprefix) :])
                    bookmarks[name] = rev
        except Exception:
            pass

        return bookmarks
###END###
def checkrevformat(self, revstr, mapname=b'splicemap'):
        """git revision string is a 40 byte hex"""
        self.checkhexformat(revstr, mapname)
###END###
def __init__(self, ui, repotype, path, revs=None):
        # avoid import cycle
        from . import convcmd

        super(p4_source, self).__init__(ui, repotype, path, revs=revs)

        if b"/" in path and not path.startswith(b'//'):
            raise common.NoRepo(
                _(b'%s does not look like a P4 repository') % path
            )

        common.checktool(b'p4', abort=False)

        self.revmap = {}
        self.encoding = self.ui.config(
            b'convert', b'p4.encoding', convcmd.orig_encoding
        )
        self.re_type = re.compile(
            br"([a-z]+)?(text|binary|symlink|apple|resource|unicode|utf\d+)"
            br"(\+\w+)?$"
        )
        self.re_keywords = re.compile(
            br"\$(Id|Header|Date|DateTime|Change|File|Revision|Author)"
            br":[^$\n]*\$"
        )
        self.re_keywords_old = re.compile(br"\$(Id|Header):[^$\n]*\$")

        if revs and len(revs) > 1:
            raise error.Abort(
                _(
                    b"p4 source does not support specifying "
                    b"multiple revisions"
                )
            )
###END###
def setrevmap(self, revmap):
        """Sets the parsed revmap dictionary.

        Revmap stores mappings from a source revision to a target revision.
        It is set in convertcmd.convert and provided by the user as a file
        on the commandline.

        Revisions in the map are considered beeing present in the
        repository and ignored during _parse(). This allows for incremental
        imports if a revmap is provided.
        """
        self.revmap = revmap
###END###
def _parse_view(self, path):
        """Read changes affecting the path"""
        cmd = b'p4 -G changes -s submitted %s' % procutil.shellquote(path)
        stdout = procutil.popen(cmd, mode=b'rb')
        p4changes = {}
        for d in loaditer(stdout):
            c = d.get(b"change", None)
            if c:
                p4changes[c] = True
        return p4changes
###END###
def _parse(self, ui, path):
        """Prepare list of P4 filenames and revisions to import"""
        p4changes = {}
        changeset = {}
        files_map = {}
        copies_map = {}
        localname = {}
        depotname = {}
        heads = []

        ui.status(_(b'reading p4 views\n'))

        # read client spec or view
        if b"/" in path:
            p4changes.update(self._parse_view(path))
            if path.startswith(b"//") and path.endswith(b"/..."):
                views = {path[:-3]: b""}
            else:
                views = {b"//": b""}
        else:
            cmd = b'p4 -G client -o %s' % procutil.shellquote(path)
            clientspec = marshal.load(procutil.popen(cmd, mode=b'rb'))

            views = {}
            for client in clientspec:
                if client.startswith(b"View"):
                    sview, cview = clientspec[client].split()
                    p4changes.update(self._parse_view(sview))
                    if sview.endswith(b"...") and cview.endswith(b"..."):
                        sview = sview[:-3]
                        cview = cview[:-3]
                    cview = cview[2:]
                    cview = cview[cview.find(b"/") + 1 :]
                    views[sview] = cview

        # list of changes that affect our source files
        p4changes = sorted(p4changes.keys(), key=int)

        # list with depot pathnames, longest first
        vieworder = sorted(views.keys(), key=len, reverse=True)

        # handle revision limiting
        startrev = self.ui.config(b'convert', b'p4.startrev')

        # now read the full changelists to get the list of file revisions
        ui.status(_(b'collecting p4 changelists\n'))
        lastid = None
        for change in p4changes:
            if startrev and int(change) < int(startrev):
                continue
            if self.revs and int(change) > int(self.revs[0]):
                continue
            if change in self.revmap:
                # Ignore already present revisions, but set the parent pointer.
                lastid = change
                continue

            if lastid:
                parents = [lastid]
            else:
                parents = []

            d = self._fetch_revision(change)
            c = self._construct_commit(d, parents)

            descarr = c.desc.splitlines(True)
            if len(descarr) > 0:
                shortdesc = descarr[0].rstrip(b'\r\n')
            else:
                shortdesc = b'**empty changelist description**'

            t = b'%s %s' % (c.rev, shortdesc)
            ui.status(stringutil.ellipsis(t, 80) + b'\n')

            files = []
            copies = {}
            copiedfiles = []
            i = 0
            while (b"depotFile%d" % i) in d and (b"rev%d" % i) in d:
                oldname = d[b"depotFile%d" % i]
                filename = None
                for v in vieworder:
                    if oldname.lower().startswith(v.lower()):
                        filename = decodefilename(views[v] + oldname[len(v) :])
                        break
                if filename:
                    files.append((filename, d[b"rev%d" % i]))
                    depotname[filename] = oldname
                    if d.get(b"action%d" % i) == b"move/add":
                        copiedfiles.append(filename)
                    localname[oldname] = filename
                i += 1

            # Collect information about copied files
            for filename in copiedfiles:
                oldname = depotname[filename]

                flcmd = b'p4 -G filelog %s' % procutil.shellquote(oldname)
                flstdout = procutil.popen(flcmd, mode=b'rb')

                copiedfilename = None
                for d in loaditer(flstdout):
                    copiedoldname = None

                    i = 0
                    while (b"change%d" % i) in d:
                        if (
                            d[b"change%d" % i] == change
                            and d[b"action%d" % i] == b"move/add"
                        ):
                            j = 0
                            while (b"file%d,%d" % (i, j)) in d:
                                if d[b"how%d,%d" % (i, j)] == b"moved from":
                                    copiedoldname = d[b"file%d,%d" % (i, j)]
                                    break
                                j += 1
                        i += 1

                    if copiedoldname and copiedoldname in localname:
                        copiedfilename = localname[copiedoldname]
                        break

                if copiedfilename:
                    copies[filename] = copiedfilename
                else:
                    ui.warn(
                        _(b"cannot find source for copied file: %s@%s\n")
                        % (filename, change)
                    )

            changeset[change] = c
            files_map[change] = files
            copies_map[change] = copies
            lastid = change

        if lastid and len(changeset) > 0:
            heads = [lastid]

        return {
            b'changeset': changeset,
            b'files': files_map,
            b'copies': copies_map,
            b'heads': heads,
            b'depotname': depotname,
        }
###END###
def _parse_once(self):
        return self._parse(self.ui, self.path)
###END###
def copies(self):
        return self._parse_once[b'copies']
###END###
def files(self):
        return self._parse_once[b'files']
###END###
def changeset(self):
        return self._parse_once[b'changeset']
###END###
def heads(self):
        return self._parse_once[b'heads']
###END###
def depotname(self):
        return self._parse_once[b'depotname']
###END###
def getheads(self):
        return self.heads
###END###
def getfile(self, name, rev):
        cmd = b'p4 -G print %s' % procutil.shellquote(
            b"%s#%s" % (self.depotname[name], rev)
        )

        lasterror = None
        while True:
            stdout = procutil.popen(cmd, mode=b'rb')

            mode = None
            contents = []
            keywords = None

            for d in loaditer(stdout):
                code = d[b"code"]
                data = d.get(b"data")

                if code == b"error":
                    # if this is the first time error happened
                    # re-attempt getting the file
                    if not lasterror:
                        lasterror = IOError(d[b"generic"], data)
                        # this will exit inner-most for-loop
                        break
                    else:
                        raise lasterror

                elif code == b"stat":
                    action = d.get(b"action")
                    if action in [b"purge", b"delete", b"move/delete"]:
                        return None, None
                    p4type = self.re_type.match(d[b"type"])
                    if p4type:
                        mode = b""
                        flags = (p4type.group(1) or b"") + (
                            p4type.group(3) or b""
                        )
                        if b"x" in flags:
                            mode = b"x"
                        if p4type.group(2) == b"symlink":
                            mode = b"l"
                        if b"ko" in flags:
                            keywords = self.re_keywords_old
                        elif b"k" in flags:
                            keywords = self.re_keywords

                elif code == b"text" or code == b"binary":
                    contents.append(data)

                lasterror = None

            if not lasterror:
                break

        if mode is None:
            return None, None

        contents = b''.join(contents)

        if keywords:
            contents = keywords.sub(b"$\\1$", contents)
        if mode == b"l" and contents.endswith(b"\n"):
            contents = contents[:-1]

        return contents, mode
###END###
def getchanges(self, rev, full):
        if full:
            raise error.Abort(_(b"convert from p4 does not support --full"))
        return self.files[rev], self.copies[rev], set()
###END###
def _construct_commit(self, obj, parents=None):
        """
        Constructs a common.commit object from an unmarshalled
        `p4 describe` output
        """
        desc = self.recode(obj.get(b"desc", b""))
        date = (int(obj[b"time"]), 0)  # timezone not set
        if parents is None:
            parents = []

        return common.commit(
            author=self.recode(obj[b"user"]),
            date=dateutil.datestr(date, b'%Y-%m-%d %H:%M:%S %1%2'),
            parents=parents,
            desc=desc,
            branch=None,
            rev=obj[b'change'],
            extra={b"p4": obj[b'change'], b"convert_revision": obj[b'change']},
        )
###END###
def _fetch_revision(self, rev):
        """Return an output of `p4 describe` including author, commit date as
        a dictionary."""
        cmd = b"p4 -G describe -s %s" % rev
        stdout = procutil.popen(cmd, mode=b'rb')
        return marshal.load(stdout)
###END###
def getcommit(self, rev):
        if rev in self.changeset:
            return self.changeset[rev]
        elif rev in self.revmap:
            d = self._fetch_revision(rev)
            return self._construct_commit(d, parents=None)
        raise error.Abort(
            _(b"cannot find %s in the revmap or parsed changesets") % rev
        )
###END###
def gettags(self):
        return {}
###END###
def getchangedfiles(self, rev, i):
        return sorted([x[0] for x in self.files[rev]])
###END###
def __init__(self, ui, repotype, path, revs=None):
        super(gnuarch_source, self).__init__(ui, repotype, path, revs=revs)

        if not os.path.exists(os.path.join(path, b'{arch}')):
            raise common.NoRepo(
                _(b"%s does not look like a GNU Arch repository") % path
            )

        # Could use checktool, but we want to check for baz or tla.
        self.execmd = None
        if procutil.findexe(b'baz'):
            self.execmd = b'baz'
        else:
            if procutil.findexe(b'tla'):
                self.execmd = b'tla'
            else:
                raise error.Abort(_(b'cannot find a GNU Arch tool'))

        common.commandline.__init__(self, ui, self.execmd)

        self.path = os.path.realpath(path)
        self.tmppath = None

        self.treeversion = None
        self.lastrev = None
        self.changes = {}
        self.parents = {}
        self.tags = {}
        self.encoding = encoding.encoding
        self.archives = []
###END###
def before(self):
        # Get registered archives
        self.archives = [
            i.rstrip(b'\n') for i in self.runlines0(b'archives', b'-n')
        ]

        if self.execmd == b'tla':
            output = self.run0(b'tree-version', self.path)
        else:
            output = self.run0(b'tree-version', b'-d', self.path)
        self.treeversion = output.strip()

        # Get name of temporary directory
        version = self.treeversion.split(b'/')
        self.tmppath = os.path.join(
            pycompat.fsencode(tempfile.gettempdir()), b'hg-%s' % version[1]
        )

        # Generate parents dictionary
        self.parents[None] = []
        treeversion = self.treeversion
        child = None
        while treeversion:
            self.ui.status(_(b'analyzing tree version %s...\n') % treeversion)

            archive = treeversion.split(b'/')[0]
            if archive not in self.archives:
                self.ui.status(
                    _(
                        b'tree analysis stopped because it points to '
                        b'an unregistered archive %s...\n'
                    )
                    % archive
                )
                break

            # Get the complete list of revisions for that tree version
            output, status = self.runlines(
                b'revisions', b'-r', b'-f', treeversion
            )
            self.checkexit(
                status, b'failed retrieving revisions for %s' % treeversion
            )

            # No new iteration unless a revision has a continuation-of header
            treeversion = None

            for l in output:
                rev = l.strip()
                self.changes[rev] = self.gnuarch_rev(rev)
                self.parents[rev] = []

                # Read author, date and summary
                catlog, status = self.run(b'cat-log', b'-d', self.path, rev)
                if status:
                    catlog = self.run0(b'cat-archive-log', rev)
                self._parsecatlog(catlog, rev)

                # Populate the parents map
                self.parents[child].append(rev)

                # Keep track of the current revision as the child of the next
                # revision scanned
                child = rev

                # Check if we have to follow the usual incremental history
                # or if we have to 'jump' to a different treeversion given
                # by the continuation-of header.
                if self.changes[rev].continuationof:
                    treeversion = b'--'.join(
                        self.changes[rev].continuationof.split(b'--')[:-1]
                    )
                    break

                # If we reached a base-0 revision w/o any continuation-of
                # header, it means the tree history ends here.
                if rev[-6:] == b'base-0':
                    break
###END###
def after(self):
        self.ui.debug(b'cleaning up %s\n' % self.tmppath)
        shutil.rmtree(self.tmppath, ignore_errors=True)
###END###
def getheads(self):
        return self.parents[None]
###END###
def getfile(self, name, rev):
        if rev != self.lastrev:
            raise error.Abort(_(b'internal calling inconsistency'))

        if not os.path.lexists(os.path.join(self.tmppath, name)):
            return None, None

        return self._getfile(name, rev)
###END###
def getchanges(self, rev, full):
        if full:
            raise error.Abort(_(b"convert from arch does not support --full"))
        self._update(rev)
        changes = []
        copies = {}

        for f in self.changes[rev].add_files:
            changes.append((f, rev))

        for f in self.changes[rev].mod_files:
            changes.append((f, rev))

        for f in self.changes[rev].del_files:
            changes.append((f, rev))

        for src in self.changes[rev].ren_files:
            to = self.changes[rev].ren_files[src]
            changes.append((src, rev))
            changes.append((to, rev))
            copies[to] = src

        for src in self.changes[rev].ren_dirs:
            to = self.changes[rev].ren_dirs[src]
            chgs, cps = self._rendirchanges(src, to)
            changes += [(f, rev) for f in chgs]
            copies.update(cps)

        self.lastrev = rev
        return sorted(set(changes)), copies, set()
###END###
def getcommit(self, rev):
        changes = self.changes[rev]
        return common.commit(
            author=changes.author,
            date=changes.date,
            desc=changes.summary,
            parents=self.parents[rev],
            rev=rev,
        )
###END###
def gettags(self):
        return self.tags
###END###
def _execute(self, cmd, *args, **kwargs):
        cmdline = [self.execmd, cmd]
        cmdline += args
        cmdline = [procutil.shellquote(arg) for arg in cmdline]
        bdevnull = pycompat.bytestr(os.devnull)
        cmdline += [b'>', bdevnull, b'2>', bdevnull]
        cmdline = b' '.join(cmdline)
        self.ui.debug(cmdline, b'\n')
        return os.system(pycompat.rapply(procutil.tonativestr, cmdline))
###END###
def _update(self, rev):
        self.ui.debug(b'applying revision %s...\n' % rev)
        changeset, status = self.runlines(b'replay', b'-d', self.tmppath, rev)
        if status:
            # Something went wrong while merging (baz or tla
            # issue?), get latest revision and try from there
            shutil.rmtree(self.tmppath, ignore_errors=True)
            self._obtainrevision(rev)
        else:
            old_rev = self.parents[rev][0]
            self.ui.debug(
                b'computing changeset between %s and %s...\n' % (old_rev, rev)
            )
            self._parsechangeset(changeset, rev)
###END###
def _getfile(self, name, rev):
        mode = os.lstat(os.path.join(self.tmppath, name)).st_mode
        if stat.S_ISLNK(mode):
            data = util.readlink(os.path.join(self.tmppath, name))
            if mode:
                mode = b'l'
            else:
                mode = b''
        else:
            data = util.readfile(os.path.join(self.tmppath, name))
            mode = (mode & 0o111) and b'x' or b''
        return data, mode
###END###
def _exclude(self, name):
        exclude = [b'{arch}', b'.arch-ids', b'.arch-inventory']
        for exc in exclude:
            if name.find(exc) != -1:
                return True
        return False
###END###
def _readcontents(self, path):
        files = []
        contents = os.listdir(path)
        while len(contents) > 0:
            c = contents.pop()
            p = os.path.join(path, c)
            # os.walk could be used, but here we avoid internal GNU
            # Arch files and directories, thus saving a lot time.
            if not self._exclude(p):
                if os.path.isdir(p):
                    contents += [os.path.join(c, f) for f in os.listdir(p)]
                else:
                    files.append(c)
        return files
###END###
def _rendirchanges(self, src, dest):
        changes = []
        copies = {}
        files = self._readcontents(os.path.join(self.tmppath, dest))
        for f in files:
            s = os.path.join(src, f)
            d = os.path.join(dest, f)
            changes.append(s)
            changes.append(d)
            copies[d] = s
        return changes, copies
###END###
def _obtainrevision(self, rev):
        self.ui.debug(b'obtaining revision %s...\n' % rev)
        output = self._execute(b'get', rev, self.tmppath)
        self.checkexit(output)
        self.ui.debug(b'analyzing revision %s...\n' % rev)
        files = self._readcontents(self.tmppath)
        self.changes[rev].add_files += files
###END###
def _stripbasepath(self, path):
        if path.startswith(b'./'):
            return path[2:]
        return path
###END###
def _parsecatlog(self, data, rev):
        try:
            catlog = mail.parsebytes(data)

            # Commit date
            self.changes[rev].date = dateutil.datestr(
                dateutil.strdate(catlog['Standard-date'], b'%Y-%m-%d %H:%M:%S')
            )

            # Commit author
            self.changes[rev].author = self.recode(catlog['Creator'])

            # Commit description
            self.changes[rev].summary = b'\n\n'.join(
                (
                    self.recode(catlog['Summary']),
                    self.recode(catlog.get_payload()),
                )
            )
            self.changes[rev].summary = self.recode(self.changes[rev].summary)

            # Commit revision origin when dealing with a branch or tag
            if 'Continuation-of' in catlog:
                self.changes[rev].continuationof = self.recode(
                    catlog['Continuation-of']
                )
        except Exception:
            raise error.Abort(_(b'could not parse cat-log of %s') % rev)
###END###
def _parsechangeset(self, data, rev):
        for l in data:
            l = l.strip()
            # Added file (ignore added directory)
            if l.startswith(b'A') and not l.startswith(b'A/'):
                file = self._stripbasepath(l[1:].strip())
                if not self._exclude(file):
                    self.changes[rev].add_files.append(file)
            # Deleted file (ignore deleted directory)
            elif l.startswith(b'D') and not l.startswith(b'D/'):
                file = self._stripbasepath(l[1:].strip())
                if not self._exclude(file):
                    self.changes[rev].del_files.append(file)
            # Modified binary file
            elif l.startswith(b'Mb'):
                file = self._stripbasepath(l[2:].strip())
                if not self._exclude(file):
                    self.changes[rev].mod_files.append(file)
            # Modified link
            elif l.startswith(b'M->'):
                file = self._stripbasepath(l[3:].strip())
                if not self._exclude(file):
                    self.changes[rev].mod_files.append(file)
            # Modified file
            elif l.startswith(b'M'):
                file = self._stripbasepath(l[1:].strip())
                if not self._exclude(file):
                    self.changes[rev].mod_files.append(file)
            # Renamed file (or link)
            elif l.startswith(b'=>'):
                files = l[2:].strip().split(b' ')
                if len(files) == 1:
                    files = l[2:].strip().split(b'\t')
                src = self._stripbasepath(files[0])
                dst = self._stripbasepath(files[1])
                if not self._exclude(src) and not self._exclude(dst):
                    self.changes[rev].ren_files[src] = dst
            # Conversion from file to link or from link to file (modified)
            elif l.startswith(b'ch'):
                file = self._stripbasepath(l[2:].strip())
                if not self._exclude(file):
                    self.changes[rev].mod_files.append(file)
            # Renamed directory
            elif l.startswith(b'/>'):
                dirs = l[2:].strip().split(b' ')
                if len(dirs) == 1:
                    dirs = l[2:].strip().split(b'\t')
                src = self._stripbasepath(dirs[0])
                dst = self._stripbasepath(dirs[1])
                if not self._exclude(src) and not self._exclude(dst):
                    self.changes[rev].ren_dirs[src] = dst
###END###
def __init__(self, rev):
            self.rev = rev
            self.summary = b''
            self.date = None
            self.author = b''
            self.continuationof = None
            self.add_files = []
            self.mod_files = []
            self.del_files = []
            self.ren_files = {}
            self.ren_dirs = {}
###END###
def __init__(self, ui, repotype, path, revs=None):
        common.converter_source.__init__(self, ui, repotype, path, revs=revs)
        common.commandline.__init__(self, ui, b'darcs')

        # check for _darcs, ElementTree so that we can easily skip
        # test-convert-darcs if ElementTree is not around
        if not os.path.exists(os.path.join(path, b'_darcs')):
            raise NoRepo(_(b"%s does not look like a darcs repository") % path)

        common.checktool(b'darcs')
        version = self.run0(b'--version').splitlines()[0].strip()
        if version < b'2.1':
            raise error.Abort(
                _(b'darcs version 2.1 or newer needed (found %r)') % version
            )

        if b"ElementTree" not in globals():
            raise error.Abort(_(b"Python ElementTree module is not available"))

        self.path = os.path.realpath(path)

        self.lastrev = None
        self.changes = {}
        self.parents = {}
        self.tags = {}

        # Check darcs repository format
        format = self.format()
        if format:
            if format in (b'darcs-1.0', b'hashed'):
                raise NoRepo(
                    _(
                        b"%s repository format is unsupported, "
                        b"please upgrade"
                    )
                    % format
                )
        else:
            self.ui.warn(_(b'failed to detect repository format!'))
###END###
def before(self):
        self.tmppath = pycompat.mkdtemp(
            prefix=b'convert-' + os.path.basename(self.path) + b'-'
        )
        output, status = self.run(b'init', repodir=self.tmppath)
        self.checkexit(status)

        tree = self.xml(
            b'changes', xml_output=True, summary=True, repodir=self.path
        )
        tagname = None
        child = None
        for elt in tree.findall(b'patch'):
            node = elt.get(b'hash')
            name = elt.findtext(b'name', b'')
            if name.startswith(b'TAG '):
                tagname = name[4:].strip()
            elif tagname is not None:
                self.tags[tagname] = node
                tagname = None
            self.changes[node] = elt
            self.parents[child] = [node]
            child = node
        self.parents[child] = []
###END###
def after(self):
        self.ui.debug(b'cleaning up %s\n' % self.tmppath)
        shutil.rmtree(self.tmppath, ignore_errors=True)
###END###
def recode(self, s, encoding=None):
        if isinstance(s, pycompat.unicode):
            # XMLParser returns unicode objects for anything it can't
            # encode into ASCII. We convert them back to str to get
            # recode's normal conversion behavior.
            s = s.encode('latin-1')
        return super(darcs_source, self).recode(s, encoding)
###END###
def xml(self, cmd, **kwargs):
        # NOTE: darcs is currently encoding agnostic and will print
        # patch metadata byte-for-byte, even in the XML changelog.
        etree = ElementTree()
        # While we are decoding the XML as latin-1 to be as liberal as
        # possible, etree will still raise an exception if any
        # non-printable characters are in the XML changelog.
        parser = XMLParser(encoding=b'latin-1')
        p = self._run(cmd, **kwargs)
        etree.parse(p.stdout, parser=parser)
        p.wait()
        self.checkexit(p.returncode)
        return etree.getroot()
###END###
def format(self):
        output, status = self.run(b'show', b'repo', repodir=self.path)
        self.checkexit(status)
        m = re.search(r'^\s*Format:\s*(.*)$', output, re.MULTILINE)
        if not m:
            return None
        return b','.join(sorted(f.strip() for f in m.group(1).split(b',')))
###END###
def manifest(self):
        man = []
        output, status = self.run(
            b'show', b'files', no_directories=True, repodir=self.tmppath
        )
        self.checkexit(status)
        for line in output.split(b'\n'):
            path = line[2:]
            if path:
                man.append(path)
        return man
###END###
def getheads(self):
        return self.parents[None]
###END###
def getcommit(self, rev):
        elt = self.changes[rev]
        dateformat = b'%a %b %d %H:%M:%S %Z %Y'
        date = dateutil.strdate(elt.get(b'local_date'), dateformat)
        desc = elt.findtext(b'name') + b'\n' + elt.findtext(b'comment', b'')
        # etree can return unicode objects for name, comment, and author,
        # so recode() is used to ensure str objects are emitted.
        newdateformat = b'%Y-%m-%d %H:%M:%S %1%2'
        return common.commit(
            author=self.recode(elt.get(b'author')),
            date=dateutil.datestr(date, newdateformat),
            desc=self.recode(desc).strip(),
            parents=self.parents[rev],
        )
###END###
def pull(self, rev):
        output, status = self.run(
            b'pull',
            self.path,
            all=True,
            match=b'hash %s' % rev,
            no_test=True,
            no_posthook=True,
            external_merge=b'/bin/false',
            repodir=self.tmppath,
        )
        if status:
            if output.find(b'We have conflicts in') == -1:
                self.checkexit(status, output)
            output, status = self.run(b'revert', all=True, repodir=self.tmppath)
            self.checkexit(status, output)
###END###
def getchanges(self, rev, full):
        if full:
            raise error.Abort(_(b"convert from darcs does not support --full"))
        copies = {}
        changes = []
        man = None
        for elt in self.changes[rev].find(b'summary').getchildren():
            if elt.tag in (b'add_directory', b'remove_directory'):
                continue
            if elt.tag == b'move':
                if man is None:
                    man = self.manifest()
                source, dest = elt.get(b'from'), elt.get(b'to')
                if source in man:
                    # File move
                    changes.append((source, rev))
                    changes.append((dest, rev))
                    copies[dest] = source
                else:
                    # Directory move, deduce file moves from manifest
                    source = source + b'/'
                    for f in man:
                        if not f.startswith(source):
                            continue
                        fdest = dest + b'/' + f[len(source) :]
                        changes.append((f, rev))
                        changes.append((fdest, rev))
                        copies[fdest] = f
            else:
                changes.append((elt.text.strip(), rev))
        self.pull(rev)
        self.lastrev = rev
        return sorted(changes), copies, set()
###END###
def getfile(self, name, rev):
        if rev != self.lastrev:
            raise error.Abort(_(b'internal calling inconsistency'))
        path = os.path.join(self.tmppath, name)
        try:
            data = util.readfile(path)
            mode = os.lstat(path).st_mode
        except IOError as inst:
            if inst.errno == errno.ENOENT:
                return None, None
            raise
        mode = (mode & 0o111) and b'x' or b''
        return data, mode
###END###
def gettags(self):
        return self.tags
###END###
def __init__(self):
        self.pipeo = self.pipei = self.pipee = None
        self.subprocess = None
        self.connected = False
###END###
def connect(self, cachecommand):
        if self.pipeo:
            raise error.Abort(_(b"cache connection already open"))
        self.pipei, self.pipeo, self.pipee, self.subprocess = procutil.popen4(
            cachecommand
        )
        self.connected = True
###END###
def close(self):
        def tryclose(pipe):
            try:
                pipe.close()
            except Exception:
                pass

        if self.connected:
            try:
                self.pipei.write(b"exit\n")
            except Exception:
                pass
            tryclose(self.pipei)
            self.pipei = None
            tryclose(self.pipeo)
            self.pipeo = None
            tryclose(self.pipee)
            self.pipee = None
            try:
                # Wait for process to terminate, making sure to avoid deadlock.
                # See https://docs.python.org/2/library/subprocess.html for
                # warnings about wait() and deadlocking.
                self.subprocess.communicate()
            except Exception:
                pass
            self.subprocess = None
        self.connected = False
###END###
def request(self, request, flush=True):
        if self.connected:
            try:
                self.pipei.write(request)
                if flush:
                    self.pipei.flush()
            except IOError:
                self.close()
###END###
def receiveline(self):
        if not self.connected:
            return None
        try:
            result = self.pipeo.readline()[:-1]
            if not result:
                self.close()
        except IOError:
            self.close()

        return result
###END###
def __init__(self, repo):
        ui = repo.ui
        self.repo = repo
        self.ui = ui
        self.cacheprocess = ui.config(b"remotefilelog", b"cacheprocess")
        if self.cacheprocess:
            self.cacheprocess = util.expandpath(self.cacheprocess)

        # This option causes remotefilelog to pass the full file path to the
        # cacheprocess instead of a hashed key.
        self.cacheprocesspasspath = ui.configbool(
            b"remotefilelog", b"cacheprocess.includepath"
        )

        self.debugoutput = ui.configbool(b"remotefilelog", b"debug")

        self.remotecache = cacheconnection()
###END###
def setstore(self, datastore, historystore, writedata, writehistory):
        self.datastore = datastore
        self.historystore = historystore
        self.writedata = writedata
        self.writehistory = writehistory
###END###
def _connect(self):
        return self.repo.connectionpool.get(self.repo.fallbackpath)
###END###
def request(self, fileids):
        """Takes a list of filename/node pairs and fetches them from the
        server. Files are stored in the local cache.
        A list of nodes that the server couldn't find is returned.
        If the connection fails, an exception is raised.
        """
        if not self.remotecache.connected:
            self.connect()
        cache = self.remotecache
        writedata = self.writedata

        repo = self.repo
        total = len(fileids)
        request = b"get\n%d\n" % total
        idmap = {}
        reponame = repo.name
        for file, id in fileids:
            fullid = getcachekey(reponame, file, id)
            if self.cacheprocesspasspath:
                request += file + b'\0'
            request += fullid + b"\n"
            idmap[fullid] = file

        cache.request(request)

        progress = self.ui.makeprogress(_(b'downloading'), total=total)
        progress.update(0)

        missed = []
        while True:
            missingid = cache.receiveline()
            if not missingid:
                missedset = set(missed)
                for missingid in idmap:
                    if not missingid in missedset:
                        missed.append(missingid)
                self.ui.warn(
                    _(
                        b"warning: cache connection closed early - "
                        + b"falling back to server\n"
                    )
                )
                break
            if missingid == b"0":
                break
            if missingid.startswith(b"_hits_"):
                # receive progress reports
                parts = missingid.split(b"_")
                progress.increment(int(parts[2]))
                continue

            missed.append(missingid)

        global fetchmisses
        fetchmisses += len(missed)

        fromcache = total - len(missed)
        progress.update(fromcache, total=total)
        self.ui.log(
            b"remotefilelog",
            b"remote cache hit rate is %r of %r\n",
            fromcache,
            total,
            hit=fromcache,
            total=total,
        )

        oldumask = os.umask(0o002)
        try:
            # receive cache misses from master
            if missed:
                # When verbose is true, sshpeer prints 'running ssh...'
                # to stdout, which can interfere with some command
                # outputs
                verbose = self.ui.verbose
                self.ui.verbose = False
                try:
                    with self._connect() as conn:
                        remote = conn.peer
                        if remote.capable(
                            constants.NETWORK_CAP_LEGACY_SSH_GETFILES
                        ):
                            if not isinstance(remote, _sshv1peer):
                                raise error.Abort(
                                    b'remotefilelog requires ssh servers'
                                )
                            step = self.ui.configint(
                                b'remotefilelog', b'getfilesstep'
                            )
                            getfilestype = self.ui.config(
                                b'remotefilelog', b'getfilestype'
                            )
                            if getfilestype == b'threaded':
                                _getfiles = _getfiles_threaded
                            else:
                                _getfiles = _getfiles_optimistic
                            _getfiles(
                                remote,
                                self.receivemissing,
                                progress.increment,
                                missed,
                                idmap,
                                step,
                            )
                        elif remote.capable(b"x_rfl_getfile"):
                            if remote.capable(b'batch'):
                                batchdefault = 100
                            else:
                                batchdefault = 10
                            batchsize = self.ui.configint(
                                b'remotefilelog', b'batchsize', batchdefault
                            )
                            self.ui.debug(
                                b'requesting %d files from '
                                b'remotefilelog server...\n' % len(missed)
                            )
                            _getfilesbatch(
                                remote,
                                self.receivemissing,
                                progress.increment,
                                missed,
                                idmap,
                                batchsize,
                            )
                        else:
                            raise error.Abort(
                                b"configured remotefilelog server"
                                b" does not support remotefilelog"
                            )

                    self.ui.log(
                        b"remotefilefetchlog",
                        b"Success\n",
                        fetched_files=progress.pos - fromcache,
                        total_to_fetch=total - fromcache,
                    )
                except Exception:
                    self.ui.log(
                        b"remotefilefetchlog",
                        b"Fail\n",
                        fetched_files=progress.pos - fromcache,
                        total_to_fetch=total - fromcache,
                    )
                    raise
                finally:
                    self.ui.verbose = verbose
                # send to memcache
                request = b"set\n%d\n%s\n" % (len(missed), b"\n".join(missed))
                cache.request(request)

            progress.complete()

            # mark ourselves as a user of this cache
            writedata.markrepo(self.repo.path)
        finally:
            os.umask(oldumask)
###END###
def receivemissing(self, pipe, filename, node):
        line = pipe.readline()[:-1]
        if not line:
            raise error.ResponseError(
                _(b"error downloading file contents:"),
                _(b"connection closed early"),
            )
        size = int(line)
        data = pipe.read(size)
        if len(data) != size:
            raise error.ResponseError(
                _(b"error downloading file contents:"),
                _(b"only received %s of %s bytes") % (len(data), size),
            )

        self.writedata.addremotefilelognode(
            filename, bin(node), zlib.decompress(data)
        )
###END###
def connect(self):
        if self.cacheprocess:
            cmd = b"%s %s" % (self.cacheprocess, self.writedata._path)
            self.remotecache.connect(cmd)
        else:
            # If no cache process is specified, we fake one that always
            # returns cache misses.  This enables tests to run easily
            # and may eventually allow us to be a drop in replacement
            # for the largefiles extension.
            class simplecache(object):
                def __init__(self):
                    self.missingids = []
                    self.connected = True

                def close(self):
                    pass

                def request(self, value, flush=True):
                    lines = value.split(b"\n")
                    if lines[0] != b"get":
                        return
                    self.missingids = lines[2:-1]
                    self.missingids.append(b'0')

                def receiveline(self):
                    if len(self.missingids) > 0:
                        return self.missingids.pop(0)
                    return None

            self.remotecache = simplecache()
###END###
def close(self):
        if fetches:
            msg = (
                b"%d files fetched over %d fetches - "
                + b"(%d misses, %0.2f%% hit ratio) over %0.2fs\n"
            ) % (
                fetched,
                fetches,
                fetchmisses,
                float(fetched - fetchmisses) / float(fetched) * 100.0,
                fetchcost,
            )
            if self.debugoutput:
                self.ui.warn(msg)
            self.ui.log(
                b"remotefilelog.prefetch",
                msg.replace(b"%", b"%%"),
                remotefilelogfetched=fetched,
                remotefilelogfetches=fetches,
                remotefilelogfetchmisses=fetchmisses,
                remotefilelogfetchtime=fetchcost * 1000,
            )

        if self.remotecache.connected:
            self.remotecache.close()
###END###
def prefetch(
        self, fileids, force=False, fetchdata=True, fetchhistory=False
    ):
        """downloads the given file versions to the cache"""
        repo = self.repo
        idstocheck = []
        for file, id in fileids:
            # hack
            # - we don't use .hgtags
            # - workingctx produces ids with length 42,
            #   which we skip since they aren't in any cache
            if (
                file == b'.hgtags'
                or len(id) == 42
                or not repo.shallowmatch(file)
            ):
                continue

            idstocheck.append((file, bin(id)))

        datastore = self.datastore
        historystore = self.historystore
        if force:
            datastore = contentstore.unioncontentstore(*repo.shareddatastores)
            historystore = metadatastore.unionmetadatastore(
                *repo.sharedhistorystores
            )

        missingids = set()
        if fetchdata:
            missingids.update(datastore.getmissing(idstocheck))
        if fetchhistory:
            missingids.update(historystore.getmissing(idstocheck))

        # partition missing nodes into nullid and not-nullid so we can
        # warn about this filtering potentially shadowing bugs.
        nullids = len(
            [None for unused, id in missingids if id == self.repo.nullid]
        )
        if nullids:
            missingids = [
                (f, id) for f, id in missingids if id != self.repo.nullid
            ]
            repo.ui.develwarn(
                (
                    b'remotefilelog not fetching %d null revs'
                    b' - this is likely hiding bugs' % nullids
                ),
                config=b'remotefilelog-ext',
            )
        if missingids:
            global fetches, fetched, fetchcost
            fetches += 1

            # We want to be able to detect excess individual file downloads, so
            # let's log that information for debugging.
            if fetches >= 15 and fetches < 18:
                if fetches == 15:
                    fetchwarning = self.ui.config(
                        b'remotefilelog', b'fetchwarning'
                    )
                    if fetchwarning:
                        self.ui.warn(fetchwarning + b'\n')
                self.logstacktrace()
            missingids = [(file, hex(id)) for file, id in sorted(missingids)]
            fetched += len(missingids)
            start = time.time()
            missingids = self.request(missingids)
            if missingids:
                raise error.Abort(
                    _(b"unable to download %d files") % len(missingids)
                )
            fetchcost += time.time() - start
            self._lfsprefetch(fileids)
###END###
def _lfsprefetch(self, fileids):
        if not _lfsmod or not util.safehasattr(
            self.repo.svfs, b'lfslocalblobstore'
        ):
            return
        if not _lfsmod.wrapper.candownload(self.repo):
            return
        pointers = []
        store = self.repo.svfs.lfslocalblobstore
        for file, id in fileids:
            node = bin(id)
            rlog = self.repo.file(file)
            if rlog.flags(node) & revlog.REVIDX_EXTSTORED:
                text = rlog.rawdata(node)
                p = _lfsmod.pointer.deserialize(text)
                oid = p.oid()
                if not store.has(oid):
                    pointers.append(p)
        if len(pointers) > 0:
            self.repo.svfs.lfsremoteblobstore.readbatch(pointers, store)
            assert all(store.has(p.oid()) for p in pointers)
###END###
def logstacktrace(self):
        import traceback

        self.ui.log(
            b'remotefilelog',
            b'excess remotefilelog fetching:\n%s\n',
            b''.join(pycompat.sysbytes(s) for s in traceback.format_stack()),
        )
###END###
def x_rfl_getfile(self, file, node):
            if not self.capable(b'x_rfl_getfile'):
                raise error.Abort(
                    b'configured remotefile server does not support getfile'
                )
            f = wireprotov1peer.future()
            yield {b'file': file, b'node': node}, f
            code, data = f.value.split(b'\0', 1)
            if int(code):
                raise error.LookupError(file, node, data)
            yield data
###END###
def x_rfl_getflogheads(self, path):
            if not self.capable(b'x_rfl_getflogheads'):
                raise error.Abort(
                    b'configured remotefile server does not '
                    b'support getflogheads'
                )
            f = wireprotov1peer.future()
            yield {b'path': path}, f
            heads = f.value.split(b'\n') if f.value else []
            yield heads
###END###
def _updatecallstreamopts(self, command, opts):
            if command != b'getbundle':
                return
            if (
                constants.NETWORK_CAP_LEGACY_SSH_GETFILES
                not in self.capabilities()
            ):
                return
            if not util.safehasattr(self, '_localrepo'):
                return
            if (
                constants.SHALLOWREPO_REQUIREMENT
                not in self._localrepo.requirements
            ):
                return

            bundlecaps = opts.get(b'bundlecaps')
            if bundlecaps:
                bundlecaps = [bundlecaps]
            else:
                bundlecaps = []

            # shallow, includepattern, and excludepattern are a hacky way of
            # carrying over data from the local repo to this getbundle
            # command. We need to do it this way because bundle1 getbundle
            # doesn't provide any other place we can hook in to manipulate
            # getbundle args before it goes across the wire. Once we get rid
            # of bundle1, we can use bundle2's _pullbundle2extraprepare to
            # do this more cleanly.
            bundlecaps.append(constants.BUNDLE2_CAPABLITY)
            if self._localrepo.includepattern:
                patterns = b'\0'.join(self._localrepo.includepattern)
                includecap = b"includepattern=" + patterns
                bundlecaps.append(includecap)
            if self._localrepo.excludepattern:
                patterns = b'\0'.join(self._localrepo.excludepattern)
                excludecap = b"excludepattern=" + patterns
                bundlecaps.append(excludecap)
            opts[b'bundlecaps'] = b','.join(bundlecaps)
###END###
def _sendrequest(self, command, args, **opts):
            self._updatecallstreamopts(command, args)
            return super(remotefilepeer, self)._sendrequest(
                command, args, **opts
            )
###END###
def _callstream(self, command, **opts):
            supertype = super(remotefilepeer, self)
            if not util.safehasattr(supertype, '_sendrequest'):
                self._updatecallstreamopts(command, pycompat.byteskwargs(opts))
            return super(remotefilepeer, self)._callstream(command, **opts)
###END###
def __init__(self):
                    self.missingids = []
                    self.connected = True
###END###
def close(self):
                    pass
###END###
def request(self, value, flush=True):
                    lines = value.split(b"\n")
                    if lines[0] != b"get":
                        return
                    self.missingids = lines[2:-1]
                    self.missingids.append(b'0')
###END###
def receiveline(self):
                    if len(self.missingids) > 0:
                        return self.missingids.pop(0)
                    return None
###END###
def __init__(self, ui, path):
        super(datapackstore, self).__init__(ui, path)
###END###
def getpack(self, path):
        return datapack(path)
###END###
def get(self, name, node):
        raise RuntimeError(b"must use getdeltachain with datapackstore")
###END###
def getmeta(self, name, node):
        for pack in self.packs:
            try:
                return pack.getmeta(name, node)
            except KeyError:
                pass

        for pack in self.refresh():
            try:
                return pack.getmeta(name, node)
            except KeyError:
                pass

        raise KeyError((name, hex(node)))
###END###
def getdelta(self, name, node):
        for pack in self.packs:
            try:
                return pack.getdelta(name, node)
            except KeyError:
                pass

        for pack in self.refresh():
            try:
                return pack.getdelta(name, node)
            except KeyError:
                pass

        raise KeyError((name, hex(node)))
###END###
def getdeltachain(self, name, node):
        for pack in self.packs:
            try:
                return pack.getdeltachain(name, node)
            except KeyError:
                pass

        for pack in self.refresh():
            try:
                return pack.getdeltachain(name, node)
            except KeyError:
                pass

        raise KeyError((name, hex(node)))
###END###
def add(self, name, node, data):
        raise RuntimeError(b"cannot add to datapackstore")
###END###
def getmissing(self, keys):
        missing = []
        for name, node in keys:
            value = self._find(node)
            if not value:
                missing.append((name, node))

        return missing
###END###
def get(self, name, node):
        raise RuntimeError(
            b"must use getdeltachain with datapack (%s:%s)" % (name, hex(node))
        )
###END###
def getmeta(self, name, node):
        value = self._find(node)
        if value is None:
            raise KeyError((name, hex(node)))

        node, deltabaseoffset, offset, size = value
        rawentry = self._data[offset : offset + size]

        # see docstring of mutabledatapack for the format
        offset = 0
        offset += struct.unpack_from(b'!H', rawentry, offset)[0] + 2  # filename
        offset += 40  # node, deltabase node
        offset += struct.unpack_from(b'!Q', rawentry, offset)[0] + 8  # delta

        metalen = struct.unpack_from(b'!I', rawentry, offset)[0]
        offset += 4

        meta = shallowutil.parsepackmeta(rawentry[offset : offset + metalen])

        return meta
###END###
def getdelta(self, name, node):
        value = self._find(node)
        if value is None:
            raise KeyError((name, hex(node)))

        node, deltabaseoffset, offset, size = value
        entry = self._readentry(offset, size, getmeta=True)
        filename, node, deltabasenode, delta, meta = entry

        # If we've read a lot of data from the mmap, free some memory.
        self.freememory()

        return delta, filename, deltabasenode, meta
###END###
def getdeltachain(self, name, node):
        value = self._find(node)
        if value is None:
            raise KeyError((name, hex(node)))

        params = self.params

        # Precompute chains
        chain = [value]
        deltabaseoffset = value[1]
        entrylen = self.INDEXENTRYLENGTH
        while (
            deltabaseoffset != FULLTEXTINDEXMARK
            and deltabaseoffset != NOBASEINDEXMARK
        ):
            loc = params.indexstart + deltabaseoffset
            value = struct.unpack(
                self.INDEXFORMAT, self._index[loc : loc + entrylen]
            )
            deltabaseoffset = value[1]
            chain.append(value)

        # Read chain data
        deltachain = []
        for node, deltabaseoffset, offset, size in chain:
            filename, node, deltabasenode, delta = self._readentry(offset, size)
            deltachain.append((filename, node, filename, deltabasenode, delta))

        # If we've read a lot of data from the mmap, free some memory.
        self.freememory()

        return deltachain
###END###
def _readentry(self, offset, size, getmeta=False):
        rawentry = self._data[offset : offset + size]
        self._pagedin += len(rawentry)

        # <2 byte len> + <filename>
        lengthsize = 2
        filenamelen = struct.unpack(b'!H', rawentry[:2])[0]
        filename = rawentry[lengthsize : lengthsize + filenamelen]

        # <20 byte node> + <20 byte deltabase>
        nodestart = lengthsize + filenamelen
        deltabasestart = nodestart + NODELENGTH
        node = rawentry[nodestart:deltabasestart]
        deltabasenode = rawentry[deltabasestart : deltabasestart + NODELENGTH]

        # <8 byte len> + <delta>
        deltastart = deltabasestart + NODELENGTH
        rawdeltalen = rawentry[deltastart : deltastart + 8]
        deltalen = struct.unpack(b'!Q', rawdeltalen)[0]

        delta = rawentry[deltastart + 8 : deltastart + 8 + deltalen]
        delta = self._decompress(delta)

        if getmeta:
            metastart = deltastart + 8 + deltalen
            metalen = struct.unpack_from(b'!I', rawentry, metastart)[0]

            rawmeta = rawentry[metastart + 4 : metastart + 4 + metalen]
            meta = shallowutil.parsepackmeta(rawmeta)
            return filename, node, deltabasenode, delta, meta
        else:
            return filename, node, deltabasenode, delta
###END###
def _decompress(self, data):
        return zlib.decompress(data)
###END###
def add(self, name, node, data):
        raise RuntimeError(b"cannot add to datapack (%s:%s)" % (name, node))
###END###
def _find(self, node):
        params = self.params
        fanoutkey = struct.unpack(
            params.fanoutstruct, node[: params.fanoutprefix]
        )[0]
        fanout = self._fanouttable

        start = fanout[fanoutkey] + params.indexstart
        indexend = self._indexend

        # Scan forward to find the first non-same entry, which is the upper
        # bound.
        for i in pycompat.xrange(fanoutkey + 1, params.fanoutcount):
            end = fanout[i] + params.indexstart
            if end != start:
                break
        else:
            end = indexend

        # Bisect between start and end to find node
        index = self._index
        startnode = index[start : start + NODELENGTH]
        endnode = index[end : end + NODELENGTH]
        entrylen = self.INDEXENTRYLENGTH
        if startnode == node:
            entry = index[start : start + entrylen]
        elif endnode == node:
            entry = index[end : end + entrylen]
        else:
            while start < end - entrylen:
                mid = start + (end - start) // 2
                mid = mid - ((mid - params.indexstart) % entrylen)
                midnode = index[mid : mid + NODELENGTH]
                if midnode == node:
                    entry = index[mid : mid + entrylen]
                    break
                if node > midnode:
                    start = mid
                elif node < midnode:
                    end = mid
            else:
                return None

        return struct.unpack(self.INDEXFORMAT, entry)
###END###
def markledger(self, ledger, options=None):
        for filename, node in self:
            ledger.markdataentry(self, filename, node)
###END###
def cleanup(self, ledger):
        entries = ledger.sources.get(self, [])
        allkeys = set(self)
        repackedkeys = {
            (e.filename, e.node) for e in entries if e.datarepacked or e.gced
        }

        if len(allkeys - repackedkeys) == 0:
            if self.path not in ledger.created:
                util.unlinkpath(self.indexpath, ignoremissing=True)
                util.unlinkpath(self.packpath, ignoremissing=True)
###END###
def __iter__(self):
        for f, n, deltabase, deltalen in self.iterentries():
            yield f, n
###END###
def iterentries(self):
        # Start at 1 to skip the header
        offset = 1
        data = self._data
        while offset < self.datasize:
            oldoffset = offset

            # <2 byte len> + <filename>
            filenamelen = struct.unpack(b'!H', data[offset : offset + 2])[0]
            offset += 2
            filename = data[offset : offset + filenamelen]
            offset += filenamelen

            # <20 byte node>
            node = data[offset : offset + constants.NODESIZE]
            offset += constants.NODESIZE
            # <20 byte deltabase>
            deltabase = data[offset : offset + constants.NODESIZE]
            offset += constants.NODESIZE

            # <8 byte len> + <delta>
            rawdeltalen = data[offset : offset + 8]
            deltalen = struct.unpack(b'!Q', rawdeltalen)[0]
            offset += 8

            # TODO(augie): we should store a header that is the
            # uncompressed size.
            uncompressedlen = len(
                self._decompress(data[offset : offset + deltalen])
            )
            offset += deltalen

            # <4 byte len> + <metadata-list>
            metalen = struct.unpack_from(b'!I', data, offset)[0]
            offset += 4 + metalen

            yield (filename, node, deltabase, uncompressedlen)

            # If we've read a lot of data from the mmap, free some memory.
            self._pagedin += offset - oldoffset
            if self.freememory():
                data = self._data
###END###
def _compress(self, data):
        return zlib.compress(data)
###END###
def add(self, name, node, deltabasenode, delta, metadata=None):
        # metadata is a dict, ex. {METAKEYFLAG: flag}
        if len(name) > 2 ** 16:
            raise RuntimeError(_(b"name too long %s") % name)
        if len(node) != 20:
            raise RuntimeError(_(b"node should be 20 bytes %s") % node)

        if node in self.entries:
            # The revision has already been added
            return

        # TODO: allow configurable compression
        delta = self._compress(delta)

        rawdata = b''.join(
            (
                struct.pack(b'!H', len(name)),  # unsigned 2 byte int
                name,
                node,
                deltabasenode,
                struct.pack(b'!Q', len(delta)),  # unsigned 8 byte int
                delta,
            )
        )

        # v1 support metadata
        rawmeta = shallowutil.buildpackmeta(metadata)
        rawdata += struct.pack(b'!I', len(rawmeta))  # unsigned 4 byte
        rawdata += rawmeta

        offset = self.packfp.tell()

        size = len(rawdata)

        self.entries[node] = (deltabasenode, offset, size)

        self.writeraw(rawdata)
###END###
def createindex(self, nodelocations, indexoffset):
        entries = sorted(
            (n, db, o, s) for n, (db, o, s) in pycompat.iteritems(self.entries)
        )

        rawindex = b''
        fmt = self.INDEXFORMAT
        for node, deltabase, offset, size in entries:
            if deltabase == sha1nodeconstants.nullid:
                deltabaselocation = FULLTEXTINDEXMARK
            else:
                # Instead of storing the deltabase node in the index, let's
                # store a pointer directly to the index entry for the deltabase.
                deltabaselocation = nodelocations.get(
                    deltabase, NOBASEINDEXMARK
                )

            entry = struct.pack(fmt, node, deltabaselocation, offset, size)
            rawindex += entry

        return rawindex
###END###
